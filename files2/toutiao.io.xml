<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>cf66e2ba847b507d0ac6dc9e4ba7cb65</guid>
<title>阿里巴巴开源高性能 JSON 库 FASTJSON v2</title>
<link>https://toutiao.io/k/qy5z8im</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content                                                                     &quot; id=&quot;js_content&quot;&gt;
            &lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5MzA0ODkyMA==&amp;amp;mid=2655078619&amp;amp;idx=1&amp;amp;sn=b4db7f1880038187eecd0454d0f61be9&amp;amp;chksm=bd2918ec8a5e91fae83ccb2a9e977418dcd284c620ad44a25974071e9b1b4d8857628f26ad18&amp;amp;scene=21#wechat_redirect&quot; textvalue=&quot;‍‍&quot; linktype=&quot;text&quot; imgurl=&quot;&quot; imgdata=&quot;null&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;1&quot;&gt;&lt;span&gt;&lt;span class=&quot;js_jump_icon h5_image_link&quot; data-positionback=&quot;static&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-cropselx1=&quot;0&quot; data-cropselx2=&quot;578&quot; data-cropsely1=&quot;0&quot; data-cropsely2=&quot;325&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/AjN1jquNav9oso6Gxia5jcD3Wgg3yvKNEpy1FkFVItdSic9wV0kdNVdZbLl1nID64c4NCC9U6Pmd1eNQKQmhSM2w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5MzA0ODkyMA==&amp;amp;mid=2655078619&amp;amp;idx=1&amp;amp;sn=b4db7f1880038187eecd0454d0f61be9&amp;amp;chksm=bd2918ec8a5e91fae83ccb2a9e977418dcd284c620ad44a25974071e9b1b4d8857628f26ad18&amp;amp;scene=21#wechat_redirect&quot; textvalue=&quot;疫情之中，写给大家的几句话｜码农周刊VIP会员专属邮件周报 Vol.089&quot; linktype=&quot;text&quot; imgurl=&quot;&quot; imgdata=&quot;null&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;&lt;strong&gt;码农周刊VIP会员专属邮件周报 Vol.089&lt;/strong&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;FASTJSON v2是FASTJSON项目的重要升级，目标是&lt;strong&gt;为下一个十年提供一个高性能的JSON库&lt;/strong&gt;。通过同一套API，&lt;br/&gt;- 支持JSON/JSONB两种协议，JSONPath是一等公民；&lt;br/&gt;- 支持全量解析和部分解析；&lt;br/&gt;- 支持Java服务端、客户端Android、大数据场景。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;1.36&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/AjN1jquNav9oso6Gxia5jcD3Wgg3yvKNECticLB0cOSwYRw2owmicL2m9BLdnB61L2kwdK3CkXunp38g3zuv0Bw4g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot;/&gt;&lt;/p&gt;
          &lt;/div&gt;

          

          



           
                                
                    
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>9b38c084cfc18820f204ba54e75a0761</guid>
<title>HiveQL 进阶之以柔克刚 - 将简单语法运用到极致</title>
<link>https://toutiao.io/k/nxy3pln</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;RichText ztext Post-RichText css-14bz7qe&quot; options=&quot;[object Object]&quot;&gt;&lt;h2 data-first-child=&quot;&quot; id=&quot;h_508038040_0&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;前言&lt;/b&gt;&lt;/h2&gt;&lt;h3 id=&quot;h_508038040_1&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;初衷&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;dsStpuLp&quot;&gt;&lt;b&gt;如何高效地使用 HiveQL ，将 HiveQL 运用到极致。&lt;/b&gt;&lt;/p&gt;&lt;p data-pid=&quot;Zkg-Pxd5&quot;&gt;在大数据如此流行的今天，不只是专业的数据人员，需要经常地跟 SQL 打交道，即使是产品、运营等非技术伙伴，也会或多或少地使用过 SQL ，如何高效地发挥 SQL 的能力，继而发挥数据的能力，变得尤为重要。&lt;/p&gt;&lt;p data-pid=&quot;Un0-K3hI&quot;&gt;HiveQL 发展到今天已经颇为成熟，作为一种 SQL 方言，其支持大多数查询语法，具有较为丰富的内置函数，同时还支持开窗函数、用户自定义函数、反射机制等诸多高级特性。面对一个复杂的数据场景，或许有人技术娴熟，选择使用 HiveQL 高级特性解决，如：编写用户自定义函数扩展 SQL 的数据处理能力；或许有人选择敬而远之，转向使用其他非 SQL 类型的解决方案。本文并不讨论不同方式的优劣，而是尝试独辟蹊径，不是强调偏僻的语法特性或是复杂的 UDF 实现，而是强调 &lt;b&gt;通过灵活的、发散性的数据处理思维，就可以用最简单的语法，解决复杂的数据场景。&lt;/b&gt;&lt;/p&gt;&lt;h3 id=&quot;h_508038040_2&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;适合人群&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;tr_XElYl&quot;&gt;不论是数据开发初学者还是资深人员，本篇文章或许都能有所帮助，不过更适合中级、高级读者阅读。&lt;/p&gt;&lt;p data-pid=&quot;pZ7BqR9X&quot;&gt;本篇文章重点介绍数据处理思维，并没有涉及到过多高阶的语法，同时为了避免主题发散，文中涉及的函数、语法特性等，不会花费篇幅进行专门的介绍，读者可以按自身情况自行了解。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_3&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;内容结构&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;7k535JYC&quot;&gt;本篇文章将围绕数列生成、区间变换、排列组合、连续判别等主题进行介绍，并附以案例进行实际运用讲解。每个主题之间有轻微的前后依赖关系，依次阅读更佳。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_4&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;提示信息&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;NRQ0H870&quot;&gt;本篇文章涉及的 SQL 语句只使用到了 HiveQL 基本的语法特性，理论上可以在目前的主流版本中运行，同时特意注明，运行环境、兼容性等问题不在本篇文章关注范围内。&lt;/p&gt;&lt;h2 id=&quot;h_508038040_5&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;快速制造测试数据&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;8zF_Ahh2&quot;&gt;生成用户访问日志表 visit_log ，每一行数据表示一条用户访问日志。该表将被用作下文各类场景的测试数据。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 1
with visit_log as (
    select stack (
        6,
        &#x27;2022-01-01&#x27;, &#x27;101&#x27;, &#x27;湖北&#x27;, &#x27;武汉&#x27;, &#x27;Android&#x27;,
        &#x27;2022-01-01&#x27;, &#x27;102&#x27;, &#x27;湖南&#x27;, &#x27;长沙&#x27;, &#x27;IOS&#x27;,
        &#x27;2022-01-01&#x27;, &#x27;103&#x27;, &#x27;四川&#x27;, &#x27;成都&#x27;, &#x27;Windows&#x27;,
        &#x27;2022-01-02&#x27;, &#x27;101&#x27;, &#x27;湖北&#x27;, &#x27;孝感&#x27;, &#x27;Mac&#x27;,
        &#x27;2022-01-02&#x27;, &#x27;102&#x27;, &#x27;湖南&#x27;, &#x27;邵阳&#x27;, &#x27;Android&#x27;,
        &#x27;2022-01-03&#x27;, &#x27;101&#x27;, &#x27;湖北&#x27;, &#x27;武汉&#x27;, &#x27;IOS&#x27;
    ) as (dt, user_id, province, city, device_type)
)
select * from visit_log;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&quot;h_508038040_6&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;数列&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;EcKTYznT&quot;&gt;数列是最常见的数据形式之一，实际数据开发场景中遇到的基本都是有限数列，也是本节将要重点介绍的内容。本节将从最简单的递增数列开始，找出一般方法并推广到更泛化的场景。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_7&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;仙人指路&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_8&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;一个简单的递增数列&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;osPpRNZV&quot;&gt;首先引出一个简单的递增整数数列场景：&lt;/p&gt;&lt;ul&gt;&lt;li data-pid=&quot;42iVqSTD&quot;&gt;从数值 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+0+&quot; alt=&quot; 0 &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 开始；&lt;/li&gt;&lt;li data-pid=&quot;Ms4DHjQb&quot;&gt;之后的每个数值递增 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+1+&quot; alt=&quot; 1 &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ；&lt;/li&gt;&lt;li data-pid=&quot;rZ8Ocd6O&quot;&gt;至数值 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+3+&quot; alt=&quot; 3 &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 结束； 如何生成满足以上三个条件的数列？即 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+%5B0%2C1%2C2%2C3%5D+&quot; alt=&quot; [0,1,2,3] &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 。&lt;/li&gt;&lt;/ul&gt;&lt;p data-pid=&quot;BAujbTaN&quot;&gt;实际上，生成该数列的方式有多种，此处介绍其中一种简单且通用的方案。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 2
select
    t.pos as a_n
from (
    select posexplode(split(space(3), space(1)))
) t;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;zADWlPxf&quot;&gt;通过上述 SQL 片段可得知，生成一个递增序列只需要三个步骤：&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;MMfkR_4e&quot;&gt;生成一个长度合适的数组，数组中的元素不需要具有实际含义；&lt;/li&gt;&lt;li data-pid=&quot;Zck-p59T&quot;&gt;通过 UDTF 函数 posexplode 对数组中的每个元素生成索引下标；&lt;/li&gt;&lt;li data-pid=&quot;AkBeOn_x&quot;&gt;取出每个元素的索引下标。 以上三个步骤可以推广至更一般的数列场景：等差数列、等比数列。下文将以此为基础，直接给出最终实现模板。&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;h_508038040_9&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;等差数列&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;lzHyfwtq&quot;&gt;若设首项 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_1+%3D+a+&quot; alt=&quot; a_1 = a &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，公差为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+d+&quot; alt=&quot; d &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，则等差数列的通项公式为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_n+%3D+a_1+%2B+%28n+-+1%29d+&quot; alt=&quot; a_n = a_1 + (n - 1)d &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 。 SQL 实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 3
select
    a_1 + t.pos * d as a_n
from (
    select posexplode(split(space(n - 1), space(1)))
) t;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;h_508038040_10&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;等比数列&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;dZDIrol0&quot;&gt;若设首项 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_1+%3D+a+&quot; alt=&quot; a_1 = a &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，公比为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+r+&quot; alt=&quot; r &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，则等比数列的通项公式为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_n+%3D+ar%5E%7Bn-1%7D+&quot; alt=&quot; a_n = ar^{n-1} &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 。 SQL 实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 4
select
    a_1 * pow(r, t.pos) as a_n
from (
    select posexplode(split(space(n - 1), space(1)))
) t;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;h_508038040_11&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;应用场景举例&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_12&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;如何还原任意维度组合下的维度列簇名称？&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;LKKt37p5&quot;&gt;在多维分析场景下，可能会用到高阶聚合函数，如 &lt;i&gt;cube&lt;/i&gt; 、 &lt;i&gt;rollup&lt;/i&gt; 、 &lt;i&gt;grouping sets&lt;/i&gt; 等，可以针对不同的维度组合下的数据进行聚合统计。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_13&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;场景描述&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;UufnFZLH&quot;&gt;现有用户访问日志表 visit_log ，该表定义见 &lt;b&gt;快速制造测试数据&lt;/b&gt;。 假如针对省份 province , 城市 city, 设备类型 device_type 三个维度列，通过高阶聚合函数，统计得到了不同维度组合下的用户访问量。&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;Njahkaml&quot;&gt;如何知道一条统计结果是根据哪些维度列聚合出来的？&lt;/li&gt;&lt;li data-pid=&quot;CLGo1VS_&quot;&gt;想要输出 &lt;b&gt;聚合的维度列&lt;/b&gt; 的名称，用于下游的报表展示等场景，又该如何处理？&lt;/li&gt;&lt;/ol&gt;&lt;h3 id=&quot;h_508038040_14&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;解决思路&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;QZRLBqIN&quot;&gt;&lt;b&gt;可以借助 Hive 提供的 Grouping__ID 来实现，核心方法是对 Grouping__ID 进行逆向实现。&lt;/b&gt; 详细步骤如下：&lt;/p&gt;&lt;p data-pid=&quot;IIzS-gWa&quot;&gt;&lt;b&gt;一、准备好所有的 Grouping__ID 。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li data-pid=&quot;_f413xyP&quot;&gt;生成一个包含 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+2%5Ex+&quot; alt=&quot; 2^x &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 个数值的递增数列，每个数值表示一种 Grouping__ID ，其中 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+x+&quot; alt=&quot; x &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 为所有维度列的数量， &lt;img src=&quot;https://www.zhihu.com/equation?tex=+2%5Ex+&quot; alt=&quot; 2^x &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 为所有维度组合的数量。即 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+%7B+0%2C+1%2C+2%2C+...%2C+2%5Ex+-+1+%7D+&quot; alt=&quot; { 0, 1, 2, ..., 2^x - 1 } &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt;&lt;br/&gt; &lt;/li&gt;&lt;li data-pid=&quot;Tx_HxxHU&quot;&gt;将递增数列中的每个 Grouping__ID 转为 2 进制字符串，并展开该 2 进制字符串的每个比特位。例如&lt;/li&gt;&lt;/ul&gt;&lt;p data-pid=&quot;uD4NOzjP&quot;&gt;&lt;b&gt;二、准备好所有维度列的名称。&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;{ dim_col_1, dim_col_2, ..., dim_col_x }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;2M-Ips0B&quot;&gt;&lt;b&gt;三、将 Grouping__ID 映射到维度列名称。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li data-pid=&quot;d2R7WVvk&quot;&gt;对于递增数列中的每个数值，将该数值的 2 进制的每个比特位与维度列的下标进行映射。例如&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;grouping__id：3 =&amp;gt; { 0, 0, 0, 1, 1 }
维度列：{ dim_col_1, dim_col_2, dim_col_3, dim_col_4, dim_col_5 }
映射结果：{ 0:dim_col_1, 0:dim_col_2, 0:dim_col_3, 1:dim_col_4, 1:dim_col_5 }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li data-pid=&quot;lSj8aQoq&quot;&gt;对递增数列中的每个数值进行聚合，输出所有比特位等于 0 的维度列。&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;dim_col_1,dim_col_2,dim_col_3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;SQaeeH1W&quot;&gt;注意：不同版本的 Hive 之间， Grouping__ID 实现有差异，以上处理逻辑适用于 2.3.0 及之后的版本。 2.3.0 之前的版本基于上述步骤稍加修改即可，此处不再专门花费篇幅描述。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_15&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;SQL 实现&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 5
with group_dimension as (
    select -- 每种分组对应的维度字段
        gb.group_id, concat_ws(&quot;,&quot;, collect_list(case when gb.placeholder_bit = 0 then dim_col.val else null end)) as dimension_name
    from (
     select groups.pos as group_id, pe.*
        from (
            select posexplode(split(space(cast(pow(2, 3) as int) - 1), space(1)))
        ) groups -- 所有分组
        lateral view posexplode(split(lpad(conv(groups.pos,10,2), 3, &quot;0&quot;), &#x27;&#x27;)) pe as placeholder_idx, placeholder_bit -- 每个分组的bit信息
    ) gb
    left join ( -- 所有维度字段
     select posexplode(split(&quot;省份,城市,设备类型&quot;, &#x27;,&#x27;))
    ) dim_col on gb.placeholder_idx = dim_col.pos
    group by gb.group_id
)
select 
    group_dimension.dimension_name as dimension_name,
    province, city, device_type,
    visit_count
from (
    select
        grouping__id as group_id,
        province, city, device_type,
        count(1) as visit_count
    from visit_log b
    group by province, city, device_type
    GROUPING SETS(
        (province),
        (province, city),
        (province, city, device_type)
    )
) t
join group_dimension on t.group_id = group_dimension.group_id
order by dimension_name;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;dimension_name&lt;/th&gt;&lt;th&gt;province&lt;/th&gt;&lt;th&gt;city&lt;/th&gt;&lt;th&gt;device_type&lt;/th&gt;&lt;th&gt;visit_count&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份&lt;/td&gt;&lt;td&gt;四川&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;武汉&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;长沙&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;邵阳&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;孝感&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市&lt;/td&gt;&lt;td&gt;四川&lt;/td&gt;&lt;td&gt;成都&lt;/td&gt;&lt;td&gt;NULL&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;孝感&lt;/td&gt;&lt;td&gt;Mac&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;长沙&lt;/td&gt;&lt;td&gt;IOS&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;邵阳&lt;/td&gt;&lt;td&gt;Android&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;四川&lt;/td&gt;&lt;td&gt;成都&lt;/td&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;武汉&lt;/td&gt;&lt;td&gt;Android&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;省份,城市,设备类型&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;武汉&lt;/td&gt;&lt;td&gt;IOS&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;h_508038040_16&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;区间&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;8_FkKob4&quot;&gt;相比于数列较多用于表示离散数据，区间往往用于描述连续的数据，虽然两者具有不同的数据特征，不过在实际应用中，数列与区间的处理具有较多相通性。本节将介绍一些常见的区间场景，并抽象出通用的解决方案。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_17&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;二鬼拍门&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_18&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;区间分割&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;vEfvPwQo&quot;&gt;已知一个数值区间 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+%5Ba%2Cb%5D+%3D+%5C%7B+x+%7C+a+%5Cleq+x+%5Cleq+b+%5C%7D+&quot; alt=&quot; [a,b] = \{ x | a \leq x \leq b \} &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，如何将该区间均分成 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+n+&quot; alt=&quot; n &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 段子区间？&lt;/p&gt;&lt;p data-pid=&quot;UWHNY4-Q&quot;&gt;该问题可以简化为数列问题，数列公式为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_n+%3D+a_1+%2B+%28n+-+1%29d+&quot; alt=&quot; a_n = a_1 + (n - 1)d &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ，其中 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+a_1+%3D+a+&quot; alt=&quot; a_1 = a &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ， &lt;img src=&quot;https://www.zhihu.com/equation?tex=+d+%3D+%28b+-+a%29+%2F+n+&quot; alt=&quot; d = (b - a) / n &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; ：&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;rLBODHMA&quot;&gt;生成一个长度为 &lt;img src=&quot;https://www.zhihu.com/equation?tex=+n+&quot; alt=&quot; n &quot; loading=&quot;lazy&quot; eeimg=&quot;1&quot;/&gt; 的数组，数组中的元素不需要具有实际含义；&lt;/li&gt;&lt;li data-pid=&quot;imicPj0T&quot;&gt;通过 UDTF 函数 posexplode 对数组中的每个元素生成索引下标；&lt;/li&gt;&lt;li data-pid=&quot;q_8ErFKX&quot;&gt;取出每个元素的索引下标，并进行数列公式计算，得出每个子区间的起始值与结束值。&lt;/li&gt;&lt;/ol&gt;&lt;p data-pid=&quot;JeEOHGKC&quot;&gt;SQL 实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 6
select
    a_1 + t.pos * d as sub_interval_start, -- 子区间起始值
    a_1 + (t.pos + 1) * d as sub_interval_end -- 子区间结束值
from (
    select posexplode(split(space(n - 1), space(1)))
) t;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&quot;h_508038040_19&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;区间交叉&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;3Wzn_eDP&quot;&gt;已知两个日期区间存在交叉 [&#x27;2022-01-01&#x27;, &#x27;2022-01-03&#x27;] 、 [&#x27;2022-01-02&#x27;, &#x27;2022-01-04&#x27;]&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;E9SHb3tr&quot;&gt;如何合并两个日期区间，并返回合并后的新区间？&lt;/li&gt;&lt;li data-pid=&quot;VoWgwEC3&quot;&gt;如何知道哪些日期是交叉日期，并返回该日期交叉次数？&lt;/li&gt;&lt;/ol&gt;&lt;p data-pid=&quot;UAtkkIHA&quot;&gt;解决上述问题的方法有多种，此处介绍其中一种简单且通用的方案。 核心思路是结合数列生成、区间分割方法，先将日期区间分解为最小处理单元，即多个日期组成的数列，然后再基于日期粒度做统计。具体步骤如下：&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;iLC7MPIR&quot;&gt;获取每个日期区间包含的天数；&lt;/li&gt;&lt;li data-pid=&quot;72zlNEqr&quot;&gt;按日期区间包含的天数，将日期区间拆分为相应数量的递增日期序列；&lt;/li&gt;&lt;li data-pid=&quot;oLMWWfGY&quot;&gt;通过日期序列统计合并后的区间，交叉次数；&lt;/li&gt;&lt;/ol&gt;&lt;p data-pid=&quot;o4WHSqIg&quot;&gt;SQL 实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 7
with tbl as (
    select stack(
        2,
        &#x27;2022-01-01&#x27;, &#x27;2022-01-03&#x27;,
        &#x27;2022-01-02&#x27;, &#x27;2022-01-04&#x27;
    ) as (date_start, date_end)
)
select 
    min(date_item) as date_start_merged, 
    max(date_item) as date_end_merged, 
    collect_set( -- 交叉日期计数
        case when date_item_cnt &amp;gt; 1 then concat(date_item, &#x27;:&#x27;, date_item_cnt) else null end
    ) as overlap_date
from (
    select 
        -- 拆解后的单个日期
        date_add(date_start, pos) as date_item,
        -- 拆解后的单个日期出现的次数
        count(1) over(partition by date_add(date_start, pos)) as date_item_cnt
    from tbl
    lateral view posexplode(split(space(datediff(date_end, date_start)), space(1))) t as pos, val
) t;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;date_start_merged&lt;/th&gt;&lt;th&gt;date_end_merged&lt;/th&gt;&lt;th&gt;overlap_date&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2022-01-01&lt;/td&gt;&lt;td&gt;2022-01-04&lt;/td&gt;&lt;td&gt;[&quot;2022-01-02:2&quot;,&quot;2022-01-03:2&quot;]&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p data-pid=&quot;CQkMHxzV&quot;&gt;&lt;b&gt;  增加点儿难度 ！&lt;/b&gt;&lt;/p&gt;&lt;p data-pid=&quot;-JmYYdi-&quot;&gt;如果有多个日期区间，且区间之间交叉状态未知，上述问题又该如何求解。即：&lt;/p&gt;&lt;ol&gt;&lt;li data-pid=&quot;5pMysqoq&quot;&gt;如何合并多个日期区间，并返回合并后的多个新区间？&lt;/li&gt;&lt;li data-pid=&quot;H9mKhjwP&quot;&gt;如何知道哪些日期是交叉日期，并返回该日期交叉次数？&lt;/li&gt;&lt;/ol&gt;&lt;p data-pid=&quot;u8cs4JQo&quot;&gt;SQL 实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 8
with tbl as (
    select stack(
        5,
        &#x27;2022-01-01&#x27;, &#x27;2022-01-03&#x27;,
        &#x27;2022-01-02&#x27;, &#x27;2022-01-04&#x27;,
        &#x27;2022-01-06&#x27;, &#x27;2022-01-08&#x27;,
        &#x27;2022-01-08&#x27;, &#x27;2022-01-08&#x27;,
        &#x27;2022-01-07&#x27;, &#x27;2022-01-10&#x27;
    ) as (date_start, date_end)
)
select
    min(date_item) as date_start_merged, 
    max(date_item) as date_end_merged,
    collect_set( -- 交叉日期计数
        case when date_item_cnt &amp;gt; 1 then concat(date_item, &#x27;:&#x27;, date_item_cnt) else null end
    ) as overlap_date
from (
    select 
        -- 拆解后的单个日期
        date_add(date_start, pos) as date_item,
        -- 拆解后的单个日期出现的次数
        count(1) over(partition by date_add(date_start, pos)) as date_item_cnt,
        -- 对于拆解后的单个日期，重组为新区间的标记
        date_add(date_add(date_start, pos), 1 - dense_rank() over(order by date_add(date_start, pos))) as cont
    from tbl
    lateral view posexplode(split(space(datediff(date_end, date_start)), space(1))) t as pos, val
) t
group by cont;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;date_start_merged&lt;/th&gt;&lt;th&gt;date_end_merged&lt;/th&gt;&lt;th&gt;overlap_date&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2022-01-01&lt;/td&gt;&lt;td&gt;2022-01-04&lt;/td&gt;&lt;td&gt;[&quot;2022-01-02:2&quot;,&quot;2022-01-03:2&quot;]&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2022-01-06&lt;/td&gt;&lt;td&gt;2022-01-10&lt;/td&gt;&lt;td&gt;[&quot;2022-01-07:2&quot;,&quot;2022-01-08:3&quot;]&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;h_508038040_20&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;应用场景举例&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_21&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;如何按任意时段统计时间区间数据？&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_22&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;场景描述&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;zWNDsQAF&quot;&gt;现有用户还款计划表 user_repayment ，该表内的一条数据，表示用户在指定日期区间内 [date_start, date_end] ，每天还款 repayment 元。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 9
with user_repayment as (
    select stack(
        3,
        &#x27;101&#x27;, &#x27;2022-01-01&#x27;, &#x27;2022-01-15&#x27;, 10,
        &#x27;102&#x27;, &#x27;2022-01-05&#x27;, &#x27;2022-01-20&#x27;, 20,
        &#x27;103&#x27;, &#x27;2022-01-10&#x27;, &#x27;2022-01-25&#x27;, 30
    ) as (user_id, date_start, date_end, repayment)
)
select * from user_repayment;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;N90ZojeZ&quot;&gt;如何统计某个时段内，每天所有用户的应还款总额？&lt;/p&gt;&lt;h3 id=&quot;h_508038040_23&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;解决思路&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;ITdaB4ka&quot;&gt;核心思路是将日期区间转换为日期序列，再按日期序列进行汇总统计。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_24&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;SQL 实现&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 10
select 
    date_item as day, 
    sum(repayment) as total_repayment
from (
    select 
        date_add(date_start, pos) as date_item,
        repayment
    from user_repayment
    lateral view posexplode(split(space(datediff(date_end, date_start)), space(1))) t as pos, val
) t
where date_item &amp;gt;= &#x27;2022-01-15&#x27; and date_item &amp;lt;= &#x27;2022-01-16&#x27;
group by date_item
order by date_item;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;day&lt;/th&gt;&lt;th&gt;total_repayment&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2022-01-15&lt;/td&gt;&lt;td&gt;60&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2022-01-16&lt;/td&gt;&lt;td&gt;50&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;h_508038040_25&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;排列组合&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;JMrmQHtz&quot;&gt;排列组合是针对离散数据常用的数据组织方法，实际应用场景中又以组合更为常见，本节将分别介绍排列、组合的实现方法，并结合实例着重介绍通过组合对数据的处理。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_26&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;双马饮泉&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_27&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;排列&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;20Ryiuwt&quot;&gt;已知字符序列 [ &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27; ] ，每次从该序列中可重复地选取出 2 个字符，如何获取到所有的排列？&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 11
select 
    concat(val1, val2) as perm
from (select split(&#x27;A,B,C&#x27;, &#x27;,&#x27;) as characters) dummy
lateral view explode(characters) t1 as val1
lateral view explode(characters) t2 as val2;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;C-vEsb-p&quot;&gt;整体实现比较简单。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_28&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;组合&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;8YimMt8d&quot;&gt;已知字符序列 [ &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27; ] ，每次从该序列中可重复地选取出 2 个字符，如何获取到所有的组合？&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 12
select 
    concat(least(val1, val2), greatest(val1, val2)) as comb
from (select split(&#x27;A,B,C&#x27;, &#x27;,&#x27;) as characters) dummy
lateral view explode(characters) t1 as val1
lateral view explode(characters) t2 as val2
group by least(val1, val2), greatest(val1, val2);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;d1CxD-f-&quot;&gt;整体实现比较简单。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_29&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;应用场景举例&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_30&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;如何对比统计所有组合？&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_31&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;场景描述&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;LzAuyJm8&quot;&gt;现有用户访问日志表 visit_log ，该表定义见 &lt;b&gt;快速制造测试数据&lt;/b&gt;。 如何按省份两两建立对比组，按对比组展示省份的用户访问量？&lt;/p&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;对比组&lt;/th&gt;&lt;th&gt;省份&lt;/th&gt;&lt;th&gt;用户访问量&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;湖北-湖南&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;xxx&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;湖北-湖南&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;xxx&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;h_508038040_32&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;解决思路&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;3m7PJU2S&quot;&gt;核心思路是从所有省份列表中不重复地取出 2 个省份，生成所有的组合结果，然后关联 visit_log 表分组统计结果。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_33&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;SQL 实现&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 13
select
    combs.province_comb,
    log.province,
    count(1) as visit_count
from visit_log log
join ( -- 所有对比组
    select 
        concat(least(val1, val2), &#x27;-&#x27;, greatest(val1, val2)) as province_comb,
        least(val1, val2) as province_1, greatest(val1, val2) as province_2
    from (
        select collect_set(province) as provinces
        from visit_log
    ) dummy
    lateral view explode(provinces) t1 as val1
    lateral view explode(provinces) t2 as val2
    where val1 &amp;lt;&amp;gt; val2
    group by least(val1, val2), greatest(val1, val2)
) combs on 1 = 1
where log.province in (combs.province_1, combs.province_2)
group by combs.province_comb, log.province
order by combs.province_comb, log.province;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;对比组&lt;/th&gt;&lt;th&gt;省份&lt;/th&gt;&lt;th&gt;用户访问量&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;四川-湖北&lt;/td&gt;&lt;td&gt;四川&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;四川-湖北&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;四川-湖南&lt;/td&gt;&lt;td&gt;四川&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;四川-湖南&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;湖北-湖南&lt;/td&gt;&lt;td&gt;湖北&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;湖北-湖南&lt;/td&gt;&lt;td&gt;湖南&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;h_508038040_34&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;连续&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;eRCfoBOl&quot;&gt;本节主要介绍连续性问题，重点描述了连续活跃场景。对于静态类型的连续活跃、动态类型的连续活跃，分别阐述了不同的实现方案。 本节内容直接贴近具体的应用，大部分篇幅以 SQL 内容为主。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_35&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;静态连续活跃场景统计&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_36&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;场景描述&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;TWXeU9Um&quot;&gt;现有用户访问日志表 visit_log ，该表定义见 &lt;b&gt;快速制造测试数据&lt;/b&gt;。 如何获取连续登录大于或等于 2 天的用户？&lt;/p&gt;&lt;p data-pid=&quot;8BraI7QM&quot;&gt;上述问题在分析连续性时，获取连续性的结果以超过固定阈值为准，可归类为 &lt;b&gt;连续活跃大于 N 天的静态连续活跃场景统计&lt;/b&gt;。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_37&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;SQL 实现&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_38&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;基于相邻日期差实现（ lag / lead 版）&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 14
select user_id
from (
    select 
        *,
        lag(dt, 2 - 1) over(partition by user_id order by dt) as lag_dt
    from (select dt, user_id from visit_log group by dt, user_id) t0
) t1
where datediff(dt, lag_dt) + 1 = 2
group by user_id;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;ZciMVFDG&quot;&gt;整体实现比较简单。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_39&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;基于相邻日期差实现（排序版）&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 15
select user_id
from (
    select *, 
        dense_rank() over(partition by user_id order by dt) as dr
    from visit_log
) t1
where datediff(dt, date_add(dt, 1 - dr)) + 1 = 2
group by user_id;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;bCHgsALV&quot;&gt;整体实现比较简单。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_40&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;基于连续活跃天数实现&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 16
select user_id
from (
    select 
        *,
        -- 连续活跃天数
        count(distinct dt) 
            over(partition by user_id, cont) as cont_days
    from (
        select 
            *, 
            date_add(dt, 1 - dense_rank() 
                over(partition by user_id order by dt)) as cont
        from visit_log
    ) t1
) t2
where cont_days &amp;gt;= 2
group by user_id;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;S4TcMlFt&quot;&gt;可以视作 &lt;b&gt;基于相邻日期差实现（排序版）&lt;/b&gt; 的衍生版本，该实现能获取到更多信息，如连续活跃天数。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_41&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;基于连续活跃区间实现&lt;/b&gt;&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 17
select user_id
from (
    select 
        user_id, cont, 
        -- 连续活跃区间
        min(dt) as cont_date_start, max(dt) as cont_date_end
    from (
        select 
            *, 
            date_add(dt, 1 - dense_rank() 
                over(partition by user_id order by dt)) as cont
        from visit_log
    ) t1
    group by user_id, cont
) t2
where datediff(cont_date_end, cont_date_start) + 1 &amp;gt;= 2
group by user_id;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p data-pid=&quot;MlNnz0SI&quot;&gt;可以视作 &lt;b&gt;基于相邻日期差实现（排序版）&lt;/b&gt; 的衍生版本，该实现能获取到更多信息，如连续活跃区间。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_42&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;动态连续活跃场景统计&lt;/b&gt;&lt;/h3&gt;&lt;h3 id=&quot;h_508038040_43&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;场景描述&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;otsZyDOQ&quot;&gt;现有用户访问日志表 visit_log ，该表定义见 &lt;b&gt;快速制造测试数据&lt;/b&gt;。 如何获取最长的 2 个连续活跃，输出用户、最长连续活跃天数、最长连续活跃日期区间？&lt;/p&gt;&lt;p data-pid=&quot;xPgYZLXs&quot;&gt;上述问题在分析连续性时，获取连续性的结果不是且无法与固定的阈值作比较，而是各自以最长连续活跃作为动态阈值，可归类为 &lt;b&gt;动态连续活跃场景统计&lt;/b&gt;。&lt;/p&gt;&lt;h3 id=&quot;h_508038040_44&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;SQL 实现&lt;/b&gt;&lt;/h3&gt;&lt;p data-pid=&quot;nBHgf8dR&quot;&gt;基于 &lt;b&gt;静态连续活跃场景统计&lt;/b&gt; 的思路进行扩展即可，此处直接给出最终 SQL ：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-- SQL - 18
select
    user_id, 
    -- 最长连续活跃天数
    datediff(max(dt), min(dt)) + 1 as cont_days,
    -- 最长连续活跃日期区间
    min(dt) as cont_date_start, max(dt) as cont_date_end
from (
    select 
        *, 
        date_add(dt, 1 - dense_rank() 
            over(partition by user_id order by dt)) as cont
    from visit_log
) t1
group by user_id, cont
order by cont_days desc
limit 2;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table data-draft-node=&quot;block&quot; data-draft-type=&quot;table&quot; data-size=&quot;normal&quot; data-row-style=&quot;normal&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;user_id&lt;/th&gt;&lt;th&gt;cont_days&lt;/th&gt;&lt;th&gt;cont_date_start&lt;/th&gt;&lt;th&gt;cont_date_end&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;101&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2022-01-01&lt;/td&gt;&lt;td&gt;2022-01-03&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;102&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2022-01-01&lt;/td&gt;&lt;td&gt;2022-01-02&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;h_508038040_45&quot; data-into-catalog-status=&quot;&quot;&gt;&lt;b&gt;结语&lt;/b&gt;&lt;/h2&gt;&lt;p data-pid=&quot;_S7ZgFKU&quot;&gt;&lt;b&gt;通过灵活的、散发性的数据处理思维，就可以用最简单的语法，解决复杂的数据场景&lt;/b&gt; 是本篇文章贯穿全文的思想。文中针对数列生成、区间变换、排列组合、连续判别等常见的场景，给出了相对通用的解决方案，并结合实例进行了实际运用的讲解。&lt;/p&gt;&lt;p data-pid=&quot;LyOINeuF&quot;&gt;本篇文章尝试独辟蹊径，强调灵活的数据处理思维，希望能让读者觉得眼前一亮，更希望真的能给读者产生帮助。同时毕竟个人能力有限，思路不一定是最优的，甚至可能出现错误，欢迎提出意见或建议。为了便于交流探讨，文中的每个 SQL 都标记了编号，可以直接在评论区 @SQL编号 沟通。&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>7673d947a67377feb1af45a57d9292b6</guid>
<title>DataX 快速入门</title>
<link>https://toutiao.io/k/ltbtbsk</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;DataX 版本：3.0&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;Github主页地址：https://github.com/alibaba/DataX&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;DataX 是一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP 等各种异构数据源之间稳定高效的数据同步功能。具体请查阅：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI0OTYwNTQ4Ng==&amp;amp;mid=2247485711&amp;amp;idx=1&amp;amp;sn=8910e6917b9fb0ad77081e1af36b07af&amp;amp;scene=21#wechat_redirect&quot; data-linktype=&quot;2&quot;&gt;DataX 异构数据源离线同步&lt;/a&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;1. 环境要求&lt;/h2&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;Linux&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;JDK(1.8 以上，推荐 1.8)&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Python(推荐 Python2.6.X)&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2. 下载&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;直接下载 DataX 工具包：下载地址。下载后解压至本地 /opt 目录下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;tar -zxvf datax.tar.gz -C /opt/&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;除了直接下载安装包之外，你也可以下载 DataX 源码，自己编译&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;进入 bin 目录，即可运行同步作业。可以运行如下自查脚本检查安装是否成功：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;python {YOUR_DATAX_HOME}/bin/datax.py {YOUR_DATAX_HOME}/job/job.json&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在我们这需要运行如下语句：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;python /opt/datax/bin/datax.py /opt/datax/job/job.json&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;看到如下运行信息表示我们已经运行成功了：&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-ratio=&quot;0.0984375&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/nKovjAe6Lrq9O0Uj9O4MPAYIBYiaHMfFsHQxDexynVL6HyzpjDTe8Q1W0IkClic7vs0DvQqPjxs3g6NYE0czvGeA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3. 示例&lt;br/&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们第一个简单示例是从 Stream 读取数据并打印到控制台。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3.1 查看配置模板&lt;span/&gt;&lt;/h3&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;配置文件为 json 格式&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;DataX 为不同的 Reader 和 Writer 分别提供了不同的配置模块，可以通过如下命令指定 Reader 和 Writer 查看配置模板：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;python {YOUR_DATAX_HOME}/bin/datax.py -r {YOUR_READER} -w {YOUR_WRITER}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在这我们需要运行如下语句：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;python /opt/datax/bin/datax.py -r streamreader -w streamwriter&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;输出信息中包含了如下配置模板 JSON：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;{&lt;br/&gt;    &lt;span&gt;&quot;job&quot;&lt;/span&gt;: {&lt;br/&gt;        &lt;span&gt;&quot;content&quot;&lt;/span&gt;: [&lt;br/&gt;            {&lt;br/&gt;                &lt;span&gt;&quot;reader&quot;&lt;/span&gt;: {&lt;br/&gt;                    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamreader&quot;&lt;/span&gt;,&lt;br/&gt;                    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;                        &lt;span&gt;&quot;column&quot;&lt;/span&gt;: [],&lt;br/&gt;                        &lt;span&gt;&quot;sliceRecordCount&quot;&lt;/span&gt;: &lt;span&gt;&quot;&quot;&lt;/span&gt;&lt;br/&gt;                    }&lt;br/&gt;                },&lt;br/&gt;                &lt;span&gt;&quot;writer&quot;&lt;/span&gt;: {&lt;br/&gt;                    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamwriter&quot;&lt;/span&gt;,&lt;br/&gt;                    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;                        &lt;span&gt;&quot;encoding&quot;&lt;/span&gt;: &lt;span&gt;&quot;&quot;&lt;/span&gt;,&lt;br/&gt;                        &lt;span&gt;&quot;print&quot;&lt;/span&gt;: &lt;span&gt;true&lt;/span&gt;&lt;br/&gt;                    }&lt;br/&gt;                }&lt;br/&gt;            }&lt;br/&gt;        ],&lt;br/&gt;        &lt;span&gt;&quot;setting&quot;&lt;/span&gt;: {&lt;br/&gt;            &lt;span&gt;&quot;speed&quot;&lt;/span&gt;: {&lt;br/&gt;                &lt;span&gt;&quot;channel&quot;&lt;/span&gt;: &lt;span&gt;&quot;&quot;&lt;/span&gt;&lt;br/&gt;            }&lt;br/&gt;        }&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;从配置模板中可以看到配置文件需要配置三部分&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;配置同步任务的读取端 reader：配置同步任务的读取端数据信息&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;配置同步任务的写入端 writer：配置同步任务的写入端数据信息&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;配置通道控制 setting：配置同步任务全局信息（不包含读取端、写入端外配置信息）。你可以在 setting 中进行同步速率配置，新版本DataX 3.0 提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度。此外还提供了脏数据探测能力，可以实现脏数据精确过滤、识别、采集、展示，为用户提供多种的脏数据处理模式。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3.2 根据模板编写配置文件&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;(1) 配置同步任务的读取端&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;通过配置模板已生成了基本的读取端配置。此时你可以继续手动配置同步任务的读取端数据信息，如下所示输出5条记录，第一个字段数据类型为 Long 的 10，第二个字段数据类型为 String 的 &lt;code&gt;hello，DataX&lt;/code&gt;：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&quot;reader&quot;&lt;/span&gt;: {&lt;br/&gt;    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamreader&quot;&lt;/span&gt;,&lt;br/&gt;    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;        &lt;span&gt;&quot;column&quot;&lt;/span&gt;: [&lt;br/&gt;          {&lt;br/&gt;              &lt;span&gt;&quot;type&quot;&lt;/span&gt;: &lt;span&gt;&quot;long&quot;&lt;/span&gt;,&lt;br/&gt;              &lt;span&gt;&quot;value&quot;&lt;/span&gt;: &lt;span&gt;&quot;10&quot;&lt;/span&gt;&lt;br/&gt;          },&lt;br/&gt;          {&lt;br/&gt;              &lt;span&gt;&quot;type&quot;&lt;/span&gt;: &lt;span&gt;&quot;string&quot;&lt;/span&gt;,&lt;br/&gt;              &lt;span&gt;&quot;value&quot;&lt;/span&gt;: &lt;span&gt;&quot;hello，DataX&quot;&lt;/span&gt;&lt;br/&gt;          }&lt;br/&gt;        ],&lt;br/&gt;        &lt;span&gt;&quot;sliceRecordCount&quot;&lt;/span&gt;: &lt;span&gt;&quot;5&quot;&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;(2) 配置同步任务的写入端&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;配置完成读取端数据信息后，可以继续手动配置同步任务的写入端数据信息：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&quot;writer&quot;&lt;/span&gt;: {&lt;br/&gt;    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamwriter&quot;&lt;/span&gt;,&lt;br/&gt;    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;        &lt;span&gt;&quot;encoding&quot;&lt;/span&gt;: &lt;span&gt;&quot;UTF-8&quot;&lt;/span&gt;,&lt;br/&gt;        &lt;span&gt;&quot;print&quot;&lt;/span&gt;: &lt;span&gt;true&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;(3) 配置通道控制&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当上述步骤配置完成后，则需要配置同步速率：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&quot;setting&quot;&lt;/span&gt;: {&lt;br/&gt;    &lt;span&gt;&quot;speed&quot;&lt;/span&gt;: {&lt;br/&gt;        &lt;span&gt;&quot;channel&quot;&lt;/span&gt;: &lt;span&gt;&quot;1&quot;&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;创建配置文件 stream2stream.json 并放入 /opt/datax/job/ 目录下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;{&lt;br/&gt;    &lt;span&gt;&quot;job&quot;&lt;/span&gt;: {&lt;br/&gt;        &lt;span&gt;&quot;content&quot;&lt;/span&gt;: [&lt;br/&gt;            {&lt;br/&gt;                &lt;span&gt;&quot;reader&quot;&lt;/span&gt;: {&lt;br/&gt;                    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamreader&quot;&lt;/span&gt;,&lt;br/&gt;                    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;                        &lt;span&gt;&quot;column&quot;&lt;/span&gt;: [&lt;br/&gt;                          {&lt;br/&gt;                              &lt;span&gt;&quot;type&quot;&lt;/span&gt;: &lt;span&gt;&quot;long&quot;&lt;/span&gt;,&lt;br/&gt;                              &lt;span&gt;&quot;value&quot;&lt;/span&gt;: &lt;span&gt;&quot;10&quot;&lt;/span&gt;&lt;br/&gt;                          },&lt;br/&gt;                          {&lt;br/&gt;                              &lt;span&gt;&quot;type&quot;&lt;/span&gt;: &lt;span&gt;&quot;string&quot;&lt;/span&gt;,&lt;br/&gt;                              &lt;span&gt;&quot;value&quot;&lt;/span&gt;: &lt;span&gt;&quot;hello，DataX&quot;&lt;/span&gt;&lt;br/&gt;                          }&lt;br/&gt;                        ],&lt;br/&gt;                        &lt;span&gt;&quot;sliceRecordCount&quot;&lt;/span&gt;: &lt;span&gt;&quot;5&quot;&lt;/span&gt;&lt;br/&gt;                    }&lt;br/&gt;                },&lt;br/&gt;                &lt;span&gt;&quot;writer&quot;&lt;/span&gt;: {&lt;br/&gt;                    &lt;span&gt;&quot;name&quot;&lt;/span&gt;: &lt;span&gt;&quot;streamwriter&quot;&lt;/span&gt;,&lt;br/&gt;                    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;: {&lt;br/&gt;                        &lt;span&gt;&quot;encoding&quot;&lt;/span&gt;: &lt;span&gt;&quot;UTF-8&quot;&lt;/span&gt;,&lt;br/&gt;                        &lt;span&gt;&quot;print&quot;&lt;/span&gt;: &lt;span&gt;true&lt;/span&gt;&lt;br/&gt;                    }&lt;br/&gt;                }&lt;br/&gt;            }&lt;br/&gt;        ],&lt;br/&gt;        &lt;span&gt;&quot;setting&quot;&lt;/span&gt;: {&lt;br/&gt;            &lt;span&gt;&quot;speed&quot;&lt;/span&gt;: {&lt;br/&gt;                &lt;span&gt;&quot;channel&quot;&lt;/span&gt;: &lt;span&gt;&quot;1&quot;&lt;/span&gt;&lt;br/&gt;            }&lt;br/&gt;        }&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;3.3 运行&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;直接运行如下命令：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;/opt/datax/bin/datax.py /opt/datax/job/stream2stream.json&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;输出如下信息：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;DataX (DATAX-OPENSOURCE-3.0), From Alibaba !&lt;br/&gt;Copyright (C) 2010-2017, Alibaba Group. All Rights Reserved.&lt;br/&gt;&lt;br/&gt;2022-04-30 23:19:42.460 [main] INFO  VMInfo - VMInfo&lt;span&gt;# operatingSystem class =&amp;gt; sun.management.OperatingSystemImpl&lt;/span&gt;&lt;br/&gt;2022-04-30 23:19:42.469 [main] INFO  Engine - the machine info  =&amp;gt;&lt;br/&gt;&lt;br/&gt;...&lt;br/&gt;&lt;br/&gt;2022-04-30 23:19:42.492 [main] INFO  Engine -&lt;br/&gt;{&lt;br/&gt; &lt;span&gt;&quot;content&quot;&lt;/span&gt;:[&lt;br/&gt;  {&lt;br/&gt;   &lt;span&gt;&quot;reader&quot;&lt;/span&gt;:{&lt;br/&gt;    &lt;span&gt;&quot;name&quot;&lt;/span&gt;:&lt;span&gt;&quot;streamreader&quot;&lt;/span&gt;,&lt;br/&gt;    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;:{&lt;br/&gt;     &lt;span&gt;&quot;column&quot;&lt;/span&gt;:[&lt;br/&gt;      {&lt;br/&gt;       &lt;span&gt;&quot;type&quot;&lt;/span&gt;:&lt;span&gt;&quot;long&quot;&lt;/span&gt;,&lt;br/&gt;       &lt;span&gt;&quot;value&quot;&lt;/span&gt;:&lt;span&gt;&quot;10&quot;&lt;/span&gt;&lt;br/&gt;      },&lt;br/&gt;      {&lt;br/&gt;       &lt;span&gt;&quot;type&quot;&lt;/span&gt;:&lt;span&gt;&quot;string&quot;&lt;/span&gt;,&lt;br/&gt;       &lt;span&gt;&quot;value&quot;&lt;/span&gt;:&lt;span&gt;&quot;hello，DataX&quot;&lt;/span&gt;&lt;br/&gt;      }&lt;br/&gt;     ],&lt;br/&gt;     &lt;span&gt;&quot;sliceRecordCount&quot;&lt;/span&gt;:&lt;span&gt;&quot;5&quot;&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;   },&lt;br/&gt;   &lt;span&gt;&quot;writer&quot;&lt;/span&gt;:{&lt;br/&gt;    &lt;span&gt;&quot;name&quot;&lt;/span&gt;:&lt;span&gt;&quot;streamwriter&quot;&lt;/span&gt;,&lt;br/&gt;    &lt;span&gt;&quot;parameter&quot;&lt;/span&gt;:{&lt;br/&gt;     &lt;span&gt;&quot;encoding&quot;&lt;/span&gt;:&lt;span&gt;&quot;UTF-8&quot;&lt;/span&gt;,&lt;br/&gt;     &lt;span&gt;&quot;print&quot;&lt;/span&gt;:&lt;span&gt;true&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;   }&lt;br/&gt;  }&lt;br/&gt; ],&lt;br/&gt; &lt;span&gt;&quot;setting&quot;&lt;/span&gt;:{&lt;br/&gt;  &lt;span&gt;&quot;speed&quot;&lt;/span&gt;:{&lt;br/&gt;   &lt;span&gt;&quot;channel&quot;&lt;/span&gt;:&lt;span&gt;&quot;1&quot;&lt;/span&gt;&lt;br/&gt;  }&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;2022-04-30 23:19:42.511 [main] WARN  Engine - prioriy &lt;span&gt;set&lt;/span&gt; to 0, because NumberFormatException, the value is: null&lt;br/&gt;2022-04-30 23:19:42.513 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=&lt;span&gt;false&lt;/span&gt;, priority=0&lt;br/&gt;2022-04-30 23:19:42.513 [main] INFO  JobContainer - DataX jobContainer starts job.&lt;br/&gt;2022-04-30 23:19:42.515 [main] INFO  JobContainer - Set jobId = 0&lt;br/&gt;2022-04-30 23:19:42.530 [job-0] INFO  JobContainer - jobContainer starts to &lt;span&gt;do&lt;/span&gt; prepare ...&lt;br/&gt;2022-04-30 23:19:42.531 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] &lt;span&gt;do&lt;/span&gt; prepare work .&lt;br/&gt;2022-04-30 23:19:42.532 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] &lt;span&gt;do&lt;/span&gt; prepare work .&lt;br/&gt;2022-04-30 23:19:42.532 [job-0] INFO  JobContainer - jobContainer starts to &lt;span&gt;do&lt;/span&gt; split ...&lt;br/&gt;2022-04-30 23:19:42.532 [job-0] INFO  JobContainer - Job &lt;span&gt;set&lt;/span&gt; Channel-Number to 1 channels.&lt;br/&gt;2022-04-30 23:19:42.532 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] splits to [1] tasks.&lt;br/&gt;2022-04-30 23:19:42.533 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] splits to [1] tasks.&lt;br/&gt;2022-04-30 23:19:42.549 [job-0] INFO  JobContainer - jobContainer starts to &lt;span&gt;do&lt;/span&gt; schedule ...&lt;br/&gt;2022-04-30 23:19:42.554 [job-0] INFO  JobContainer - Scheduler starts [1] taskGroups.&lt;br/&gt;2022-04-30 23:19:42.556 [job-0] INFO  JobContainer - Running by standalone Mode.&lt;br/&gt;2022-04-30 23:19:42.563 [taskGroup-0] INFO  TaskGroupContainer - taskGroupId=[0] start [1] channels &lt;span&gt;for&lt;/span&gt; [1] tasks.&lt;br/&gt;2022-04-30 23:19:42.567 [taskGroup-0] INFO  Channel - Channel &lt;span&gt;set&lt;/span&gt; byte_speed_limit to -1, No bps activated.&lt;br/&gt;2022-04-30 23:19:42.567 [taskGroup-0] INFO  Channel - Channel &lt;span&gt;set&lt;/span&gt; record_speed_limit to -1, No tps activated.&lt;br/&gt;2022-04-30 23:19:42.580 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started&lt;br/&gt;10 hello，DataX&lt;br/&gt;10 hello，DataX&lt;br/&gt;10 hello，DataX&lt;br/&gt;10 hello，DataX&lt;br/&gt;10 hello，DataX&lt;br/&gt;2022-04-30 23:19:42.685 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[107]ms&lt;br/&gt;2022-04-30 23:19:42.686 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] completed it&lt;span&gt;&#x27;s tasks.&lt;br/&gt;2022-04-30 23:19:52.573 [job-0] INFO  StandAloneJobContainerCommunicator - Total 5 records, 65 bytes | Speed 6B/s, 0 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%&lt;br/&gt;2022-04-30 23:19:52.573 [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.&lt;br/&gt;2022-04-30 23:19:52.574 [job-0] INFO  JobContainer - DataX Writer.Job [streamwriter] do post work.&lt;br/&gt;2022-04-30 23:19:52.574 [job-0] INFO  JobContainer - DataX Reader.Job [streamreader] do post work.&lt;br/&gt;2022-04-30 23:19:52.575 [job-0] INFO  JobContainer - DataX jobId [0] completed successfully.&lt;br/&gt;2022-04-30 23:19:52.576 [job-0] INFO  HookInvoker - No hook invoked, because base dir not exists or is a file: /opt/datax/hook&lt;br/&gt;2022-04-30 23:19:52.579 [job-0] INFO  JobContainer -&lt;br/&gt;  [total cpu info] =&amp;gt;&lt;br/&gt;  averageCpu                     | maxDeltaCpu                    | minDeltaCpu&lt;br/&gt;  -1.00%                         | -1.00%                         | -1.00%&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;  [total gc info] =&amp;gt;&lt;br/&gt;   NAME                 | totalGCCount       | maxDeltaGCCount    | minDeltaGCCount    | totalGCTime        | maxDeltaGCTime     | minDeltaGCTime&lt;br/&gt;   PS MarkSweep         | 0                  | 0                  | 0                  | 0.000s             | 0.000s             | 0.000s&lt;br/&gt;   PS Scavenge          | 0                  | 0                  | 0                  | 0.000s             | 0.000s             | 0.000s&lt;br/&gt;&lt;br/&gt;2022-04-30 23:19:52.580 [job-0] INFO  JobContainer - PerfTrace not enable!&lt;br/&gt;2022-04-30 23:19:52.580 [job-0] INFO  StandAloneJobContainerCommunicator - Total 5 records, 65 bytes | Speed 6B/s, 0 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%&lt;br/&gt;2022-04-30 23:19:52.581 [job-0] INFO  JobContainer -&lt;br/&gt;任务启动时刻                    : 2022-04-30 23:19:42&lt;br/&gt;任务结束时刻                    : 2022-04-30 23:19:52&lt;br/&gt;任务总计耗时                    :                 10s&lt;br/&gt;任务平均流量                    :                6B/s&lt;br/&gt;记录写入速度                    :              0rec/s&lt;br/&gt;读出记录总数                    :                   5&lt;br/&gt;读写失败总数                    :                   0&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>d1cca609cf9118cbf30f94e971dfba94</guid>
<title>这5个字，能优化你80%的程序性能问题</title>
<link>https://toutiao.io/k/m9sc96j</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzkwOTIxNDQ3OA==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8YRIaicYx5pzj5Cxwick8DamnOgbTJu96QTibKyHEDZt1815yOV1r27oZ6HgoYTEYWYLRz4jIV4iasHgg/0?wx_fmt=png&quot; data-nickname=&quot;dbaplus社群&quot; data-alias=&quot;dbaplus&quot; data-signature=&quot;围绕Database、BigData、AIOps的企业级专业社群。资深大咖、技术干货，每天精品原创文章推送，每周线上技术分享，每月线下技术沙龙，每季度Gdevops&amp;amp;DAMS行业大会.&quot; data-from=&quot;0&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;444&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.7818181818181819&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134HDibbVHONr1DabWeezyfqmMGed1jgOvjBBpaf4yxkElRrF4he0polhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;990&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;五字优化诀：持续分煎饼&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;本篇关注程序性能优化。聚焦这个主题，本是偶然。始于玩笑，终于本心，也算是为我党成立百年献礼了。本想找点高大上的让人直呼牛逼的东西，奈何能力有限，只能给大家一些既便宜、又好用、还简单的普通东西了，不知道你们会不会喜欢。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分为五个主题，分别是『池』『序』『分』『减』『并』：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;一、『池』字诀&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;344&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.60625&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134wgF6JLV12dsXvUpvjrFOPUVgb2UlwEJmCb7wGSJALwslndUbH4QDrw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;池化，降低可重用对象的创建和回收代价。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不知道你们发现没有，无论是电影还是游戏中，主角总是孤胆单英雄，最多三五成群。但Boss不一样，Boss手一挥，必须有一群小怪一拥而上，毕竟帮主角刷点经验也是好的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;小怪的特点是：数量多，容易死，循环用。电影不可能请太多的群演，因此我们经常能发现一人分饰多角的超级龙套。而游戏里，也不可能每一个小怪都完全不一样，因为创建它们还挺消耗时间和内存的。哈哈，现在你知道了，你正在打的小怪很可能与刚刚死前掉金币的那只是同一只。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码中，如果某些对象有重用的价值，并且创建的时候会消耗大量的CPU或IO资源。那么在出现性能瓶颈的时候，一个合理的优化方向就是池化。刚刚例子中，对游戏中小怪进行池化，通常称为对象池，类似的还有线程池、连接池等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;paragraph&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section label=&quot;Copyright © 2015 Yead All Rights Reserved.&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;灰太狼：我一定会回来的~&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;二、『序』字诀&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;顺序读写，减少随机IO，减少cache miss。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;『内存顺序读写的性能要远好于随机读写』，『磁盘顺序读写的性能要远好于随机读写』。类似的话我相信很多程序员都似曾相识，然而，我同样相信很多程序员在写代码的时候从来没有认真考虑过这件事件。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;351&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.6183035714285714&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134bW0DNy9BFMVuMAbLblVaPHYLeX4GkMQapasaNBG8CZxAWRVRVbaXgQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;896&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当年做游戏的时候，我见过很多人使用hash表存储场景中的各类对象：花鸟鱼虫、白云苍狗，并每帧遍历hash表以确定位置或攻击信息。我建议他们改成遍历有序表的快照（MVCC了解一下），其中一个原因是可以提高遍历性能，而另一个更重要的原因是可以在遍历的过程中修改表结构（插入删除对象）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;paragraph&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section label=&quot;Copyright © 2015 Yead All Rights Reserved.&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;有序表一种基于有序数组的字典结构，C#中有一个名为SortList的标准库实现&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;多年以来，CPU一直是计算机中跑得最快的部件，也因此被惯出来一个多吃多占的毛病。无论是读内存还是读磁盘，从来不讲究按需分配，而是大块大块的拿数据。当把一大块连续的数据拿到手里的时候，CPU自己也知道一次肯定吞不下，但总觉得多吞几次就好了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但这个特点其实是需要程序员去配合的，如果代码使用连续内存的数据结构，比如array，那在遍历的时候就相当于投其所好；而如果代码使用hash表，则遍历的时候cache miss的可能性就大大增加。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;鉴于java在服务器领域的成功，有些公司使用java开发游戏服务器，我建议他们换一种语言。原因是游戏服务器很可能需要处理大量跟点、向量等三维空间相关的运算，而在java中，默认一切都是对象。于是在一个Vertex（顶点）数组中，看似连续的Vertex对象在物理内存中实际上是离散的，这样的遍历效果就会差很多。而对C++, Golang，甚至C#这类语言来说，它们都支持struct。当把Vertex定义为struct的时候，一个Vertex数组的占用的内存就是连续的，遍历效果会比java好很多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在后端的技术栈中，kafka绝对是一个非常另类的存在。擅长kafka的人，觉得无它不欢，而不了解它的人，则觉得可有可无。关于kafka为什么这么快的讨论有很多，其中有一个绕不过的原因是：kafka会顺序读写磁盘。我们通常认为磁盘的读写性能远低于内存，但实际上，在关闭fsync的前提下，SSD固态硬盘的顺序读写速度与内存的随机读写速度是相当的，大概都是1 GiB/s，而如果是随机读取，则SSD固态硬盘SSD的Seek速度会直降到70MiB/s，速度下降到1/15。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;三、『分』字诀&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;鸿蒙伊始，民智不开。为教化万民，神器计算机降世。下凡走得急，零件没凑齐。计算机天生残疾，CPU与其它IO部件速度差异过大。为平衡这种速度差异，人间有智者布局各方，分字诀应劫而生，它们是分批、分帧、分页、分时、分片、分区、分库、分表、分离。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86152&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;span data-bgopacity=&quot;40%&quot;/&gt; &lt;span data-bgopacity=&quot;25%&quot;/&gt; &lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-brushtype=&quot;text&quot;&gt;&lt;span&gt;&lt;strong&gt;1. 分批≈缓存+缓冲&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;317&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.55859375&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134feibULNmUN0XiaqzCJfmRBacz80vtKj6mmzOnOsrRfxeUQQrCs6gzHJg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在系统设计之初，每次修改对应一次IO可能是最简单、最直接的设计，不过随着系统流量变大，IO可能会很快成为系统瓶颈。相比于内存操作来说，磁盘IO吞吐小，网络IO延迟大。为了减少IO与内存之间的速度差异，争取每次IO都物尽其用才是上上之选。将多次IO合并为一次，减少磁盘IO（特别是fsync）的次数和网络IO的round-trip次数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所谓读优化靠缓存，写优化靠缓冲，而分批≈缓存+缓冲。往小了说，缓存就是一块用于读内存，缓冲就是一块用于写的内存。往大了说，缓存就是Redis，缓冲就是Kafka。这些都是在微服务体系中司空见惯的招数，不用我多说。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分批优化化身千万，可谓无孔不入，甚至多数流行的数据中间件都提供了批量IO操作的API：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;&gt;&lt;section data-role=&quot;list&quot;&gt;&lt;section data-role=&quot;list&quot;&gt;&lt;section data-role=&quot;list&quot;&gt;&lt;section data-role=&quot;list&quot;&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;list&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分批的缺点是在数据一致性差，无论是缓存还是缓冲都存在同样的问题。缓存的话，如果不存在严格的一致保障手段，往往只建议展示用，不作为数据修改依据。参考文章《缓存就像showgirl，看看就行了》。而缓冲在内存的这段时间内，如果进程崩溃了，会丢失部分数据。因此在选择分批优化的时候，架构师需要仔细斟酌数据一致性问题，比如是否可以接受偶尔丢失数据，或者是否需要在数据输入端提供重试策略。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但不管如何，分批都可能是最重要的IO优化手段：它逻辑简单，不涉及多线程并发，对数据结构没有特殊的要求，系统改造成本低。可以说，无论何时遇到IO性能瓶颈，分批改造都应该是顺位第一的候选方案。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86152&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;span data-bgopacity=&quot;40%&quot;/&gt; &lt;span data-bgopacity=&quot;25%&quot;/&gt; &lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-brushtype=&quot;text&quot;&gt;&lt;span&gt;&lt;strong&gt;2. 分帧、分页、分时&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）分帧，是专用于单线程+阻塞IO的平滑技术&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多时候，由于受框架限制（比如游戏或网页渲染），我们不得不在单一线程中同时处理用户逻辑与IO操作，这时如果IO消耗时间过长，就会阻塞用户逻辑代码，从而让用户感觉到卡顿现象。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Redis由于其单线程特性，很多耗时比较长的操作也需要分摊到多帧执行。比如hash表的rehash操作，当表的键值对过多时，rehash会产生的庞大的计算量，如果一次性完成可能会导致server对外暂停服务。Redis选择将rehash分摊到多次执行，称为渐进式rehash。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另外，对于比较大的hash表，hgetall一次性获取所有数据可能会卡死server进程甚至导致宕机，建议采用hscan来分次获取hash表中的数据。Redis中像这类支持迭代式扫描的命令有四个，分别是：scan, sscan, hscan和zscan。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）分页可以看作是一种另类的分帧操作&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在运营类项目中，由于后台数据庞大，很多时候无法在单一网页中显示，通过分页显示，可以避免一次性数据获取带来的DB加载压力和网络传输压力。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）分时复用指多对象轮流使用同一个硬件的技术，多见于跟硬件打交道的底层软件。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;比如，分时操作系统：一种采用时间片轮转的方式同时为几个甚至几百个用户服务的操作系统；分时复用网络：指采用同一物理连接的不同时段来传输不同的信号，以达到多路传输目的的网络基础设施。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但有一种分时复用技术，虽然它的名字里没有分时二字，却与后端开发息息相关，那就是IO多路复用（洋名：Reactor）：单线程同时监听多个文件句柄，哪个句柄就绪，就通知应用线程读写哪个句柄的技术。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86152&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;span data-bgopacity=&quot;40%&quot;/&gt; &lt;span data-bgopacity=&quot;25%&quot;/&gt; &lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-brushtype=&quot;text&quot;&gt;&lt;span&gt;&lt;strong&gt;3. 分片、分区、分库、分表&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;381&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.67109375&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134q2Z9oyq9jOwHDIqI0AWH2eiaamngT9AuEUYxwxQOTwmHXWclP1TX00A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;随着77年恢复高考，这些年近视的年轻人越来越多了，于是人们越发的无法区分那些换个马夹儿就重新出来混的二货们。就像洗发水，猛一看飘柔、海飞丝、潘婷百花齐放，仔细一看，全特么是宝洁的。没错，这么土味的名字，竟然不是国货，害我自以为爱了这么多年的国。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;分片、分区、分库、分表也一样，名字挺花，疗效一样，都是为了突破单体性能限制的水平扩展的方案，洋名字：scale out。因为方案类似，大家遇到的问题自然也是相同的。它们首先要搞定的就是路由问题，也就是把数据拆分之后，某个key储存到哪个分片/区/库/表的问题。路由方案可简单分为两类：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一种是非确定性路由，即相同的key多次路由可以映射到的不同的计算单元，常见方案有：轮询、随机。非确定性路由多用于无状态节点间的任务分配，比如nginx把请求随机分配到无状态的微服务节点上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一种是确定性路由，即相同的key多次路由必须映射到的相同的存储单元，常见方案为：区间，Hash，配置表。确定性路由多用于有状态节点间的任务分配，比如Kafka按user_id把来自同一用户的请求映射到同一个存储分区。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以MySQL为例，它支持四种分区类型，分别是Range, List, Hash, Key。因为跟存储密切相关，它们全是确定性路由算法，其中Range对应区间，List是配置表，Hash与Key则都是Hash类型。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了应用于多机水平扩展，在单机内存中分片方案也有应用。比如JDK1.8之前，ConcurrentHashMap通过将整个Map划分成N（默认16个）个Segment，而每个Segment各自持有独立的锁，从而从整体上减少并发冲突。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86152&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;span data-bgopacity=&quot;40%&quot;/&gt; &lt;span data-bgopacity=&quot;25%&quot;/&gt; &lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;section data-brushtype=&quot;text&quot;&gt;&lt;span&gt;&lt;strong&gt;4. 分离&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;分离式设计是一种架构模式，通过把单元功能单一化、纯粹化、专业化，可以降低开发和维护成本，同时提高功能单元的可复用性，这在设计模式中我们通常称之为单一职责。目前，常见的分离式架构设计有读写分离、存算分离。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;读写分离在国人的文章中常用于指代MySQL写走主库，而读走从库，这有些狭义了。广义上读写分离的重点是：读路径不关心写，写路径不关心读，两者均关注于自己的功能实现，而毋需为对方的作出任何牺牲或让步。更多的细节我在文章《23.kafka心中的事件溯源》中有更详细的描述，感兴趣的读者可以点击查看。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;426&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134A350DiaS9xB26faib8jW8A9fWwjMGIxIicxJ4nJVlkc6GIia7VicGzicr9lQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;660&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里我想强调的是，读写分离是分离式DB（Unbunding Databases）的雏形。我们应该认识到，不存在一种单一的数据模型可以满足所有的访问模式。MySQL在线业务、Redis加速查询、ES全文索引、DW离线分析，每一种衍生数据系统都有各自不可替代作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如上图所示，通过统一写端，派生读端，可以形成一种遵循unix传统的架构模型：单一任务做好单一事情，内部通过低级API（pipe）通信，外部通过高级语言（shell）组合。在分离式DB架构中，目前看来最合适的，能起到粘接剂作用的是Event Stream（Event Log）。期望未来有那么一天，我们能像在shell中写ps | grep java一样，写出mysql | elasticsearch这样的代码，届时就是分离式DB摘取王之桂冠的荣耀时刻。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果说读写分离是拆功能，那么存算分离就是拆资源：把计算资源（CPU、内存）和存储资源（磁盘）拆分开来。早期的云DB，其实是把单体DB搬到云上。人们很快发现云DB与单机DB的不同之处：一是随着企业数字化转型的深入，数量总量飙升，单机存储捉襟见肘；二是在应对双十一这类突发性流量时，计算峰值波动很大，这使得云DB对弹性伸缩能力要求极高。问题一可以通过分库分表这类trick的方式缓解，但问题二对原有单体DB『存算一体』的架构提出了挑战。于是，存算分离的架构应运而生，也就是云原生数据库架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;228&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.40185185185185185&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/ufWcjcomw8Z4nCx06DL4uOb58W1Wz134ZiaBlErw8r8Zf9tics06Bf4Kba3Hsj2ozTvcWusDkqu7Hkq5z8s3IKgw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;存算分离听起来很云端，似乎跟我们平时的工作关系很小。但其实有一类架构它就在我们身边，只是我们可能没有意识到它也是存算分离架构，那就是微服务架构。计算节点无状态，存储节点无计算；计算节点横向扩展，存储节点纵向扩展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;paragraph&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section label=&quot;Copyright © 2015 Yead All Rights Reserved.&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;就像Duck Test讲的：如果它看起来像鸭子、游泳像鸭子、叫声像鸭子，那么它可能就是只鸭子。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;四、『减』字诀&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;很多时候我们都讲不要过早优化，因为多数业务的初期，数据量都很少，任何设计都不太可能出现性能问题。反而业务上线后，由于进展不符合预期，导致调整业务逻辑的可能性更大。因此，怎么简单怎么来才是最优选择，更快的实现业务比业务跑得更快优先级更高。再者，你难道不觉得，就凭咱们拿的那万儿八千的工资，在非常合理的情况下，就不应该写出支撑千万并发的系统，嘛？&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;万一哪天系统开始出现性能瓶颈呢？该怎么办？恭喜哈，这是好事，说明公司赚钱了，更重要是通过你的系统赚钱了。首先，你要做的第一件事就是跟老板要经费，经费到手，万事不愁。然后，想办法把系统恢复到数据量小的时候，这不就又顺理成章的撑住了嘛？接着，经费没地方花了不是？总得找个地方放啊，我私下认为你的钱包就是一个不错的地方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;怎么把数据量再次变小呢？劝退用户是一个办法，但我估计老板可能太不乐意。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;一个更可行的选择是优化数据结构和算法。比如给DB建索引就是一个办法，同样的查询，同样是千万级的数据，有索引和无索引的查询速度千差万别，因为索引会极大减少扫描的数据行数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个选择是裁剪数据。就像Java的GC会定期清理垃圾一样，如果你发现DB里的数据大部分都是过期无效的，或者是基本上不会再查询到的数据，把过期数据归档，减少线上数据集的尺寸会是一个非常好的选择。该操作系统改造成本极低，对线上业务无影响，对数据后台则可以看心情逐步改造。所有相关人员都压力不大，但效果却是线上系统从此秒开，绝对骚得一批。其实前面提到的分片、分区、分库、分表，也都是在变相减小单位处理单元上的数据量，只不过它们改造成本高，实施难度大。特别是，在决定采用分表之前，应慎重考虑归档是不是一个更合理的选择。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;当年做手游，发现游戏在4k屏的手机上运行极慢，手机的GPU根本带不动，尝试了各种优化手段都不好使。谁曾想，最终解决问题的，竟然是降低游戏的输出分辨率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;算法上缩，物理上减，现在连国家不开始提倡65岁退休了，我们的老系统也一定能才坚持几年。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;五、『并』字诀&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;终于到了并发，一个大部分人都觉得应该有用，同时大部分人觉得用起来发怵的优化手段。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果说前面的『池』、『序』、『分』、『减』这几条顶多算工程技巧，凭着我们聪明的大脑&amp;amp;反复思考就有可能搞清楚的话，『并发』这一条就是一个学术问题。换句话说，即使经过几年的系统学习，也很少有人敢拍胸脯说自己的并发代码无 bug。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;并发真有这么难么？从编程语言的角度看，并发不就是多线程和死锁么？贤者大神们的文章已经解释得很清楚了，像《不环保的死锁：破解死锁，我们一般从下三路入手》、《线程安全，唯快不破》，只要规避掉死锁的几个必要条件，还不是怎么顺手怎么写？而且，运气好的话还可以无锁解决并发问题，不但准，而且快。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;然而真相远不是这么简单。我们知道DB是解决数据安全和数据一致性问题的集大成者，我们尝试从DB的角度来观察一下。DB事务有ACID四个属性，其中I是指隔离性Isolation，而它的研究核心就是并发问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;毛爷爷说事务物是运动发展的，我觉得他说的是对的。两年之前还没有新冠肺炎呢，今天它已经像吃饭喝水一样融入到了我们每个人的生活当中。在早期的ANSI SQL92标准中，涉及到的并发异象只有4种，然而发展到今天，常见的并发异象已经有7种之多，它们分别是：脏读、脏写、读倾斜（不可重复读）、写倾斜，不可重复读、幻读、更新丢失。每一种并发异象都有不同的原因，以及不同的解决方案，而这还不是全部。其实我很想稍微给大家科普一下这些异象相关的内容，但是我发现细节实在太多了。我盲猜后端的知识体系中，可能有一半都跟并发有关。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;paragraph&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section label=&quot;Copyright © 2015 Yead All Rights Reserved.&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;没人否定当年敲定SQL92标准的应该算DB专家吧？连当前ANSI的专家都没能完全搞清楚事情，谁敢告诉我他轻松就能搞定？&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;所谓单机并发榨硬件，多机并发扩上限（scale out）。当单线程服务遭遇性能瓶颈，同时相应的机器硬件还有富余的时候，进行多线程改造从而充分压榨硬件性能可能会是一个比较好的选择。相应的，当单机性能已经无法满足服务需要的时候，就需要进行分布式改造，通过水平扩容的方式提升整体服务能力。这两种思路，对应到『分』字诀中，恰好是分表与分库的区别：如果只是数据量上去了，CPU和内存压力都不大，那就分表再压榨一下；反之，如果流量大增，单机负载已经抗不住了，就可以考虑选择分库。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;『并』字诀好使，但并不好掌握。引入并发会极大增大代码复杂度，提高维持数据一致性的难度。就像分库分表一样，它的痛，只有用过的人才知道，因此，往往只会作为终极优化手段。不用则已，用则需要有面对困难的勇气和决心。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;86318&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-brushtype=&quot;text&quot;&gt;六、优化即置换&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;span&gt;作为一名程序员，你一定听过这样一句话：好的架构不是设计出来的，而是演化出来的。想要获得什么，就要付出代价，就像想要讨老婆，就得努力挣钱一样。优化会使代码逻辑变得复杂，流程变得混乱。因此简单设计，先上线，用钱堆，这些看起来很土的选择很多时候可能比盲目优化更能使我们远离漩涡。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但无论如何，经常的，持续的分煎饼是个好习惯。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-role=&quot;paragraph&quot; data-custom=&quot;#138bde&quot;&gt;&lt;section label=&quot;Copyright © 2015 Yead All Rights Reserved.&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;特别是山东煎饼，山东泰安的。&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;section&gt;&lt;span&gt;作者丨普通熊猫&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;来源丨公众号：吹牛拍码（ID：boasting-architect）&lt;/span&gt;&lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;dbaplus社群欢迎广大技术人员投稿，投稿邮箱：&lt;/span&gt;&lt;span&gt;editor@dbaplus.cn&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>0192fdd5ab54573ec3acbc433954f912</guid>
<title>解Bug之路-NAT引发的性能瓶颈-完整版​</title>
<link>https://toutiao.io/k/6mj7kgf</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content                                                                     &quot; id=&quot;js_content&quot;&gt;
            &lt;h1&gt;解Bug之路-NAT引发的性能瓶颈-完整版&lt;/h1&gt;&lt;p&gt;笔者最近解决了一个非常曲折的问题，从抓包开始一路排查到不同内核版本间的细微差异，最后才完美解释了所有的现象。在这里将整个过程写成博文记录下来，希望能够对读者有所帮助。(篇幅可能会有点长,耐心看完,绝对物有所值~)&lt;/p&gt;&lt;h2&gt;环境介绍&lt;/h2&gt;&lt;p&gt;先来介绍一下出问题的环境吧，调用拓扑如下图所示:&lt;/p&gt;&lt;h3&gt;调用拓扑图&lt;/h3&gt;&lt;p&gt;&lt;img data-ratio=&quot;0.534037558685446&quot; data-type=&quot;png&quot; data-w=&quot;1704&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJdVNBHuJgmbwFOQzJOfvic58axmXZlEICicicD89N5pMS6ibQ9eIMAd9Xkw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;合作方的多台机器用NAT将多个源ip映射成同一个出口ip 20.1.1.1，而我们内网将多个Nginx映射成同一个目的ip 30.1.1.1。这样，在防火墙和LVS之间，所有的请求始终是通过(20.1.1.1,30.1.1.1)这样一个ip地址对进行访问。&lt;br/&gt;同时还固定了一个参数，那就是目的端口号始终是443。&lt;br/&gt;&lt;img data-ratio=&quot;0.44040697674418605&quot; data-type=&quot;png&quot; data-w=&quot;1376&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJclhNBibg7kuahLWq3fNulMgbYIcoezAH3XJy4s5aTqic5yvSwsFdeictA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;短连接-HTTP1.0&lt;/h3&gt;&lt;p&gt;由于对方是采用短连接和Nginx进行交互的，而且采用的协议是HTTP-1.0。所以我们的Nginx在每个请求完成后，会主动关闭连接，从而造成有大量的TIME_WAIT。&lt;br/&gt;&lt;img data-ratio=&quot;0.41522988505747127&quot; data-type=&quot;png&quot; data-w=&quot;1392&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJmpKs9qgExfrPmicPErvmJ5XnlBMPcRNYx4ibuvzJwChaQVR8c1jCMxicw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;值得注意的是，TIME_WAIT取决于Server端和Client端谁先关闭这个Socket。所以Nginx作为Server端先关闭的话，也必然会产生TIME_WAIT。&lt;br/&gt;&lt;img data-ratio=&quot;0.6185852981969486&quot; data-type=&quot;png&quot; data-w=&quot;1442&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJV9lUd9Hk1icuL4vcKR0LkaGmpX03yvbSqviaeur9Xdrs7xwmsUG88u9A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;内核参数配置&lt;/h3&gt;&lt;p&gt;内核参数配置如下所示:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;cat /proc/sys/net/ipv4/tcp_tw_reuse 0&lt;br/&gt;cat /proc/sys/net/ipv4/tcp_tw_recycle 0&lt;br/&gt;cat /proc/sys/net/ipv4/tcp_timestamps 1&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中tcp_tw_recycle设置为0。这样，可以有效解决tcp_timestamps和tcp_tw_recycle在NAT情况下导致的连接失败问题。具体见笔者之前的博客:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;https://my.oschina.net/alchemystar/blog/3119992&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;Bug现场&lt;/h2&gt;&lt;p&gt;好了，介绍完环境，我们就可以正式描述Bug现场了。&lt;/p&gt;&lt;h3&gt;Client端大量创建连接异常,而Server端无法感知&lt;/h3&gt;&lt;p&gt;表象是合作方的应用出现大量的创建连接异常，而Server端确没有任何关于这些异常的任何异常日志，仿佛就从来没有出现过这些请求一样。&lt;br/&gt;&lt;img data-ratio=&quot;0.5&quot; data-type=&quot;png&quot; data-w=&quot;1692&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJQTy4hVXY7xZXXd63icf0GEMqe0alJPybbCiaUicC5ribfPAjWGFCzbkD2A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;LVS监控曲线&lt;/h3&gt;&lt;p&gt;出现问题后，笔者翻了下LVS对应的监控曲线，其中有个曲线的变现非常的诡异。如下图所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.4596354166666667&quot; data-type=&quot;png&quot; data-w=&quot;1536&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJSjtYOGKkUlJyjZreRcIOplRxZzjnEFxYy34vhGkWEb8BZSIjPpRBiaA/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;什么情况?看上去像建立不了连接了？但是虽然业务有大量的报错，依旧有很高的访问量,看日志的话，每秒请求应该在550向上!和这个曲线上面每秒只有30个新建连接是矛盾的！&lt;/p&gt;&lt;h3&gt;每天发生的时间点非常接近&lt;/h3&gt;&lt;p&gt;观察了几天后。发现，每天都在10点左右开始发生报错，同时在12点左右就慢慢恢复。&lt;br/&gt;&lt;img data-ratio=&quot;0.42961876832844575&quot; data-type=&quot;png&quot; data-w=&quot;1364&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJ55f9OsMiaHvtwCwVicClVf447XnRmHL4DCcIwtfmvo9vnKPqvelkTunA/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;感觉就像每天10点在做活动，导致流量超过了系统瓶颈，进而暴露出问题。而11:40之后，流量慢慢下降，系统才慢慢恢复。难道LVS这点量都撑不住?才550TPS啊？就崩溃了？&lt;/p&gt;&lt;h3&gt;难道是网络问题?&lt;/h3&gt;&lt;p&gt;难道就是传说中的网络问题？看了下监控，流量确实增加，不过只占了将近1/8的带宽，离打爆网络还远着呢。&lt;br/&gt;&lt;img data-ratio=&quot;0.42686170212765956&quot; data-type=&quot;png&quot; data-w=&quot;1504&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJibeSCfsibYAR1rSwBrgHgAF3Wbl0S1zuyyqicFCh6mcWfhKErYJAvdiayg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h2&gt;进行抓包&lt;/h2&gt;&lt;p&gt;不管三七二十一，先抓包吧!&lt;/p&gt;&lt;h3&gt;抓包结果&lt;/h3&gt;&lt;p&gt;在这里笔者给出一个典型的抓包结果:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;序号&lt;/th&gt;&lt;th&gt;时间&lt;/th&gt;&lt;th&gt;源地址&lt;/th&gt;&lt;th&gt;目的地址&lt;/th&gt;&lt;th&gt;源端口号&lt;/th&gt;&lt;th&gt;目的端口号&lt;/th&gt;&lt;th&gt;信息&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;09:57:30.60&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[FIN,ACK]Seq=507,Ack=2195,TSval=1164446830&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;09:57:30.64&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[FIN,ACK]Seq=2195,Ack=508,TSval=2149756058&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;09:57:30.64&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[ACK]Seq=508,Ack=2196,TSval=1164446863&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;09:59:22.06&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[SYN]Seq=0,TSVal=21495149222&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;09:59:22.06&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[ACK]Seq=1,Ack=1487349876,TSval=1164558280&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;09:59:22.08&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[RST]Seq=1487349876&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;上面抓包结果如下图所示,一开始33735-&amp;gt;443这个Socket四次挥手。在将近两分钟后又使用了同一个33735端口和443建立连接，443给33735回了一个莫名其妙的Ack,导致33735发了RST！&lt;br/&gt;&lt;img data-ratio=&quot;0.6396276595744681&quot; data-type=&quot;png&quot; data-w=&quot;1504&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJ4QkWiciaxFLqlb11QsB8RRmpPCPXqACEf5n15ahVl21C3R2RHA3OoicCg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;现象是怎么产生的?&lt;/h3&gt;&lt;p&gt;首先最可疑的是为什么发送了一个莫名其妙的Ack回来?笔者想到,这个Ack是WireShark给我计算出来的。为了我们方便，WireShark会根据Seq=0而调整Ack的值。事实上，真正的Seq是个随机数！有没有可能是WireShark在某些情况下计算错误？&lt;br/&gt;还是看看最原始的未经过加工的数据吧，于是笔者将wireshark的&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Relative sequence numbers&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;给取消了。取消后的抓包结果立马就有意思了!调整过后抓包结果如下所示:&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;序号&lt;/th&gt;&lt;th&gt;时间&lt;/th&gt;&lt;th&gt;源地址&lt;/th&gt;&lt;th&gt;目的地址&lt;/th&gt;&lt;th&gt;源端口号&lt;/th&gt;&lt;th&gt;目的端口号&lt;/th&gt;&lt;th&gt;信息&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;09:57:30.60&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[FIN,ACK]Seq=909296387,Ack=1556577962,TSval=1164446830&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;09:57:30.64&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[FIN,ACK]Seq=1556577962,Ack=909296388,TSval=2149756058&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;09:57:30.64&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[ACK]Seq=909296388,Ack=1556577963,TSval=1164446863&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;09:59:22.06&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[SYN]Seq=69228087,TSVal=21495149222&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;09:59:22.06&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;[ACK]Seq=909296388,Ack=1556577963,TSval=1164558280&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;09:59:22.08&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[RST]Seq=1556577963&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;看表中，四次挥手里面的Seq和Ack对应的值和三次回收中那个错误的ACK完全一致！也就是说，四次回收后，五元组并没有消失，而是在111.5s内还存活着！按照TCPIP状态转移图，只有TIME_WAIT状态才会如此。&lt;br/&gt;&lt;img data-ratio=&quot;0.5547866205305652&quot; data-type=&quot;png&quot; data-w=&quot;1734&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJbMlS0aRkT1QMUfauLbgxTdsBas5OKlFT91JK9yRGfM755AKMaEX68g/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;我们可以看看Linux关于TIME_WAIT处理的内核源码:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;    switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {&lt;br/&gt;    // 如果是TCP_TW_SYN，那么允许此SYN分节重建连接&lt;br/&gt;    // 即允许TIM_WAIT状态跃迁到SYN_RECV&lt;br/&gt;    case TCP_TW_SYN: {&lt;br/&gt;        struct sock *sk2 = inet_lookup_listener(dev_net(skb-&amp;gt;dev),&lt;br/&gt;                            &amp;amp;tcp_hashinfo,&lt;br/&gt;                            iph-&amp;gt;saddr, th-&amp;gt;source,&lt;br/&gt;                            iph-&amp;gt;daddr, th-&amp;gt;dest,&lt;br/&gt;                            inet_iif(skb));&lt;br/&gt;        if (sk2) {&lt;br/&gt;            inet_twsk_deschedule(inet_twsk(sk), &amp;amp;tcp_death_row);&lt;br/&gt;            inet_twsk_put(inet_twsk(sk));&lt;br/&gt;            sk = sk2;&lt;br/&gt;            goto process;&lt;br/&gt;        }&lt;br/&gt;        /* Fall through to ACK */&lt;br/&gt;    }&lt;br/&gt;    // 如果是TCP_TW_ACK，那么，返回记忆中的ACK,这和我们的现象一致&lt;br/&gt;    case TCP_TW_ACK:&lt;br/&gt;        tcp_v4_timewait_ack(sk, skb);&lt;br/&gt;        break;&lt;br/&gt;    // 如果是TCP_TW_RST直接发送RESET包&lt;br/&gt;    case TCP_TW_RST:&lt;br/&gt;        tcp_v4_send_reset(sk, skb);&lt;br/&gt;        inet_twsk_deschedule(inet_twsk(sk), &amp;amp;tcp_death_row);&lt;br/&gt;        inet_twsk_put(inet_twsk(sk));&lt;br/&gt;        goto discard_it;&lt;br/&gt;    // 如果是TCP_TW_SUCCESS则直接丢弃此包，不做任何响应&lt;br/&gt;    case TCP_TW_SUCCESS:;&lt;br/&gt;    }&lt;br/&gt;    goto discard_it;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上面的代码有两个分支，值得我们注意，一个是TCP_TW_ACK，在这个分支下，返回TIME_WAIT记忆中的ACK和我们的抓包现象一模一样。还有一个TCP_TW_SYN,它表明了在&lt;br/&gt;TIME_WAIT状态下，可以立马重用此五元组，跳过2MSL而达到SYN_RECV状态！&lt;br/&gt;&lt;img data-ratio=&quot;0.39526184538653364&quot; data-type=&quot;png&quot; data-w=&quot;1604&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJDibkvbM6TasuvpZZeINHibqegHWsib7EIvwxoXibaa1VsESjTwrdjLvWOg/640?wx_fmt=png&quot;/&gt;状态的迁移就在于tcp_timewait_state_process这个函数,我们着重看下想要观察的分支:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;enum tcp_tw_status&lt;br/&gt;tcp_timewait_state_process(struct inet_timewait_sock *tw, struct sk_buff *skb,&lt;br/&gt;               const struct tcphdr *th)&lt;br/&gt;{&lt;br/&gt;    bool paws_reject = false;&lt;br/&gt;    ......&lt;br/&gt;    paws_reject = tcp_paws_reject(&amp;amp;tmp_opt, th-&amp;gt;rst);&lt;br/&gt;    if (!paws_reject &amp;amp;&amp;amp;&lt;br/&gt;        (TCP_SKB_CB(skb)-&amp;gt;seq == tcptw-&amp;gt;tw_rcv_nxt &amp;amp;&amp;amp;&lt;br/&gt;         (TCP_SKB_CB(skb)-&amp;gt;seq == TCP_SKB_CB(skb)-&amp;gt;end_seq || th-&amp;gt;rst))) {&lt;br/&gt;        ......&lt;br/&gt;        // 重复的ACK,discard此包&lt;br/&gt;        return TCP_TW_SUCCESS;&lt;br/&gt;    }&lt;br/&gt;    // 如果是SYN分节，而且通过了paws校验&lt;br/&gt;    if (th-&amp;gt;syn &amp;amp;&amp;amp; !th-&amp;gt;rst &amp;amp;&amp;amp; !th-&amp;gt;ack &amp;amp;&amp;amp; !paws_reject &amp;amp;&amp;amp;&lt;br/&gt;        (after(TCP_SKB_CB(skb)-&amp;gt;seq, tcptw-&amp;gt;tw_rcv_nxt) ||&lt;br/&gt;         (tmp_opt.saw_tstamp &amp;amp;&amp;amp;&lt;br/&gt;          (s32)(tcptw-&amp;gt;tw_ts_recent - tmp_opt.rcv_tsval) &amp;lt; 0))) {&lt;br/&gt;        ......&lt;br/&gt;        // 返回TCP_TW_SYN,允许重用TIME_WAIT五元组重新建立连接&lt;br/&gt;        return TCP_TW_SYN;&lt;br/&gt;    }&lt;br/&gt;    // 如果没有通过paws校验，则增加统计参数&lt;br/&gt;    if (paws_reject)&lt;br/&gt;        NET_INC_STATS_BH(twsk_net(tw), LINUX_MIB_PAWSESTABREJECTED);&lt;br/&gt;    if (!th-&amp;gt;rst) {&lt;br/&gt;        // 如果没有通过paws校验，而且这个分节包含ack,则将TIMEWAIT持续时间重新延长&lt;br/&gt;        // 我们抓包结果的结果没有ACK,只有SYN,所以并不会延长&lt;br/&gt;        if (paws_reject || th-&amp;gt;ack)&lt;br/&gt;            inet_twsk_schedule(tw, &amp;amp;tcp_death_row, TCP_TIMEWAIT_LEN,&lt;br/&gt;                       TCP_TIMEWAIT_LEN);&lt;br/&gt;        // 返回TCP_TW_ACK,也即TCP重传ACK到对面&lt;br/&gt;        return TCP_TW_ACK;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;根据上面源码，PAWS(Protect Againest Wrapped Sequence numbers防止回绕)校验机制如果生效而拒绝此分节的话，LINUX_MIB_PAWSESTABREJECTED这个统计参数会增加，对应于Linux中的命令即是:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;netstat -s | grep reject&lt;br/&gt;216576 packets rejects in established connections because of timestamp&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这么上去后端的Nginx一统计，果然有大量的报错。而且根据笔者的观察，这个统计参数急速增加的时间段就是出问题的时间段，也就是每天早上10:00-12:00左右。每次大概会增加1W多个统计参数。&lt;br/&gt;那么什么时候PAWS会不通过呢，我们直接看下tcp_paws_reject的源码吧:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;static inline int tcp_paws_reject(const struct tcp_options_received *rx_opt,&lt;br/&gt;                  int rst)&lt;br/&gt;{&lt;br/&gt;    if (tcp_paws_check(rx_opt, 0))&lt;br/&gt;        return 0;&lt;br/&gt;    // 如果是rst，则放松要求，60s没有收到对端报文，认为PAWS检测通过 &lt;br/&gt;    if (rst &amp;amp;&amp;amp; get_seconds() &amp;gt;= rx_opt-&amp;gt;ts_recent_stamp + TCP_PAWS_MSL)&lt;br/&gt;        return 0;&lt;br/&gt;    return 1;&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;static inline int tcp_paws_check(const struct tcp_options_received *rx_opt,&lt;br/&gt;                 int paws_win)&lt;br/&gt;{&lt;br/&gt;&lt;br/&gt;    // 如果ts_recent中记录的上次报文（SYN）的时间戳，小于当前报文的时间戳（TSval），表明paws检测通过&lt;br/&gt;    // paws_win = 0&lt;br/&gt;    if ((s32)(rx_opt-&amp;gt;ts_recent - rx_opt-&amp;gt;rcv_tsval) &amp;lt;= paws_win)&lt;br/&gt;        return 1;&lt;br/&gt;    // 否则，上一次获得ts_recent时间戳的时刻的24天之后，为真表明已经有超过24天没有接收到对端的报文了，认为PAWS检测通过&lt;br/&gt;    if (unlikely(get_seconds() &amp;gt;= rx_opt-&amp;gt;ts_recent_stamp + TCP_PAWS_24DAYS))&lt;br/&gt;        return 1;&lt;br/&gt;&lt;br/&gt;    return 0;&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在抓包的过程中，我们明显发现，在四次挥手时候，记录的tsval是2149756058,而下一次syn三次握手的时候是21495149222,反而比之前的小了！&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;序号&lt;/th&gt;&lt;th&gt;时间&lt;/th&gt;&lt;th&gt;源地址&lt;/th&gt;&lt;th&gt;目的地址&lt;/th&gt;&lt;th&gt;源端口号&lt;/th&gt;&lt;th&gt;目的端口号&lt;/th&gt;&lt;th&gt;信息&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;09:57:30.64&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[FIN,ACK]TSval=2149756058&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;09:59:22.06&lt;/td&gt;&lt;td&gt;20.1.1.1&lt;/td&gt;&lt;td&gt;30.1.1.1&lt;/td&gt;&lt;td&gt;33735&lt;/td&gt;&lt;td&gt;443&lt;/td&gt;&lt;td&gt;[SYN]TSVal=21495149222&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;所以PAWS校验不过。&lt;br/&gt;那么为什么会这个SYN时间戳比之前挥手的时间戳还小呢?那当然是NAT的锅喽，NAT把多台机器的ip虚拟成同一个ip。但是多台机器的时间戳(也即从启动开始到现在的时间，非墙上时间),如下图所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.5322222222222223&quot; data-type=&quot;png&quot; data-w=&quot;1800&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJvvNgobGibVnzaiakxicw9EqaK9ayyHMFJicicLb3gMGMT65libUr6icXlKYww/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;但是还有一个疑问,笔者记得TIME_WAIT也即2MSL在Linux的代码里面是定义为了60s。为何抓包的结果却存活了将近2分钟之久呢？&lt;/p&gt;&lt;h3&gt;TIME_WAIT的持续时间&lt;/h3&gt;&lt;p&gt;于是笔者开始阅读器关于TIME_WAIT定时器的源码，具体可见笔者的另一篇博客:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;从Linux源码看TIME_WAIT状态的持续时间&lt;br/&gt;https://my.oschina.net/alchemystar/blog/4690516&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;结论如下&lt;br/&gt;&lt;img data-ratio=&quot;0.39370932754880694&quot; data-type=&quot;png&quot; data-w=&quot;1844&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJQYBfGTAJQjCTvMtRfsQCudASWicaOiblpPhiax9lqvkyasOVRIRUTIvPg/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;在TIME_WAIT很多的状态下，TIME_WAIT能够存活112.5s，将近两分钟的时间，和我们的抓包结果一致。&lt;br/&gt;当然了，这个计算只是针对Linux 2.6和3.10内核而言，而对红帽维护的3.10.1127内核版本则会有另外的变化，这个变化导致了一个令笔者感到非常奇异的现象，这个在后面会提到。&lt;/p&gt;&lt;h3&gt;问题发生条件&lt;/h3&gt;&lt;p&gt;如上面所解释，只有在Server端TIME_WAIT还没有消失时候,重用这个Socket的时候，遇上了反序的时间戳SYN，就会发生这种问题。由于NAT前面的所有机器时间戳都不相同，所以有很大概率会导致时间戳反序！&lt;/p&gt;&lt;h3&gt;那么什么时候重用TIME_WAIT状态的Socket呢&lt;/h3&gt;&lt;p&gt;笔者知道，防火墙的端口号选择逻辑是RoundRobin的，也即从2048开始一直增长到65535，再回绕到2048,如下图所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.604221635883905&quot; data-type=&quot;png&quot; data-w=&quot;1516&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJsB7dELcZrubsWTHFhn8LicAxqUJNCox2YJkRr6JtrP2gBAKJv6qhAJA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;为什么压测的时候不出现问题&lt;/h3&gt;&lt;p&gt;但我们在线下压测的时候，明显速率远超560tps,那为何确没有这样的问题出现呢。很简单，是因为&lt;br/&gt;TCP_SYN_SUCCESS这个分支，由于我们的压测机没有过NAT，那么时间戳始终保持单IP下的单调递增，即便&amp;gt;560TPS之后，走的也是TCP_SYN_SUCCESS,将TIME_WAIT Socket重用为SYN_RECV,自然不会出现这样的问题，如下图所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.4913232104121475&quot; data-type=&quot;png&quot; data-w=&quot;1844&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJOd2ib1ZiaGryPzdzDAlhP76zsI6hqmAShraY1SaMsmSHHNemRDopEguw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h3&gt;如何解释LVS的监控曲线?&lt;/h3&gt;&lt;p&gt;等等,564TPS?这个和LVS陡然下跌的TPS基本相同！难道在端口号复用之后LVS就不会新建连接(其实是LVS中的session表项)？从而导致统计参数并不增加?&lt;br/&gt;于是笔者直接去撸了一下LVS的源码:&lt;/p&gt;&lt;pre&gt;&lt;code&gt;tcp_conn_schedule&lt;br/&gt;        |-&amp;gt;ip_vs_schedule&lt;br/&gt;            /* 如果新建conn表项成功，则对已有连接数++ */&lt;br/&gt;            |-&amp;gt;ip_vs_conn_stats&lt;br/&gt;而在我们的入口函数ip\_vs\_in中&lt;br/&gt;static unsigned int&lt;br/&gt;ip_vs_in(unsigned int hooknum, struct sk_buff *skb,&lt;br/&gt;     const struct net_device *in, const struct net_device *out,&lt;br/&gt;     int (*okfn) (struct sk_buff *))&lt;br/&gt;{&lt;br/&gt;    ......&lt;br/&gt;    // 如果能找到对应的五元组&lt;br/&gt;    cp = pp-&amp;gt;conn_in_get(af, skb, pp, &amp;amp;iph, iph.len, 0, &amp;amp;res_dir);&lt;br/&gt;&lt;br/&gt;    if (likely(cp)) {&lt;br/&gt;        /* For full-nat/local-client packets, it could be a response */&lt;br/&gt;        if (res_dir == IP_VS_CIDX_F_IN2OUT) {&lt;br/&gt;            return handle_response(af, skb, pp, cp, iph.len);&lt;br/&gt;        }&lt;br/&gt;    } else {&lt;br/&gt;        /* create a new connection */&lt;br/&gt;        int v;&lt;br/&gt;        // 找不到对应的五元组，则新建连接，同时conn++&lt;br/&gt;        if (!pp-&amp;gt;conn_schedule(af, skb, pp, &amp;amp;v, &amp;amp;cp))&lt;br/&gt;            return v;&lt;br/&gt;    }&lt;br/&gt;    ......&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;很明显的，如果当前五元组表项存在，则直接复用表项，而不存在，才创建新的表项，同时conn++。而表项需要在LVS的Fintimeout时间超过后才消失(在笔者的环境里面是120s)。这样，在端口号复用的时候，因为&amp;lt;112.5s，所以LVS会直接复用表项，而统计参数不会有任何变化，从而导致了下面这个曲线。&lt;/p&gt;&lt;p&gt;&lt;img data-ratio=&quot;0.4596354166666667&quot; data-type=&quot;png&quot; data-w=&quot;1536&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJSjtYOGKkUlJyjZreRcIOplRxZzjnEFxYy34vhGkWEb8BZSIjPpRBiaA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;当流量慢慢变小，无法达到重用端口号的条件的时候，曲线又会垂直上升。和笔者的推测一致。也就是说在五元组固定四元的情况下&amp;gt;529tps(63487/120)的时候，在此固定业务下的新建连接数不会增加。&lt;/p&gt;&lt;p&gt;而图中仅存的560-529=&amp;gt;21+个连接创建，是由另一个业务的vip引起，在这个vip上，由于量很小，没有端口复用。但是LVS统计的是总数量，所以在端口号开始复用之后，始终会有少量的新建连接存在。&lt;/p&gt;&lt;p&gt;值得注意的是，端口号复用之后，LVS转发的时候就会直接使用这个映射表项，所以相同的五元组到LVS后会转发给相同的Nginx,而不会进行WRR(Weight Round Robin)负载均衡,表现出了一定的”亲和性”。如下图所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.4086859688195991&quot; data-type=&quot;png&quot; data-w=&quot;1796&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJxtxVLg7Ka6vXjHkd6dxLjKJiaCGAk0J2uNytBMIo1QIEjBBOXrEva4A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h2&gt;NAT下固定ip地址对的性能瓶颈&lt;/h2&gt;&lt;p&gt;好了，现在可以下结论了。在ip源和目的地址固定，目的端口号也固定的情况下，五元组的可变量只有ip源端口号了。而源端口号最多是65535个，如果计算保留端口号(0-2048)的话(假设防火墙保留2048个)，那么最多可使用63487个端口。&lt;br/&gt;由于每使用一个端口号，在高负载的情况下，都会产生一个112.5s才消失的TIME_WAIT。那么在63487/112.5也就是564TPS(使用短连接)的情况下，就会复用TIME_WAIT下的Socket。再加上PAWS校验，就会造成大量的连接创建异常！&lt;br/&gt;&lt;img data-ratio=&quot;0.5498614958448753&quot; data-type=&quot;png&quot; data-w=&quot;1444&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJsxMRIaw6Y4oqxicXFiaeEt19W0U7pvSDs1SbfEdeMVw9OtkREFjFQkUA/640?wx_fmt=png&quot;/&gt;这个论断和笔者观察到的应用报错以及LVS监控曲线一致。&lt;/p&gt;&lt;h3&gt;LVS曲线异常事件和报错时间接近&lt;/h3&gt;&lt;p&gt;因为LVS是在529TPS时候开始垂直下降，而端口号复用是在564TPS的时候开始，两者所需TPS非常接近，所以一般LVS出现曲线异常的时候，基本就是开始报错的时候！但是LVS曲线异常只能表明复用表项，并不能表明一定有问题，因为可以通过调节某些内核参数使得在端口号复用的时候不报错！&lt;br/&gt;&lt;img data-ratio=&quot;0.5006385696040868&quot; data-type=&quot;png&quot; data-w=&quot;1566&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJTejqLGQ2ictvmPTic8ic4iahWJQvy8d7P5CuGjU5jNc7L56Q4rp8NL5Vfw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;在端口号复用情况下，lvs本身的新建连接数无法代表真实TPS。&lt;/p&gt;&lt;h2&gt;尝试修复&lt;/h2&gt;&lt;h3&gt;设置tcp_tw_max_bucket&lt;/h3&gt;&lt;p&gt;首先，笔者尝试限制Nginx所在Linux中最大TIME_WAIT数量&lt;/p&gt;&lt;pre&gt;&lt;code&gt;echo &#x27;5000&#x27;  &amp;gt; /proc/sys/net/ipv4/tcp_tw_max_bucket&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这基于一个很简单的想法，TIME_WAIT状态越少，那么命中TIME_WAIT状态Socket的概率肯定越小。设置了之后，确实报错量确实减少了好多。但由于TPS超越极限之后端口号不停的回绕，导致还是一直在报错，不会有根本性好转。&lt;br/&gt;&lt;img data-ratio=&quot;0.483160621761658&quot; data-type=&quot;png&quot; data-w=&quot;1544&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJNnJE7Rtz4bKfnKuP8aP14K9UplC2s1TdiafmmTFHHicLO2cgWKCk4VMw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;如果将tcp_tw_max_bucket设置为0，那么按理论上来说不会出问题了。&lt;br/&gt;但是无疑将TCP精心设计的TIME_WAIT这个状态给废弃了，笔者觉得这样做过于冒险，于是没有进行尝试。&lt;/p&gt;&lt;h3&gt;尝试扩展源地址&lt;/h3&gt;&lt;p&gt;这个问题本质是由于五元组在限定了4元，只有源端口号可变的情况下，端口号只有&lt;br/&gt;2048-65535可用。那么我们放开源地址的限定，例如将源IP增加到3个，无疑可以将TPS扩大三倍。&lt;br/&gt;&lt;img data-ratio=&quot;0.3622159090909091&quot; data-type=&quot;png&quot; data-w=&quot;1408&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJVC2KdSzZCPkX6BVaU2Cm09EjudIeDzSEtK3Xu7waXCMCUdet8ByWog/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;同理，将目的地址给扩容，也能达到类似的效果。&lt;br/&gt;但据网工反映，合作方通过他们的防火墙出来之后就只有一个IP,而一个IP在我们的防火墙上并不能映射成多个IP,多以在不变更它们网络设置的情况下无法扩展源地址。而扩容目的地址，也需要对合作方网络设置进行修改。本着不让合作方改动的服务精神，笔者开始尝试其它方案。&lt;/p&gt;&lt;h3&gt;扩容Nginx?没效果&lt;/h3&gt;&lt;p&gt;在一开始笔者没有搞明白LVS那个诡异的曲线的时候，笔者并不知道在端口复用的情况下,LVS会表现出”亲和性”。于是想着，如果扩容Nginx后，根据负载均衡原则，正好落到有这个TIME_WAIT五元组的概率会降低，所以尝试着另扩容了一倍的Nginx。但由于之前所说的LVS在端口号复用下的亲和性，反而加大了TIME_WAIT段！&lt;br/&gt;&lt;img data-ratio=&quot;0.5010822510822511&quot; data-type=&quot;png&quot; data-w=&quot;1848&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJAlbvdMF8l92CyYUCKJUkjMJofzz3gL7WLqtFicSmqSF8WAiaGUYicfPcg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;h4&gt;扩容Nginx的奇异现象&lt;/h4&gt;&lt;p&gt;在笔者想明白LVS的”亲和性”之后，对扩容Nginx会导致更多的报错已经有了心理预期，不过被现实啪啪啪打脸!报错量和之前基本一样。更奇怪的是，笔者发现非活跃连接数监控(即非ESTABLISHED)状态，会在端口号复用之后，呈现出一种负载不均衡的现象，如下图所示。&lt;br/&gt;&lt;img data-ratio=&quot;0.49604221635883905&quot; data-type=&quot;png&quot; data-w=&quot;1516&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJDr5ROV65ZO8icFmPWbGrhpnWta2aqY6EzugbchIpJeePxGMMKMiaotFw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;笔者上去新扩容的Nginx看了一下，发现新Nginx只有很少量的由于PAWS引起的报错，增长速率很慢，基本1个小时只有100多。而旧Nginx一个小时就有1W多！&lt;br/&gt;那么按照这个错误比例分布，就很好理解为什么形成这样的曲线了。因为LVS的亲和性,在端口号复用时刻，落到旧Nginx上会大概率失败，从而在Fintimeout到期后，重新选择一个负载均衡的时候，如果落到新Nginx上，按照统计参数来看基本都会成功，但如果还是落到旧Nginx上则基本还会失败，如此往复。就天然的形成了一个优先选择的过程，从而造成了这个曲线。&lt;br/&gt;&lt;img data-ratio=&quot;0.6022253129346314&quot; data-type=&quot;png&quot; data-w=&quot;1438&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJ7kHGL8nibSYKFGAWAUjYwpb8OS6rvafAHqSnn7cezcw3Bfjzr0fdd8A/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;当然实际的过程会比这个复杂一点，多一些步骤，但大体是这个思路。&lt;br/&gt;而在端口复用结束后，不管落到哪个Nginx上都会成功，所以负载均衡又会慢慢趋于均衡。&lt;/p&gt;&lt;h3&gt;为什么新扩容的Nginx表现异常优异呢?&lt;/h3&gt;&lt;p&gt;新扩容的Nginx表现异常优异，在这个TPS下没有问题，那到底是为什么呢？笔者想了一天都没想明白。睡了一觉之后，对比了两者的内核参数，突然豁然开朗。原来新扩容的Nginx所在的内核版本变了，变成了3.10!&lt;br/&gt;笔者连忙对比起了原来的2.6内核和3.10的内核版本变化，但毫无所得。。。思维有陷入了停滞&lt;/p&gt;&lt;h4&gt;Linux官方3.10和红帽的3.10.1127分支差异&lt;/h4&gt;&lt;p&gt;等等，我们线上的内核版本是3.10.1127,并不是官方的内核，难道代码有所不同？于是笔者立马下载了3.10.1127的源码。这一比对，终于让笔者找到了原因所在，看如下代码！&lt;/p&gt;&lt;pre&gt;&lt;code&gt;void inet_twdr_twkill_work(struct work_struct *work)&lt;br/&gt;{&lt;br/&gt;    struct inet_timewait_death_row *twdr =&lt;br/&gt;        container_of(work, struct inet_timewait_death_row, twkill_work);&lt;br/&gt;    bool rearm_timer = false;&lt;br/&gt;    int i;&lt;br/&gt;&lt;br/&gt;    BUILD_BUG_ON((INET_TWDR_TWKILL_SLOTS - 1) &amp;gt;&lt;br/&gt;            (sizeof(twdr-&amp;gt;thread_slots) * 8));&lt;br/&gt;&lt;br/&gt;    while (twdr-&amp;gt;thread_slots) {&lt;br/&gt;        spin_lock_bh(&amp;amp;twdr-&amp;gt;death_lock);&lt;br/&gt;        for (i = 0; i &amp;lt; INET_TWDR_TWKILL_SLOTS; i++) {&lt;br/&gt;            if (!(twdr-&amp;gt;thread_slots &amp;amp; (1 &amp;lt;&amp;lt; i)))&lt;br/&gt;                continue;&lt;br/&gt;&lt;br/&gt;            while (inet_twdr_do_twkill_work(twdr, i) != 0) {&lt;br/&gt;                // 如果这次没处理完，将rearm_timer设置为true&lt;br/&gt;                rearm_timer = true;&lt;br/&gt;                if (need_resched()) {&lt;br/&gt;                    spin_unlock_bh(&amp;amp;twdr-&amp;gt;death_lock);&lt;br/&gt;                    schedule();&lt;br/&gt;                    spin_lock_bh(&amp;amp;twdr-&amp;gt;death_lock);&lt;br/&gt;                }&lt;br/&gt;            }&lt;br/&gt;&lt;br/&gt;            twdr-&amp;gt;thread_slots &amp;amp;= ~(1 &amp;lt;&amp;lt; i);&lt;br/&gt;        }&lt;br/&gt;        spin_unlock_bh(&amp;amp;twdr-&amp;gt;death_lock);&lt;br/&gt;    }&lt;br/&gt;    // 在这边多了一个rearm_timer,并将定时器设置为1s之后&lt;br/&gt;    // 这样，原来需要额外等待的7.5s现在收敛为额外等待1s&lt;br/&gt;    if (rearm_timer)&lt;br/&gt;        mod_timer(&amp;amp;twdr-&amp;gt;tw_timer, jiffies + HZ);&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如代码所示，3.10.1127对TIME_WAIT的时间轮处理做了加速，让原来需要额外等待的7.5s收敛为额外等待的1s。经过校正后的时间轮如下所示:&lt;br/&gt;&lt;img data-ratio=&quot;0.4945054945054945&quot; data-type=&quot;png&quot; data-w=&quot;1820&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJIrpEpBao8F5x6tDkTtZbxIlBmdqibhEZq0eVonX3wjib3mtn9iaO2RykQ/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;那么TIME_WAIT的存活时间就从112.5s下降到60.5s(计算公式8.5*7+1)。&lt;br/&gt;那么，在这个状态下，我们的端口复用临界TPS就达到了(65535-2048)/60.5=1049tps,由于线上业务量并没有达到这一tps。所以对于新扩容的Nginx，并不会造成TIME_WAIT下的端口复用。所以错误量并没有变多！当然，由于旧Nginx的存在，错误量也没有变少。&lt;br/&gt;但是，由于那个神奇的选择性负载均衡的存在，在端口复用时间越长，每秒钟的报错量会越少！直到LVS的表项全部指到新Nginx集群，就不会再有报错了！&lt;/p&gt;&lt;h4&gt;TPS涨到1049tps依旧会报错&lt;/h4&gt;&lt;p&gt;当然了，根据上面的计算，在TPS继续上涨到1049后，依旧会产生错误。新版本内核只不过拉高了临界值，所以笔者还是要寻求更加彻底的解决方案。&lt;/p&gt;&lt;h4&gt;顺便吐槽一句&lt;/h4&gt;&lt;p&gt;Linux TCP的实现对TIME_WAIT的处理用时间轮在笔者看来并不是什么高明的处理方式。&lt;br/&gt;Linux本身对于Timer的处理本身就提供了红黑树这样的方案。放着这样好的方案不用，偏偏去实现一个精度不高还很复杂的时间轮。&lt;br/&gt;所幸在Linux 4.x版本中，摈弃了时间轮，直接使用Linux本身的红黑树方案。感觉自然多了！&lt;/p&gt;&lt;h3&gt;关闭tcp_timestamps&lt;/h3&gt;&lt;p&gt;笔者一开始并不想修改这个参数，因为修改意味着关闭PAWS校验。要是真有个什么乱序包之类，就少了一层防御手段。但是事到如今，为了不让合作方修改，只能改这个参数了。不过由于是我们是专线！所以风险可控。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;echo &#x27;0&#x27; &amp;gt; /proc/sys/net/ipv4/tcp_timestamps&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;运行至今，业务上反馈良好。终于，这个问题终于被解决了！！！！！！&lt;br/&gt;补充一句,关闭tcp_timestamps只是笔者在种种限制下所做的选择，更好的方案应该是扩充源或者目的地址。&lt;/p&gt;&lt;h1&gt;总结&lt;/h1&gt;&lt;p&gt;解决这个问题真的是一波三折。在问题解决过程中，从LVS源码看到Linux 2.6内核对TIME_WAIT状态的处理，再到3.10内核和3.10.1127内核之间的细微区别。&lt;br/&gt;为了解释所有的疑点，笔者始终在找寻着各种蛛丝马迹。虽然不追寻这些，问题依旧大概率能够通过各种尝试得到解决。但是，那些奇怪的曲线始终萦绕在笔者心头，让笔者日思夜想。然后，突然灵光乍现，找到线索后顿悟的那种感觉实在是太棒了！这也是笔者解决复杂问题源源不断的动力！&lt;/p&gt;&lt;p&gt;欢迎大家关注我公众号，里面有各种干货，还有大礼包相送哦!&lt;br/&gt;&lt;img data-ratio=&quot;0.359781121751026&quot; data-type=&quot;png&quot; data-w=&quot;1462&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/yiaiaFLiaflYRQHr7zQ9mpLVcTH1qrw3XpJicamExCgibPpLoIt8eTzWQSG7nOywknww1J3bQoGYsqnzggRkBR0feJA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
          &lt;/div&gt;

          

          



           
                          
              &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>