<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>f8028b4fcba6fdffdceecadd411cf538</guid>
<title>欢迎加入读者圈子，一起交流！</title>
<link>https://toutiao.io/k/mv211dm</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content                                                                     &quot; id=&quot;js_content&quot;&gt;
            &lt;p&gt;&lt;span&gt;&lt;strong&gt;欢迎加入读者圈子，一起交流！&lt;br/&gt;↓↓↓&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-cropselx1=&quot;0&quot; data-cropselx2=&quot;558&quot; data-cropsely1=&quot;0&quot; data-cropsely2=&quot;307&quot; data-ratio=&quot;0.5493333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/AjN1jquNavich3VaNkKeiaAwUhz7TQbQmic4fFsr58X9PAYleYzxqc1K1vZjeBoZDMUsmia0xH67EQYINGRvNOtLmA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;圈子剧透&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1、600+圈子成员，以中高级程序员为主，更有架构师、CTO坐镇交流；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、1000+优质主题，数十G独家资料，每日分享，精挑细选；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、全年52期专属邮件周报，让你轻松掌握业界资讯、技术干货，提升认知水平；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;4、全年52本好书共读，让你花最少的时间，获取更好的知识；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;……&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;心动不如行动，赶快加入吧！&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
          &lt;/div&gt;

          

          



           
                                
                    
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>8eb196c0181711efed7929745f53ead7</guid>
<title>性能提升40倍——线上真实重构案例分享</title>
<link>https://toutiao.io/k/izbqpxo</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content                                                                     &quot; id=&quot;js_content&quot;&gt;
            &lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;写在前面&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这篇文章和大家分享一下最近和团队成员一起重构的围栏服务真实案例分享，二话不说，先上图:&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.49940023990403837&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/kulR6Bq67W8icZBialGwyfQOtCEAIt7ucMsaldHpic9KLyXMJdH1vZzglibkUzpgXocWJzhvOslmVLHDT3icRm3VXFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2501&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;重构前后对比(单台docker服务压测结果)&lt;/p&gt;&lt;section data-tool=&quot;mdnice编辑器&quot;&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;对比项&lt;/th&gt;&lt;th&gt;QPS&lt;/th&gt;&lt;th&gt;平均RT&lt;/th&gt;&lt;th&gt;P995耗时&lt;/th&gt;&lt;th&gt;说明&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;重构前&lt;/td&gt;&lt;td&gt;120&lt;/td&gt;&lt;td&gt;50ms&lt;/td&gt;&lt;td&gt;800ms&lt;/td&gt;&lt;td&gt;压测达到性能瓶颈&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;重构后&lt;/td&gt;&lt;td&gt;5000&lt;/td&gt;&lt;td&gt;5ms&lt;/td&gt;&lt;td&gt;50ms&lt;/td&gt;&lt;td&gt;压测未到达性能瓶颈&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;重构之后性能提升40倍，效果非常明显。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面分享详细技术方案。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;技术方案&lt;/span&gt;&lt;/h2&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;一、背景/现状&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;多次压测反馈，目前线上机器8台docker大概只能支撑1k/QPS, 单机120/QPS。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;无城市查询围栏场景，会循环判断该业务线下全国的围栏是否命中，耗CPU严重，高峰期性能瓶颈特别明显。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;二、目标&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;派单围栏服务流程重构，通过派单系统自建空间索引RTree方式， 利用空间换时间的方式，先通过RTree搜索围栏，再判断坐标是否在围栏内。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;现有资源不变情况下，提升接口性能，并且支持水平扩展。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;重构方案&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;重构前流程图&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.232527759634226&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/kulR6Bq67W8icZBialGwyfQOtCEAIt7ucMdBVictZiaV3jvHMeJLLumEQoRWAh7f2UZcjwe7frEDWAvtwnjnklsvAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1531&quot;/&gt;&lt;/figure&gt;&lt;ol start=&quot;2&quot; data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;重构后流程图&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;1.46730571722013&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/kulR6Bq67W8icZBialGwyfQOtCEAIt7ucMbhDFyCdUplBIy0TgtwRRWBIqcxTcNCpAF2nOayv8gF9jcDjGZenibtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2921&quot;/&gt;&lt;/figure&gt;&lt;ol start=&quot;3&quot; data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-1&quot;&gt;&lt;li&gt;&lt;section&gt;RTREE数据结构简介:
每个节点是能把对应的围栏包起来 的最小外包矩形&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.9079754601226994&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/kulR6Bq67W8icZBialGwyfQOtCEAIt7ucMVEtNwyECqGxRYLbZ7SeXLJ8OAUOEybgCUib5Vl6NEL9YfZ1enmteUfw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;489&quot;/&gt;&lt;/figure&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;四、里程碑节点&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;section data-tool=&quot;mdnice编辑器&quot;&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;阶段&lt;/th&gt;&lt;th&gt;事项&lt;/th&gt;&lt;th&gt;开发时间&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;开发阶段&lt;/td&gt;&lt;td&gt;1. 围栏系统自建Rtree开发&lt;br/&gt;2. 灰度方案开发(按流量灰度，支持header指定强制走新系统(方便压测)， &lt;br/&gt;3. 数据对比: 通过抓取线上日志，通过流量回放，同时请求新老围栏系统，判断结果是否完全一致。&lt;br/&gt;4. 异常补偿: 强制刷新Rtree接口)&lt;/td&gt;&lt;td&gt;5d&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;测试阶段&lt;/td&gt;&lt;td&gt;由测试同学评估,开发提供影响范围&lt;/td&gt;&lt;td&gt;2d&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;灰度阶段&lt;/td&gt;&lt;td&gt;1. 按流量灰度平滑迁移  2. 压测&lt;/td&gt;&lt;td&gt;5d&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/section&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;五、灰度方案&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;按接口请求流量灰度&lt;/p&gt;&lt;section data-tool=&quot;mdnice编辑器&quot;&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;一阶段&lt;/th&gt;&lt;th&gt;二阶段【压测】&lt;/th&gt;&lt;th&gt;三阶段&lt;/th&gt;&lt;th&gt;四阶段&lt;/th&gt;&lt;th&gt;阶段&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;万分之一&lt;/td&gt;&lt;td&gt;1%&lt;/td&gt;&lt;td&gt;20%&lt;/td&gt;&lt;td&gt;50%&lt;/td&gt;&lt;td&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/section&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;六、所需资源&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;无需新增资源&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;总结:&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;本次重构优化效果很明显，其实改动并不是很大，可见合理的技术方案是非常重要的。作为技术人，我认为写代码是其次的，也是最基本的，除此之外应该多深入思考一下，多问问自己:&quot;这样实现会不会有什么瓶颈？有没有更好的方案？我这样实现之后能不能水平扩展？如果不能，我有什么应对方案？&quot;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;另外，借鉴之前老大说的一句话:&quot;只要人觉得可能出现问题的地方，就一定会出问题！&quot; ，敬畏心也是非常重要的，所以灰度方案，也是非常重要的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;刚接手这个新团队不久，重构之路刚刚启程，这篇文章也是记录一下心路历程，希望对大家也有所感悟和帮助。&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;
          &lt;/div&gt;

          

          



           
                          
              &lt;div class=&quot;reward_qrcode_area reward_area tc&quot; id=&quot;js_reward_qrcode&quot;&gt;
                  &lt;p class=&quot;tips_global&quot; aria-hidden=&quot;true&quot; id=&quot;js_a11y_reward_qr_title&quot;&gt;Long-press QR code to transfer me a reward&lt;/p&gt;
                                    &lt;p role=&quot;option&quot; aria-labelledby=&quot;js_a11y_reward_qr_word js_a11y_comma js_a11y_reward_qr_title js_a11y_reward_qr_money&quot; aria-describedby=&quot;js_a11y_reward_qr_tips &quot; class=&quot;reward_tips&quot; id=&quot;js_a11y_reward_qr_word&quot;/&gt;
                  &lt;span class=&quot;reward_qrcode_img_wrp&quot;&gt;&lt;img alt=&quot;赞赏二维码&quot; class=&quot;reward_qrcode_img&quot; id=&quot;js_reward_qrcode_img&quot;/&gt;&lt;/span&gt;
                  &lt;p aria-hidden=&quot;true&quot; id=&quot;js_a11y_reward_qr_tips&quot; class=&quot;tips_global&quot;&gt;As required by Apple&#x27;s new policy, the Reward feature has been disabled on Weixin for iOS. You can still reward an Official Account by transferring money via QR code.&lt;/p&gt;
                &lt;/div&gt;
                                              
                    
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>684716632e4119669e5e361fb31dbfed</guid>
<title>上篇：技术架构的设计方法</title>
<link>https://toutiao.io/k/7ee750e</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;profile_inner&quot;&gt;
                  &lt;strong class=&quot;profile_nickname&quot;&gt;阿里巴巴中间件&lt;/strong&gt;
                  &lt;img class=&quot;profile_avatar&quot; id=&quot;js_profile_qrcode_img&quot; src=&quot;&quot; alt=&quot;&quot;/&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;Weixin ID&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;Aliware_2018&lt;/span&gt;
                  &lt;/p&gt;

                  &lt;p class=&quot;profile_meta&quot;&gt;
                  &lt;label class=&quot;profile_meta_label&quot;&gt;About Feature&lt;/label&gt;
                  &lt;span class=&quot;profile_meta_value&quot;&gt;Aliware阿里巴巴中间件官方账号&lt;/span&gt;
                  &lt;/p&gt;
                &lt;/div&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>5ec311e8f7bc09cd35f6828f60f8761c</guid>
<title>前方高能 | HDFS 的架构，你吃透了吗？</title>
<link>https://toutiao.io/k/n4qw0ty</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;Mzg3Nzc1MzQxMQ==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/GxcviaLQ4UP6SUsO99KV60rsibT4lKAXBQPJIhb2QibmyO9dBrgB8jnGmdbCSMMGByx4VC33dsUDp0c8bUK2yfyMw/0?wx_fmt=png&quot; data-nickname=&quot;脚丫先生&quot; data-alias=&quot;&quot; data-signature=&quot;一枚热爱技术的大数据工程师，致力全栈。分享技术，乐享生活。&quot; data-from=&quot;0&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;span/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;前言&lt;/span&gt;&lt;br/&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        HDFS 是 Hadoop 中存储数据的基石，存储着所有的数据，具有&lt;span&gt;&lt;strong&gt;高可靠&lt;/strong&gt;&lt;strong&gt;性，高容错性，高可扩展性，高吞吐量 &lt;/strong&gt;&lt;/span&gt;等特征，能够部署在大规模廉价的集群上，极大地降低了部署成本。有意思的是，&lt;span&gt;&lt;strong&gt;其良好的架构特征使其能够存储海量的数据&lt;/strong&gt;&lt;/span&gt;。本篇文章，我们就来系统学习一下，Hadoop HDFS的架构！&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg wxw-img&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_png/BFibC8wtke3uwlsxOnqANFtOlKCzxIcnRQFcNwGh5ndggFn98bwabZ0Nx5Mic5BWFXGmBN6KYZMiasVM9hPwO63PQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;HDFS架构&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        HDFS采用 &lt;strong&gt;Master/Slave&lt;/strong&gt; 架构存储数据，且支持 NameNode 的 HA。HDFS架构主要包含客户端，&lt;code&gt;NameNode&lt;/code&gt;，&lt;code&gt;SecondaryNameNode&lt;/code&gt; 和 &lt;code&gt;DataNode&lt;/code&gt; 四个重要组成部分，如图所示：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.49263502454991814&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_png/BFibC8wtke3uwlsxOnqANFtOlKCzxIcnRBaoMPnfR7gibaEzYKtynqwW0GwBhwbaD66yR8fNwyTf8McxOkEc7iasQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1222&quot;/&gt;&lt;/figure&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;br/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）客户端向NameNode&lt;strong&gt;发起请求&lt;/strong&gt;，获取元数据信息，这些元数据信息包括命名空间、块映射信息及 DataNode 的位置信息等。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）NameNode 将元数据信息&lt;strong&gt;返回&lt;/strong&gt;给客户端。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （3）客户端获取到元数据信息后，到相应的 DataNode 上&lt;strong&gt;读/写数据&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （4）相关联的 DataNode 之间会&lt;strong&gt;相互复制数据&lt;/strong&gt;，以达到 DataNode 副本数的要求&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （5）DataNode 会&lt;strong&gt;定期&lt;/strong&gt;向 NameNode 发送心跳信息，将自身节点的状态信息报告给 NameNode。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （6）SecondaryNameNode 并不是 NameNode 的备份。SecondaryNameNode 会定期获取 NameNode 上的 &lt;code&gt;fsimage&lt;/code&gt;和 &lt;code&gt;edits log&lt;/code&gt; 日志，并将二者进行合并，产生 &lt;code&gt;fsimage.ckpt&lt;/code&gt; 推送给 NameNode。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1、NameNode&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;&lt;span&gt;NameNode 是整个 Hadooop 集群中至关重要的组件，它维护着整个 HDFS 树，以及文件系统树中所有的文件和文件路径的元数据信息&lt;/span&gt;&lt;/strong&gt;。这些元数据信息包括&lt;strong&gt;文件名&lt;/strong&gt;，&lt;strong&gt;命令空间&lt;/strong&gt;，&lt;strong&gt;文件属性&lt;/strong&gt;（文件生成的时间、文件的副本数、文件的权限）、&lt;strong&gt;文件数据块&lt;/strong&gt;、&lt;strong&gt;文件数据块与所在 DataNode 之间的映射关系&lt;/strong&gt;等。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;&lt;span&gt;一旦 NameNode 宕机或 NameNode 上的元数据信息损坏或丢失，基本上就会丢失 Hadoop 集群中存储的所有数据，整个 Hadoop 集群也会随之瘫痪&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        在 Hadoop 运行的过程中， NameNode 的主要功能如下图所示：&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.28913324708926263&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_png/BFibC8wtke3uwlsxOnqANFtOlKCzxIcnRt6sdqBfa92icMNDJtPNlvvq74w2PgdCR5ZUmRicC88MUAicnZzqzUnTVg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1546&quot;/&gt;&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;2、SecondaryNameNode&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;&lt;span&gt;SecondaryNameNode 并不是 NameNode 的备份，在NameNode 发生故障时也不能立刻接管 NameNode 的工作&lt;/span&gt;&lt;/strong&gt;。SecondaryNameNode 在 Hadoop 运行的过程中具有两个作用：一个是&lt;strong&gt;备份数据镜像&lt;/strong&gt;，另一个是&lt;strong&gt;定期合并日志与镜像&lt;/strong&gt;，因此可以称其为 Hadoop 的&lt;strong&gt;检查点&lt;/strong&gt;（checkpoint）。&lt;strong&gt;&lt;span&gt;SecondaryNameNode 定期合并 NameNode 中的 fsimage 和 edits log，能够防止 NameNode 重启时把整个 fsimage 镜像文件加载到内存，耗费过长的启动时间&lt;/span&gt;&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        SecondaryNameNode 的工作流程如图所示：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;br/&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.6178571428571429&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_png/BFibC8wtke3uwlsxOnqANFtOlKCzxIcnR72vGrx4Gvee2GA4RU1uBpqN1mbqcscp5KyddbGibdeqPNMjK7OyTXibA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1120&quot;/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;SecondaryNameNode的工作流程如下：&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）SecondaryNameNode 会&lt;strong&gt;通知&lt;/strong&gt; NameNode &lt;strong&gt;生成&lt;/strong&gt;新的 edits log 日志文件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）NameNode &lt;strong&gt;生成&lt;/strong&gt;新的 edits log 日志文件，然后将新的日志信息&lt;strong&gt;写到&lt;/strong&gt;新生成的 edits log 日志文件中。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （3）SecondaryNameNode &lt;strong&gt;复制&lt;/strong&gt; NameNode 上的 fsimage 镜像和 edits log 日志文件，此时使用的是 &lt;strong&gt;http get &lt;/strong&gt;方式。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （4）SecondaryNameNode 将fsimage将镜像文件&lt;strong&gt;加载&lt;/strong&gt;到内存中，然后执行 edits log 日志文件中的操作，&lt;strong&gt;生成&lt;/strong&gt;新的镜像文件 fsimage.ckpt。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （5）SecondaryNameNode 将 fsimage.ckpt 文件&lt;strong&gt;发送&lt;/strong&gt;给 NameNode，此时使用的是 &lt;strong&gt;http post &lt;/strong&gt;方式。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （6）NameNode 将 edits log 日志文件&lt;strong&gt;替换&lt;/strong&gt;成新生成的 edits.log 日志文件，同样将 fsimage文件&lt;strong&gt;替换&lt;/strong&gt;成 SecondaryNameNode 发送过来的新的 fsimage 文件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （7）NameNode&lt;strong&gt; 更新&lt;/strong&gt; fsimage 文件，将此次执行 checkpoint 的时间&lt;strong&gt;写入&lt;/strong&gt; fstime 文件中。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        经过 SecondaryNameNode 对 fsimage 镜像文件和 edits log 日志文件的&lt;strong&gt;复制和合并&lt;/strong&gt;操作之后，NameNode 中的 fsimage 镜像文件就&lt;strong&gt;保存&lt;/strong&gt;了最新的 checkpoint 的元数据信息， edits log 日志文件也会重新&lt;strong&gt;写入&lt;/strong&gt;数据，两个文件中的数据不会变得很大。因此，当 &lt;strong&gt;重启 NameNode 时，不会耗费太长的启动时间&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;SecondaryNameNode 周期性地进行 checkpoint 操作需要满足一定的前提条件，这些条件如下&lt;/strong&gt;：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）&lt;code&gt;edits log&lt;/code&gt; 日志文件的大小达到了一定的阈值，此时会对其进行合并操作。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）每隔一段时间进行 &lt;strong&gt;checkpoint&lt;/strong&gt; 操作。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        这些条件可以在&lt;code&gt;core-site.xml&lt;/code&gt;文件中进行配置和调整，代码如下所示：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&amp;lt;property&amp;gt;&lt;br/&gt;         &amp;lt;name&amp;gt;fs.checkpoint.period&amp;lt;/name&amp;gt;&lt;br/&gt;         &amp;lt;value&amp;gt;3600&amp;lt;/value&amp;gt;&lt;br/&gt;&amp;lt;/property&amp;gt;&lt;br/&gt;&amp;lt;property&amp;gt;&lt;br/&gt;         &amp;lt;name&amp;gt;fs.checkpoint.size&amp;lt;/name&amp;gt;&lt;br/&gt;         &amp;lt;value&amp;gt;67108864&amp;lt;/value&amp;gt;&lt;br/&gt;&amp;lt;/property&amp;gt;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        上述代码配置了 &lt;span&gt;checkpoint &lt;/span&gt;发生的时间周期和 &lt;span&gt;edits log &lt;/span&gt;日志文件的大小阈值，说明如下。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）&lt;strong&gt;fs.checkpoint.period&lt;/strong&gt;：表示触发 &lt;code&gt;checkpoint&lt;/code&gt;发生的时间周期，这里配置的时间周期为 1 h。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）&lt;strong&gt;fs.checkpoint.size&lt;/strong&gt;：表示 &lt;code&gt;edits log&lt;/code&gt; 日志文件大小达到了多大的阈值时会发生 &lt;code&gt;checkpoint&lt;/code&gt;操作，这里配置的 &lt;code&gt;edits log&lt;/code&gt;大小阈值为 64 MB。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        上述代码中配置的 &lt;code&gt;checkpoint&lt;/code&gt;操作发生的情况如下：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）如果 &lt;span&gt;edits log&lt;/span&gt; 日志文件经过 1 h 未能达到 64 MB，但是满足了 &lt;span&gt;checkpoint&lt;/span&gt;&lt;span&gt;发生的周期为 1 h 的条件，也会发生&lt;/span&gt;&lt;span&gt; checkpoint &lt;/span&gt;&lt;span&gt;操作。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;        （2）如果 &lt;/span&gt;&lt;span&gt;edits log &lt;/span&gt;&lt;span&gt;日志文件大小在 1 h 之内达到了 64MB，满足了 &lt;/span&gt;&lt;span&gt;checkpoint&lt;/span&gt;&lt;span&gt; 发生的&lt;/span&gt;&lt;span&gt; edits log &lt;/span&gt;&lt;span&gt;日志文件大小阈值的条件，则会发生 &lt;/span&gt;&lt;span&gt;checkpoint &lt;/span&gt;&lt;span&gt;操作。&lt;/span&gt;&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：如果 NameNode 发生故障或 NameNode 上的元数据信息丢失或损坏导致 NameNode 无法启动，此时就需要&lt;strong&gt;人工干预&lt;/strong&gt;，将 NameNode 中的元数据状态恢复到 SecondaryNameNode 中的&lt;strong&gt;元数据&lt;/strong&gt;状态。此时，如果 SecondaryNameNode 上的元数据信息与 NameNode 宕机时的元数据信息不同步，则或多或少地会导致 Hadoop 集群中丢失一部分数据。出于此原因，&lt;span&gt;&lt;strong&gt;应尽量避免将 NameNode 和 SecondaryNameNode 部署在同一台服务器&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;上&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3、DataNode&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        &lt;strong&gt;&lt;span&gt;DataNode 是真正存储数据的节点&lt;/span&gt;&lt;/strong&gt;，这些数据以&lt;span&gt;&lt;strong&gt;数据块&lt;/strong&gt;&lt;/span&gt;的形式存储在 DataNode 上。一个数据块包含两个文件：一个是&lt;span&gt;&lt;strong&gt;存储数据本身的文件&lt;/strong&gt;&lt;/span&gt;，另一个是&lt;span&gt;&lt;strong&gt;存储元数据的文件&lt;/strong&gt;&lt;/span&gt;（这些元数据主要包括&lt;strong&gt;&lt;span&gt;数据块的长度&lt;/span&gt;、&lt;span&gt;数据块的检验和&lt;/span&gt;、&lt;span&gt;时间戳&lt;/span&gt;&lt;/strong&gt;）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        DataNode 运行时的&lt;strong&gt;工作机制&lt;/strong&gt;如图所示：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.4852589641434263&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_png/BFibC8wtke3uwlsxOnqANFtOlKCzxIcnRficCU28dBjicGJYrnfhZv4Z0K2S00W3qBfHy3FUojeI3dZjh0781oTJA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1255&quot;/&gt;        如图所示，DataNode 运行时的工作机制如下：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）DataNode启动之后，向 NameNode &lt;strong&gt;注册&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）NameNode &lt;strong&gt;返回&lt;/strong&gt;注册成功的消息给 DataNode。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （3）DataNode 收到 NameNode 返回的注册成功的信息之后，会&lt;strong&gt;周期性&lt;/strong&gt;地向 NameNode &lt;strong&gt;上报&lt;/strong&gt;当前 DataNode 的所有块信息，默认发送所有数据块的时间周期是 &lt;strong&gt;1h&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （4）DataNode &lt;strong&gt;周期性&lt;/strong&gt;地向NameNode 发送心跳信息；NameNode &lt;strong&gt;收到&lt;/strong&gt; DataNode 发来的心跳信息后，会将DataNode 需要执行的命令放入到 &lt;strong&gt;心跳信息&lt;/strong&gt;的 返回数据中，&lt;strong&gt;返回&lt;/strong&gt;给 DataNode。DataNode 向 NameNode 发送心跳信息的默认时间周期是 &lt;strong&gt;3s&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （5）NameNode &lt;strong&gt;超过一定的时间&lt;/strong&gt;没有收到 DataNode 发来的心跳信息，则 NameNode 会认为对应的 DataNode &lt;strong&gt;不可用&lt;/strong&gt;。默认的超时时间是 &lt;strong&gt;10 min&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （6）&lt;strong&gt;在存储上相互关联的 DataNode 会同步数据块，以达到数据副本数的要求&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        当 DataNode 发生故障导致 DataNode 无法与 NameNode 通信时，NameNode 不会立即认为 DataNode 已经 “死亡”。要经过一段&lt;strong&gt;短暂的超时时长&lt;/strong&gt;后才会认为 DataNode 已经 “死亡”。HDFS 中默认的超时时长为 10 min + 30 s，可以用如下公式来表示这个超时时长：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;timeout = 2 * dfs.namenode.heartbeat.recheck-interval +10 * dfs.heartbeat.interval&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        其中，各参数的含义如下：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （1）&lt;code&gt;timeout&lt;/code&gt;：超时时长。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （2）&lt;code&gt;dfs.namenode.heartbeat.recheck-interval&lt;/code&gt;：检查过期 DataNode 的时间间隔，与 &lt;code&gt;dfs.heartbeat.interval&lt;/code&gt; 结合使用，默认的单位是 ms，默认时间是 &lt;strong&gt;5 min&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        （3）&lt;code&gt;dfs.heartbeat.interval&lt;/code&gt;：检测数据节点的时间间隔，默认的单位为 s，默认的时间是&lt;strong&gt; 3 s&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        所以，可以得出 DataNode 的默认超时时长为&lt;strong&gt; 630s&lt;/strong&gt;，如下所示：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;timeout = 2 * 5 * 60 + 10 * 3 = 630s&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        DataNode 的超时时长也可以在 &lt;code&gt;hdfs-site.xml&lt;/code&gt;文件中进行配置，代码如下所示：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&amp;lt;&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.namenode.heartbeat.recheck-interval&lt;span&gt;&amp;lt;/&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;3000&lt;span&gt;&amp;lt;/&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&amp;lt;/&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&amp;lt;&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.heartbeat.interval&lt;span&gt;&amp;lt;/&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;2&lt;span&gt;&amp;lt;/&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&amp;lt;/&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        根据上面的公式可以得出，在配置文件中配置的超时时长为：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;timeout = 2 * 3000 / 1000 + 10 * 2 = 26s&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        当 DataNode 被 NameNode 判定为 “&lt;strong&gt;死亡&lt;/strong&gt;”时，HDFS 就会马上自动进行&lt;strong&gt;数据块的容错复制&lt;/strong&gt;。此时，当被 NameNode 判定为 “死亡” 的 DataNode 重新加入集群中时，如果其存储的数据块并没有损坏，就会造成 &lt;strong&gt;HDFS 上某些数据块的备份数超过系统配置的备份数目&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        HDFS上&lt;strong&gt;删除多余的数据块&lt;/strong&gt;需要的时间长短和数据块报告的时间间隔有关。该参数可以在 &lt;code&gt;hdfs-site.xml&lt;/code&gt;文件中进行配置，代码如下所示：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;code&gt;&lt;span&gt;&amp;lt;&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;dfs.blockreport.intervalMsec&lt;span&gt;&amp;lt;/&lt;span&gt;name&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;21600000&lt;span&gt;&amp;lt;/&lt;span&gt;value&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;     &lt;span&gt;&amp;lt;&lt;span&gt;description&lt;/span&gt;&amp;gt;&lt;/span&gt;Determines block reporting interval in milliseconds.&lt;span&gt;&amp;lt;/&lt;span&gt;description&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&amp;lt;/&lt;span&gt;property&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        数据块报告的时间间隔默认为 &lt;code&gt;21600000&lt;/code&gt;ms，即 6h，可以通过调整此参数的大小来调整数据块报告的时间间隔。&lt;/p&gt;&lt;hr data-tool=&quot;mdnice编辑器&quot;/&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;小结&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;        本篇文章算是对 &lt;strong&gt;HDFS的架构&lt;/strong&gt;解释的比较透彻，相信不论是刚入门的小白，还是已经有了一定基础的大数据学者，看完都会有一定的收获，希望大家平时学习也能够多学会总结，&lt;strong&gt;&lt;span&gt;用输出倒逼自己输入&lt;/span&gt;&lt;/strong&gt;！        &lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;巨人的肩膀&lt;/span&gt;&lt;/h2&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;1、《海量数据处理与大数据技术实战》 &lt;/p&gt;&lt;p&gt;2、《大数据平台架构与原型实现》 &lt;/p&gt;&lt;p&gt;3、https://baike.baidu.com/item/hdfs/4836121?fr=aladdin&lt;/p&gt;&lt;p&gt; &lt;span&gt;4、http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_user_guide.html&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>c73722aff2911b1eabe3bfd8085be971</guid>
<title>深度学习六十年简史</title>
<link>https://toutiao.io/k/aw52jrh</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content                                                                     &quot; id=&quot;js_content&quot;&gt;
            &lt;h1 mp-original-font-size=&quot;22&quot; mp-original-line-height=&quot;30.799999237060547&quot;/&gt;&lt;section data-mpa-powered-by=&quot;yiban.io&quot; data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; data-style=&quot;white-space: normal; outline: 0px; max-width: 100%; color: rgb(34, 34, 34); letter-spacing: 0.544px; caret-color: rgb(34, 34, 34); background-color: rgb(255, 255, 255); font-family: &amp;quot;Helvetica Neue&amp;quot;, Helvetica, &amp;quot;Hiragino Sans GB&amp;quot;, &amp;quot;Microsoft YaHei&amp;quot;, Arial, sans-serif; visibility: visible; line-height: 27.2px; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__0&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section data-darkmode-bgcolor-16095509242984=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16095509242984=&quot;rgb(255, 255, 255)&quot; data-style=&quot;margin-top: 2em; padding-top: 0.5em; padding-bottom: 0.5em; outline: 0px; max-width: 100%; border-bottom-width: 1px; border-style: solid none; border-bottom-color: rgb(204, 204, 204); border-top-width: 1px; border-top-color: rgb(204, 204, 204); text-decoration: inherit; visibility: visible; line-height: 27.2px; box-sizing: border-box !important; overflow-wrap: break-word !important;&quot; class=&quot;js_darkmode__1&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot; data-darkmode-bgcolor-16535694808139=&quot;rgb(25, 25, 25)&quot; data-darkmode-original-bgcolor-16535694808139=&quot;#fff|rgb(255, 255, 255)&quot; data-darkmode-color-16535694808139=&quot;rgb(163, 163, 163)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(34, 34, 34)&quot;&gt;&lt;section&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;375&quot; data-backw=&quot;562&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6666666666666666&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/lBhAE42wKWoXphba4vcGNgWgiccNMGpNQV85ALvC4huvdmluliby9o2Tl4ePNYTAQITSUSQXmfyq23QGRDb4Q5QQ/640?wx_fmt=jpeg&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;作者｜Jean de Dieu Nyandwi&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;来源｜机器之心&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;1&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;1958 年：感知机的兴起&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;1958 年，弗兰克 · 罗森布拉特发明了感知机，这是一种非常简单的机器模型，后来成为当今智能机器的核心和起源。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;感知机是一个非常简单的二元分类器，可以确定给定的输入图像是否属于给定的类。为了实现这一点，它使用了单位阶跃激活函数。使用单位阶跃激活函数，如果输入大于 0，则输出为 1，否则为 0。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;下图是感知机的算法。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;317&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.5633333333333334&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8Zbf0FDpfRtgAtqrqic7WStfJiaPd5lRASW9Z2PN3Ze2bswzOiaDlG8RZlA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;感知机&lt;/span&gt;&lt;/em&gt;&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Frank 的意图不是将感知机构建为算法，而是构建成一种机器。感知机是在名为 Mark I 感知机的硬件中实现的。Mark I 感知机是一台纯电动机器。它有 400 个光电管（或光电探测器），其权重被编码到电位器中，权重更新（发生在反向传播中）由电动机执行。下图是 Mark I 感知机。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;442&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.7866666666666666&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZwlxAnOic1km41q65icyL6mVCCbpAGltxXdQunK2qnW292xlHbKMp6HaA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;Mark I 感知机。图片来自美国国家历史博物馆&lt;/span&gt;&lt;/em&gt;&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;就像你今天在新闻中看到的关于神经网络的内容一样，感知机也是当时的头条新闻。《纽约时报》报道说，“[海军] 期望电子计算机的初步模型能够行走、说话、观察、书写、自我复制并意识到它的存在”。今天，我们都知道机器仍然难以行走、说话、观察、书写、复制自己，而意识则是另一回事。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Mark I 感知机的目标仅仅是识别图像，而当时它只能识别两个类别。人们花了一些时间才知道添加更多层（感知机是单层神经网络）可以使网络具有学习复杂功能的能力。这进一步产生了多层感知机 (MLP)。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;2&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;1982-1986 : 循环神经网络 (RNN)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在多层感知机显示出解决图像识别问题的潜力之后，人们开始思考如何对文本等序列数据进行建模。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;循环神经网络是一类旨在处理序列的神经网络。与多层感知机 (MLP) 等前馈网络不同，RNN 有一个内部反馈回路，负责记住每个时间步长的信息状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;317&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.5633333333333334&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8Z2m9Z056OopK4dR1pEWcNxC9qb0iaEAmlyJia8yumSGpNgWQgUcVic2JTw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;前馈网络与循环神经网络&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第一种 RNN 单元在 1982 年到 1986 年之间被发现，但它并没有引起人们的注意，因为简单的 RNN 单元在用于长序列时会受到很大影响，主要是存在记忆力短和梯度不稳定的问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;3&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;1998：LeNet-5，第一个CNN架构&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet-5 是最早的卷积网络架构之一，于 1998 年用于文档识别。LeNet-5 由 3 个部分组成：2 个卷积层、2 个子采样或池化层和 3 个全连接层。卷积层中没有激活函数。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;正如论文所说，LeNet-5 已进行商业化应用，每天读取数百万张支票。下面是 LeNet-5 的架构。该图像取自其原始论文。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;159&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.2833333333333333&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZGH80KA78ElicVRYzZVJVicHJdoiaAvib5LSLU7YGgS75ZicWbsBDdD1uofw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LeNet-5 在当时确实是一个有影响力的研究，但它（常规的卷积网络）直到 20 年后才受到关注！LeNet-5 建立在早期工作的基础上，例如福岛邦彦提出的第一个卷积神经网络、反向传播（Hinton 等人，1986 年）和应用于手写邮政编码识别的反向传播（LeCun 等人，1989 年）。&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;4&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;1998：长短期记忆（LSTM）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;由于梯度不稳定的问题，简单 RNN 单元无法处理长序列问题。LSTM 是可用于处理长序列的 RNN 版本。LSTM 基本上是 RNN 单元的极端情况。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LSTM 单元的一个特殊设计差异是它有一个门机制，这是它可以控制多个时间步长的信息流的基础。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;简而言之，LSTM 使用门来控制从当前时间步长到下一个时间步长的信息流，有以下 4 种方式：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-1&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;27.200000762939453&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;输入门识别输入序列。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;遗忘门去掉输入序列中包含的所有不相关信息，并将相关信息存储在长期记忆中。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;LTSM 单元更新“更新单元“的状态值。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;输出门控制必须发送到下一个时间步长的信息。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;299&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.5316666666666666&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZryFzJRVLLiaDy9BiaXrXzS54FYepHBkVEIMBFtrqyhibIwQIvfTWkv2Yw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;LSTM 架构。图片取自 MIT 的课程《6.S191 Introduction to Deep Learning》&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LSTM 处理长序列的能力使其成为适合各种序列任务的神经网络架构，例如文本分类、情感分析、语音识别、图像标题生成和机器翻译。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;LSTM 是一种强大的架构，但它的计算成本很高。2014 年推出的 GRU（Gated Recurrent Unit）可以解决这个问题。与 LSTM 相比，GRU的参数更少，效果也很好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;5&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;2012 年：ImageNet 挑战赛、AlexNet 和 ConvNet 的兴起&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;如果跳过 ImageNet 大规模视觉识别挑战赛 (ILSVRC) 和 AlexNet，就几乎不可能讨论神经网络和深度学习的历史。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ImageNet 挑战赛的唯一目标是评估大型数据集上的图像分类和对象分类架构。它带来了许多新的、强大的、有趣的视觉架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;挑战赛始于 2010 年，但在 2012 年发生了变化，AlexNet 以 15.3% 的 Top 5 低错误率赢得了比赛，这几乎是此前获胜者错误率的一半。AlexNet 由 5 个卷积层、随后的最大池化层、3 个全连接层和一个 Softmax 层组成。AlexNet 提出了深度卷积神经网络可以很好地处理视觉识别任务的想法。但当时，这个观点还没有深入到其他应用上！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在随后的几年里，ConvNets 架构不断变得更大并且工作得更好。例如，有 19 层的 VGG 以 7.3% 的错误率赢得了挑战。GoogLeNet（Inception-v1）更进一步，将错误率降低到 6.7%。2015 年，ResNet（Deep Residual Networks）扩展了这一点，并将错误率降低到 3.6%，并表明通过残差连接，我们可以训练更深的网络（超过 100 层），在此之前，训练如此深的网络是不可能的。人们发现更深层次的网络做得更好，这导致产生了其他新架构，如 ResNeXt、Inception-ResNet、DenseNet、Xception 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;读者可以在这里找到这些架构和其他现代架构的总结和实现：&lt;/span&gt;&lt;span&gt;&lt;em&gt;https://github.com/Nyandwi/ModernConvNets&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;401&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.7126903553299493&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;985&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZldHTvuTia3jRlwhuSImFa6TDQRSnC8Pkm2hPMVuvhjtWv7KQsfdmptw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;ModernConvNets 库&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;287&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.51&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZRGbgl9jjTTb8H1v5bs0P3ibPTxBrfdSU82vJYtEwWlnRPEakpLZU4zw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;ImageNet 挑战赛。图片来自课程《 CS231n》&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;6&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;2014 年 : 深度生成网络&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生成网络用于从训练数据中生成或合成新的数据样本，例如图像和音乐。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;生成网络有很多种类型，但最流行的是由 Ian Goodfellow 在 2014 年创建的生成对抗网络 (GAN)。GAN 由两个主要组件组成：生成假样本的生成器，以及区分真实样本和生成器生成样本的判别器。生成器和鉴别器可以说是互相竞争的关系。他们都是独立训练的，在训练过程中，他们玩的是零和游戏。生成器不断生成欺骗判别器的假样本，而判别器则努力发现那些假样本（参考真实样本）。在每次训练迭代中，生成器在生成接近真实的假样本方面做得更好，判别器必须提高标准来区分不真实的样本和真实样本。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;GAN 一直是深度学习社区中最热门的研究之一，该社区以生成伪造的图像和 Deepfake 视频而闻名。如果读者对 GAN 的最新进展感兴趣，可以阅读 StyleGAN2、DualStyleGAN、ArcaneGAN 和 AnimeGANv2 的简介。如需 GAN 资源的完整列表：&lt;/span&gt;&lt;span&gt;&lt;em&gt;https://github.com/nashory/gans-awesome-applications&lt;/em&gt;&lt;/span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;。下图说明了 GAN 的模型架构。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;384&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.6833333333333333&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZUWTbU6Z6ZdSXyCibZfJicDSIZbSOSmHQ4Q62kN3OYrojmicy3VpKnLBibg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;生成对抗网络（GAN）&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GAN 是生成模型的一种。其他流行的生成模型类型还有 Variation Autoencoder (变分自编码器，VAE)、AutoEncoder （自编码器）和扩散模型等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;7&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;2017 年：Transformers 和注意力机制&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;时间来到 2017 年。ImageNet 挑战赛结束了。新的卷积网络架构也被制作出来。计算机视觉社区的每个人都对当前的进展感到高兴。核心计算机视觉任务（图像分类、目标检测、图像分割）不再像以前那样复杂。人们可以使用 GAN 生成逼真的图像。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;NLP 似乎落后了。但是随后出现了一些事情，并且在整个网络上都成为了头条新闻：一种完全基于注意力机制的新神经网络架构横空出世。并且 NLP 再次受到启发，在随后的几年，注意力机制继续主导其他方向（最显著的是视觉）。该架构被称为 Transformer 。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在此之后的 5 年，也就是现在，我们在这里谈论一下这个最大的创新成果。Transformer 是一类纯粹基于注意力机制的神经网络算法。Transformer 不使用循环网络或卷积。它由多头注意力、残差连接、层归一化、全连接层和位置编码组成，用于保留数据中的序列顺序。下图说明了 Transformer 架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;579&quot; data-backw=&quot;562&quot; data-ratio=&quot;1.03&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8Z6YWTHwWwD0ty1YCAR9dUkDVdVwaduzQZ22CjYBGE5ib47rLU4Oua2gw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;图片来自于《Attention Is All You Need》&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Transformer 彻底改变了 NLP，目前它也在改变着计算机视觉领域。在 NLP 领域，它被用于机器翻译、文本摘要、语音识别、文本补全、文档搜索等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;读者可以在其论文 《Attention is All You Need》 中了解有关 Transformer 的更多信息。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;2018 年至今&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 2017 年以来，深度学习算法、应用和技术突飞猛进。为了清楚起见，后来的介绍是按类别划分。在每个类别中，我们都会重新审视主要趋势和一些最重要的突破。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;Vision Transformers&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Transformer 在 NLP 中表现出优异的性能后不久，一些勇于创新的人就迫不及待地将注意力机制用到了图像领域。在论文《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中，谷歌的几位研究人员表明，对直接在图像块序列上运行的正常 Transformer 进行轻微修改，就可以在图像分类数据集上产生实质性的结果。他们将这种架构称为 Vision Transformer (ViT)，它在大多数计算机视觉基准测试中都有不错表现（在作者撰写本文时，ViT 是 Cifar-10 上最先进的分类模型）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;ViT 设计师并不是第一个尝试在识别任务中使用注意力机制的人。我们可以在论文 Attention Augmented Convolutional Networks 中找到第一个使用注意力机制的记录，这篇论文试图结合自注意力机制和卷积（摆脱卷积主要是由于 CNN 引入的空间归纳偏置）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;另一个例子见于论文《Visual Transformers: Token-based Image Representation and Processing for Computer Vision，这篇论文在基于滤波器的 token 或视觉 token 上运行 Transformer。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两篇论文和许多其他未在此处列出的论文突破了一些基线架构（主要是 ResNet）的界限，但当时并没有超越当前的基准。ViT 确实是最伟大的论文之一。这篇论文最重要的见解之一是 ViT 设计师实际上使用图像 patch 作为输入表示。他们对 Transformer 架构没有太大的改变。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;302&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.5366666666666666&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZbSxZbMP3MkBO7oXa1UU3MEnhpVyNRLX2FWRbhrjD4tic3ydyNt1ssFw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;Vision Transformer(ViT)&lt;/span&gt;&lt;/em&gt;&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;除了使用图像 patch 之外，使 Vision Transformer 成为强大架构的结构是 Transformer 的超强并行性及其缩放行为。但就像生活中的一切一样，没有什么是完美的。一开始，ViT 在视觉下游任务（目标检测和分割）上表现不佳。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在引入 Swin Transformers 之后，Vision Transformer 开始被用作目标检测和图像分割等视觉下游任务的骨干网络。Swin Transformer 超强性能的核心亮点是由于在连续的自注意力层之间使用了移位窗口。下图描述了 Swin Transformer 和 Vision Transformer （ViT） 在构建分层特征图方面的区别。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;516&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.9183333333333333&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZnxibfSKj0KqqzjQX3zOKXWK80UKah9BKnuDiaN1tgxHBLUgcoE6ewKzg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;图片来自 Swin Transformer 原文&lt;/span&gt;&lt;/em&gt;&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Vision Transformer 一直是近来最令人兴奋的研究领域之一。读者可以在论文《Transformers in Vision: A Survey》中了解更多信息。其他最新视觉 Transformer 还有 CrossViT、ConViT 和 SepViT 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;视觉和语言模型&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;视觉和语言模型通常被称为多模态。它们是涉及视觉和语言的模型，例如文本到图像生成（给定文本，生成与文本描述匹配的图像）、图像字幕（给定图像，生成其描述）和视觉问答（给定一个图像和关于图像中内容的问题，生成答案）。很大程度上，Transformer 在视觉和语言领域的成功促成了多模型作为一个单一的统一网络。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;实际上，所有视觉和语言任务都利用了预训练技术。在计算机视觉中，预训练需要对在大型数据集（通常是 ImageNet）上训练的网络进行微调，而在 NLP 中，往往是对预训练的 BERT 进行微调。要了解有关 V-L 任务中预训练的更多信息，请阅读论文《A Survey of Vision-Language Pre-Trained Models》。有关视觉和语言任务、数据集的一般概述，请查看论文《Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods》。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前段时间，OpenAI 发布了 DALL·E 2（改进后的 DALL·E），这是一种可以根据文本生成逼真图像的视觉语言模型。现有的文本转图像模型有很多，但 DALL·E 2 的分辨率、图像标题匹配度和真实感都相当出色。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DALL·E 2 尚未对公众开放，以下是 DALL·E 2 创建的一些图像示例。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;343&quot; data-backw=&quot;562&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.61&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZjZVYAuOyvsq1BLhnugSkShpgyLCjWQONqyViap2vkN5wpm8w2bQVg2A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;上面呈现的 DALL·E 2 生成的图像取自一些 OpenAI 员工，例如 @sama、@ilyasut、@model_mechanic 和 openaidalle。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;大规模语言模型 (LLM)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;语言模型有多种用途。它们可用于预测句子中的下一个单词或字符、总结一段文档、将给定文本从一种语言翻译成另一种语言、识别语音或将一段文本转换为语音。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;开玩笑地说，发明 Transformers 的人必须为语言模型在朝着大规模参数化方向前进而受到指责（但实际上没有人应该受到责备，Transformers 是过去十年中最伟大的发明之一，大模型令人震惊的地方在于：如果给定足够的数据和计算，它总能更好地工作）。在过去的 5 年中，语言模型的大小一直在不断增长。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在引入论文《Attention is all you need》一年后，大规模语言模型开始出现。2018 年，OpenAI 发布了 GPT（Generative Pre-trained Transformer），这是当时最大的语言模型之一。一年后，OpenAI 发布了 GPT-2，一个拥有 15 亿个参数的模型。又一年后，他们发布了 GPT-3，它有 1750 亿个参数，用了 570GB 的 文本来训练。这个模型有 175B 的参数，模型有 700GB。根据 lambdalabs 的说法，如果使用在市场上价格最低的 GPU 云&lt;/span&gt;&lt;span&gt;训练GPT-3，需要 366 年，花费 460 万美元！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;GPT-n 系列型号仅仅是个开始。还有其他更大的模型接近甚至比 GPT-3 更大。如：NVIDIA Megatron-LM 有 8.3B 参数；最新的 DeepMind Gopher 有 280B 参数。2022 年 4 月 12 日，DeepMind 发布了另一个名为 Chinchilla 的 70B 语言模型，尽管比 Gopher、GPT-3 和 Megatron-Turing NLG（530B 参数）小，但它的性能优于许多语言模型。Chinchilla 的论文表明，现有的语言模型是训练不足的，具体来说，它表明通过将模型的大小加倍，数据也应该加倍。但是，几乎在同一周内又出现了具有 5400 亿个参数的 Google Pathways 语言模型（PaLM）！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;245&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZYAs0ibHazg5UwOw3X4J0xj0AO5ksfR9sjJz7SAVPybnss1quzGY4wVQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;Chinchilla 语言模型&lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;代码生成模型&lt;/strong&gt;&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;代码生成是一项涉及补全给定代码或根据自然语言或文本生成代码的任务，或者简单地说，它是可以编写计算机程序的人工智能系统。可以猜到，现代代码生成器是基于 Transformer 的。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;可以确定地说，人们已经开始考虑让计算机编写自己的程序了（就像我们梦想教计算机做的所有其他事情一样），不过代码生成器是在 OpenAI 发布 Codex 后受到关注。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;Codex 是在 GitHub 公共仓库和其他公共源代码上微调的 GPT-3。OpenAI 表示：“OpenAI Codex 是一种通用编程模型，这意味着它基本上可以应用于任何编程任务（尽管结果可能会有所不同）。我们已经成功地将它用于编译、解释代码和重构代码。但我们知道，我们只触及了可以做的事情的皮毛。” 目前，由 Codex 支持的 GitHub Copilot 扮演着结对程序员的角色。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在我使用 Copilot 后，我对它的功能感到非常惊讶。作为不编写 Java 程序的人，我用它来准备我的移动应用程序（使用 Java）考试。人工智能帮助我准备学术考试真是太酷了！&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 OpenAI 发布 Codex 几个月后，DeepMind 发布了 AlphaCode，这是一种基于 Transformer 的语言模型，可以解决编程竞赛问题。AlphaCode 发布的博文称：“AlphaCode 通过解决需要结合批判性思维、逻辑、算法、编码和自然语言理解的新问题，在编程竞赛的参与者中估计排名前 54%。” 解决编程问题（或一般的竞争性编程）非常困难（每个做过技术面试的人都同意这一点），正如 Dzmitry 所说，击败 “人类水平仍然遥遥无期”。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;前不久，来自 Meta AI 的科学家发布了 InCoder，这是一种可以生成和编辑程序的生成模型。&lt;/span&gt;&lt;span mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;更多关于代码生成的论文和模型可以在这里找到：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em&gt;https://paperswithcode.com/task/code-generation/codeless&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;再次回到感知机&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在卷积神经网络和 Transformer 兴起之前的很长一段时间里，深度学习都围绕着感知机展开。ConvNets 在取代 MLP 的各种识别任务中表现出优异的性能。视觉 Transformer 目前也展示出似乎是一个很有前途的架构。但是感知机完全死了吗？答案可能不是。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在 2021 年 7 月，研究人员发表了两篇基于感知机的论文。一个是 MLP-Mixer: An all-MLP Architecture for Vision，另一个是 Pay Attention to MLPs（gMLP）.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;MLP-Mixer 声称卷积和注意力都不是必需的。这篇论文仅使用多层感知机 (MLP)，就在图像分类数据集上取得了很高的准确性。MLP-Mixer 的一个重要亮点是，它包含两个主要的 MLP 层：一个独立应用于图像块（通道混合），另一个是层跨块应用（空间混合）。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;gMLP 还表明，通过避免使用自注意和卷积（当前 NLP 和 CV 的实际使用的方式），可以在不同的图像识别和 NLP 任务中实现很高的准确性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;204&quot; data-backw=&quot;562&quot; data-ratio=&quot;0.36333333333333334&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8MepLrCicqqVA5lLaGM4Q8ZjovzPTy3b6gNTWOCvSFUP4bj7qWiakk1JUsj1u6Kvicia1AboLl0hWUKg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;读者显然不会使用 MLP 去获得最先进的性能，但它们与最先进的深度网络的可比性却是令人着迷的。&lt;br mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;15&quot; mp-original-line-height=&quot;29.75&quot;&gt;再次使用卷积网络：2020 年代的卷积网络&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;自 Vision Transformer（2020 年）推出以来，计算机视觉的研究围绕着 Transformer 展开（在 NLP 领域，Transformer 已经是一种规范）。Vision Transformer （ViT） 在图像分类方面取得了最先进的结果，但在视觉下游任务（对象检测和分割）中效果不佳。随着 Swin Transformers 的推出，使得Vision Transformer 很快也接管了视觉下游任务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;很多人（包括我自己）都喜欢卷积神经网络。卷积神经网络确实能起效，而且放弃已经被证明有效的东西是很难的。这种对深度网络模型结构的热爱让一些杰出的科学家回到过去，研究如何使卷积神经网络（准确地说是 ResNet）现代化，使其具有和 Vision Transformer 同样的吸引人的特征。特别是，他们探讨了「Transformers 中的设计决策如何影响卷积神经网络的性能？」这个问题。他们想把那些塑造了 Transformer 的秘诀应用到 ResNet 上。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Meta AI 的 Saining Xie 和他的同事们采用了他们在论文中明确陈述的路线图，最终形成了一个名为 ConvNeXt 的 ConvNet 架构。ConvNeXt 在不同的基准测试中取得了可与 Swin Transformer 相媲美的结果。读者可以通过 ModernConvNets 库（现代 CNN 架构的总结和实现）了解更多关于他们采用的路线图。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;9&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong mp-original-font-size=&quot;16&quot; mp-original-line-height=&quot;29.75&quot;&gt;结论&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;深度学习是一个非常有活力、非常宽广的领域，很难概括其中所发生的一切。作者只触及了表面，论文多到一个人读不完，很难跟踪所有内容。例如，我们没有讨论强化学习和深度学习算法，如 AlphaGo、蛋白质折叠 AlphaFold（这是最大的科学突破之一）、深度学习框架的演变（如 TensorFlow 和 PyTorch），以及深度学习硬件。或许，还有其他重要的事情构成了我们没有讨论过的深度学习历史、算法和应用程序的很大一部分。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为一个小小的免责声明，读者可能已经注意到，作者偏向于计算机视觉的深度学习，对其他专门为 NLP 设计的重要深度学习技术作者可能并没有涉及。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此外，很难确切地知道某项特定技术是什么时候发表的，或者是谁最先发表的，因为大多数奇特的东西往往受到前人作品的启发。如有纰漏，读者可以去原文评论区与作者讨论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;（本文经授权后发布，原文链接：&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;em data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;17&quot; mp-original-line-height=&quot;29.75&quot;&gt;&lt;span data-darkmode-color-16535694808139=&quot;rgb(136, 136, 136)&quot; data-darkmode-original-color-16535694808139=&quot;#fff|rgb(136, 136, 136)|rgb(136, 136, 136)&quot; mp-original-font-size=&quot;12&quot; mp-original-line-height=&quot;29.75&quot;&gt;https://www.getrevue.co/profile/deeprevision/issues/a-revised-history-of-deep-learning-issue-1-1145664）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其他人都在看&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;点击“&lt;/span&gt;&lt;strong&gt;&lt;span&gt;阅读原文&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;”&lt;/span&gt;&lt;span&gt;，欢迎下载体验OneFlow v0.7.0&lt;/span&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;h2&gt;&lt;hr/&gt;&lt;/h2&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzU5ODY2MTk3Nw==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/lBhAE42wKWomZrXDbZnegyP5qVxvr0Bg52ibDia1lqPAPvhiboIbuDsvznywQcExLn2dq6mfaE2hVBQ8cgZh8meMA/0?wx_fmt=png&quot; data-nickname=&quot;OneFlow&quot; data-alias=&quot;OneFlowTechnology&quot; data-signature=&quot;不止于成为世界上最快的开源深度学习框架&quot; data-from=&quot;0&quot;/&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-backh=&quot;342&quot; data-backw=&quot;578&quot; data-fileid=&quot;100002438&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5920529801324503&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;755&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/lBhAE42wKWqQktibibjLdEDJbrYibgRlmoZEoeg7hMrBGkApcTvrX4FgQibiamibcoIOibBW8ozpzRwc1EJn1zfx7OEfw/640?wx_fmt=png&quot;/&gt;​&lt;/section&gt;
          &lt;/div&gt;

          

          



           
                                
                    
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>