<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>38218c2e5d03bf32839f8fd37202bce5</guid>
<title>高吞吐、低延迟 Java 应用的 GC 优化实践</title>
<link>https://toutiao.io/k/92nisj7</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;src-views-article-detail-main-module__content--2qOBd markdown-body&quot;&gt;&lt;p&gt;“以下信息节选自涤生的翻译内容”&lt;/p&gt;
&lt;p&gt;本篇原文作者是 LinkedIn 的 Swapnil Ghike，这篇文章讲述了 LinkedIn 的 Feed 产品的 GC 优化过程，虽然文章写作于 April 8, 2014，但其中的很多内容和知识点非常有学习和参考意义。&lt;/p&gt;
&lt;h1&gt;背景&lt;/h1&gt;
&lt;p&gt;高性能应用构成了现代网络的支柱。LinkedIn 内部有许多高吞吐量服务来满足每秒成千上万的用户请求。为了获得最佳的用户体验，以低延迟响应这些请求是非常重要的。&lt;/p&gt;
&lt;p&gt;例如，我们的用户经常使用的产品是 Feed —— 它是一个不断更新的专业活动和内容的列表。Feed 在 LinkedIn 的系统中随处可见，包括公司页面、学校页面以及最重要的主页资讯信息。基础 Feed 数据平台为我们的经济图谱（会员、公司、群组等）中各种实体的更新建立索引，它必须高吞吐低延迟地实现相关的更新。如下图，LinkedIn Feeds 信息展示：&lt;br/&gt;
&lt;img src=&quot;https://a.perfma.net/img/233863&quot; alt=&quot;5.jpg&quot;/&gt;&lt;br/&gt;
为了将这些高吞吐量、低延迟类型的 Java 应用程序用于生产，开发人员必须确保在应用程序开发周期的每个阶段都保持一致的性能。确定最佳垃圾收集（Garbage Collection, GC）配置对于实现这些指标至关重要。&lt;/p&gt;
&lt;p&gt;这篇博文将通过一系列步骤来明确需求并优化 GC，它的目标读者是对使用系统方法进行 GC 优化来实现应用的高吞吐低延迟目标感兴趣的开发人员。在 LinkedIn 构建下一代 Feed 数据平台的过程中，我们总结了该方法。这些方法包括但不限于以下几点：并发标记清除（Concurrent Mark Sweep，CMS（参考[2]） 和 G1（参考 [3]） 垃圾回收器的 CPU 和内存开销、避免长期存活对象导致的持续 GC、优化 GC 线程任务分配提升性能，以及可预测 GC 停顿时间所需的 OS 配置。&lt;/p&gt;
&lt;h1&gt;优化 GC 的正确时机？&lt;/h1&gt;
&lt;p&gt;GC 的行为可能会因代码优化以及工作负载的变化而变化。因此，在一个已实施性能优化的接近完成的代码库上进行 GC 优化非常重要。而且在端到端的基本原型上进行初步分析也很有必要，该原型系统使用存根代码并模拟了可代表生产环境的工作负载。这样可以获取该架构延迟和吞吐量的真实边界，进而决定是否进行纵向或横向扩展。&lt;/p&gt;
&lt;p&gt;在下一代 Feed 数据平台的原型开发阶段，我们几乎实现了所有端到端的功能，并且模拟了当前生产基础设施提供的查询工作负载。这使我们在工作负载特性上有足够的多样性，可以在足够长的时间内测量应用程序性能和 GC 特征。&lt;/p&gt;
&lt;h1&gt;优化 GC 的步骤&lt;/h1&gt;
&lt;p&gt;下面是一些针对高吞吐量、低延迟需求优化 GC 的总体步骤。此外，还包括在 Feed 数据平台原型实施的具体细节。尽管我们还对 G1 垃圾收集器进行了试验，但我们发现 ParNew/CMS 具有最佳的 GC 性能。&lt;/p&gt;
&lt;h2&gt;1. 理解 GC 基础知识&lt;/h2&gt;
&lt;p&gt;由于 GC 优化需要调整大量的参数，因此理解 GC 工作机制非常重要。Oracle 的 Hotspot JVM 内存管理白皮书（参考 [4] ）是开始学习 Hotspot JVM GC 算法非常好的资料。而了解 G1 垃圾回收器的理论知识，可以参阅（参考 [3]）。&lt;/p&gt;
&lt;h2&gt;2. 仔细考量 GC 需求&lt;/h2&gt;
&lt;p&gt;为了降低对应用程序性能的开销，可以优化 GC 的一些特征。像吞吐量和延迟一样，这些 GC 特征应该在长时间运行的测试中观察到，以确保应用程序能够在经历多个 GC 周期中处理流量的变化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stop-the-world 回收器回收垃圾时会暂停应用线程。停顿的时长和频率不应该对应用遵守 SLA 产生不利的影响。&lt;/li&gt;
&lt;li&gt;并发 GC 算法与应用线程竞争 CPU 周期。这个开销不应该影响应用吞吐量。&lt;/li&gt;
&lt;li&gt;非压缩 GC 算法会引起堆碎片化，进而导致的 Full GC 长时间 Stop-the-world，因此，堆碎片应保持在最小值。&lt;/li&gt;
&lt;li&gt;垃圾回收工作需要占用内存。某些 GC 算法具有比其他算法更高的内存占用。如果应用程序需要较大的堆空间，要确保 GC 的内存开销不能太大。&lt;/li&gt;
&lt;li&gt;要清楚地了解 GC 日志和常用的 JVM 参数，以便轻松地调整 GC 行为。因为 GC 运行随着代码复杂性增加或工作负载特性的改变而发生变化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们使用 Linux 操作系统、Hotspot Java7u51、32GB 堆内存、6GB 新生代（Young Gen）和 -XX:CMSInitiatingOccupancyFraction 值为 70（Old GC 触发时其空间占用率）开始实验。设置较大的堆内存是用来维持长期存活对象的对象缓存。一旦这个缓存生效，晋升到 Old Gen 的对象速度会显著下降。&lt;/p&gt;
&lt;p&gt;使用最初的 JVM 配置，每 3 秒发生一次 80ms 的 Young GC 停顿，超过 99.9% 的应用请求延迟 100ms（999线）。这样的 GC 效果可能适合于 SLA 对延迟要求不太严格应用。然而，我们的目标是尽可能减少应用请求的 999 线。GC 优化对于实现这一目标至关重要。&lt;/p&gt;
&lt;h2&gt;3. 理解 GC 指标&lt;/h2&gt;
&lt;p&gt;衡量应用当前情况始终是优化的先决条件。了解 GC 日志的详细细节（参考 [5]）（使用以下选项）：&lt;/p&gt;
&lt;pre class=&quot;hljs&quot;&gt;&lt;code class=&quot;hljs-code-wrap&quot;&gt;-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps 
-XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime
&lt;/code&gt;&lt;button class=&quot;pre-button&quot;&gt;复制&lt;/button&gt;&lt;/pre&gt;
&lt;p&gt;可以对该应用的 GC 特征有总体的把握。&lt;/p&gt;
&lt;p&gt;在 LinkedIn 的内部监控 inGraphs 和报表系统 Naarad，生成了各种有用的指标可视化图形，比如 GC 停顿时间百分比、一次停顿最大持续时间以及长时间内 GC 频率。除了 Naarad，有很多开源工具比如 gclogviewer 可以从 GC 日志创建可视化图形。在此阶段，可以确定 GC 频率和暂停持续时间是否满足应用程序满足延迟的要求。&lt;/p&gt;
&lt;h2&gt;4. 降低 GC 频率&lt;/h2&gt;
&lt;p&gt;在分代 GC 算法中，降低 GC 频率可以通过：(1) 降低对象分配/晋升率；(2) 增加各代空间的大小。&lt;/p&gt;
&lt;p&gt;在 Hotspot JVM 中，Young GC 停顿时间取决于一次垃圾回收后存活下来的对象的数量，而不是 Young Gen 自身的大小。增加 Young Gen 大小对于应用性能的影响需要仔细评估：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果更多的数据存活而且被复制到 Survivor 区域，或者每次 GC 更多的数据晋升到 Old Gen，增加 Young Gen 大小可能导致更长的 Young GC 停顿。较长的 GC 停顿可能会导致应用程序延迟增加和(或)吞吐量降低。&lt;/li&gt;
&lt;li&gt;另一方面，如果每次垃圾回收后存活对象数量不会大幅增加，停顿时间可能不会延长。在这种情况下，降低 GC 频率可能会使整个应用总体延迟降低和(或)吞吐量增加。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于大部分为短期存活对象的应用，仅仅需要控制上述的参数；对于长期存活对象的应用，就需要注意，被晋升的对象可能很长时间都不能被 Old GC 周期回收。如果 Old GC 触发阈值（Old Gen 占用率百分比）比较低，应用将陷入持续的 GC 循环中。可以通过设置高的 GC 触发阈值可避免这一问题。&lt;/p&gt;
&lt;p&gt;由于我们的应用在堆中维持了长期存活对象的较大缓存，将 Old GC 触发阈值设置为&lt;/p&gt;
&lt;pre class=&quot;hljs&quot;&gt;&lt;code class=&quot;hljs-code-wrap&quot;&gt;-XX:CMSInitiatingOccupancyFraction=92 -XX:+UseCMSInitiatingOccupancyOnly
&lt;/code&gt;&lt;button class=&quot;pre-button&quot;&gt;复制&lt;/button&gt;&lt;/pre&gt;
&lt;p&gt;来增加触发 Old GC 的阈值。我们也试图增加 Young Gen 大小来减少 Young GC 频率，但是并没有采用，因为这增加了应用的 999 线。&lt;/p&gt;
&lt;h2&gt;5. 缩短 GC 停顿时间&lt;/h2&gt;
&lt;p&gt;减少 Young Gen 大小可以缩短 Young GC 停顿时间，因为这可能导致被复制到 Survivor 区域或者被晋升的数据更少。但是，正如前面提到的，我们要观察减少 Young Gen 大小和由此导致的 GC 频率增加对于整体应用吞吐量和延迟的影响。Young GC 停顿时间也依赖于 tenuring threshold （晋升阈值）和 Old Gen 大小（如步骤 6 所示）。&lt;/p&gt;
&lt;p&gt;在使用 CMS GC 时，应将因堆碎片或者由堆碎片导致的 Full GC 的停顿时间降低到最小。通过控制对象晋升比例和减小 -XX:CMSInitiatingOccupancyFraction 的值使 Old GC 在低阈值时触发。所有选项的细节调整和他们相关的权衡，请参考 Web Services 的 Java 垃圾回收（参考 [5] ）和 Java 垃圾回收精粹（参考 [6]）。&lt;/p&gt;
&lt;p&gt;我们观察到 Eden 区域的大部分 Young Gen 被回收，几乎没有 3-8 年龄对象在 Survivor 空间中死亡，所以我们将 tenuring threshold 从 8 降低到 2 (使用选项：-XX:MaxTenuringThreshold=2 ),以降低 Young GC 消耗在数据复制上的时间。&lt;/p&gt;
&lt;p&gt;我们还注意到 Young GC 暂停时间随着 Old Gen 占用率上升而延长。这意味着来自 Old Gen 的压力使得对象晋升花费更多的时间。为解决这个问题，将总的堆内存大小增加到 40GB，减小 -XX:CMSInitiatingOccupancyFraction 的值到 80，更快地开始 Old GC。尽管 -XX:CMSInitiatingOccupancyFraction 的值减小了，增大堆内存可以避免频繁的 Old GC。在此阶段，我们的结果是 Young GC 暂停 70ms，应用的 999 线在 80ms。&lt;/p&gt;
&lt;h2&gt;6. 优化 GC 工作线程的任务分配&lt;/h2&gt;
&lt;p&gt;为了进一步降低 Young GC 停顿时间，我们决定研究 GC 线程绑定任务的参数来进行优化。&lt;/p&gt;
&lt;p&gt;-XX:ParGCCardsPerStrideChunk 参数控制 GC 工作线程的任务粒度，可以帮助不使用补丁而获得最佳性能，这个补丁用来优化 Young GC 中的 Card table（卡表）扫描时间（参考[7]）。有趣的是，Young GC 时间随着 Old Gen 的增加而延长。将这个选项值设为 32678，Young GC 停顿时间降低到平均 50ms。此时应用的 999 线在 60ms。&lt;/p&gt;
&lt;p&gt;还有一些的参数可以将任务映射到 GC 线程，如果操作系统允许的话，-XX:+BindGCTaskThreadsToCPUs 参数可以绑定 GC 线程到个别的 CPU 核（见解释 [1]）。使用亲缘性 -XX:+UseGCTaskAffinity 参数可以将任务分配给 GC 工作线程（见解释 [2]）。然而，我们的应用并没有从这些选项带来任何好处。实际上，一些调查显示这些选项在 Linux 系统不起作用。&lt;/p&gt;
&lt;h2&gt;7. 了解 GC 的 CPU 和内存开销&lt;/h2&gt;
&lt;p&gt;并发 GC 通常会增加 CPU 使用率。虽然我们观察到 CMS 的默认设置运行良好，但是 G1 收集器的并发 GC 工作会导致 CPU 使用率的增加，显著降低了应用程序的吞吐量和延迟。与 CMS 相比，G1 还增加了内存开销。对于不受 CPU 限制的低吞吐量应用程序，GC 导致的高 CPU 使用率可能不是一个紧迫的问题。&lt;/p&gt;
&lt;p&gt;下图是 ParNew/CMS 和 G1 的 CPU 使用百分比：相对来说 CPU 使用率变化明显的节点使用 G1 参数 -XX:G1RSetUpdatingPauseTimePercent=20：&lt;br/&gt;
&lt;img src=&quot;https://a.perfma.net/img/233872&quot; alt=&quot;6.jpg&quot;/&gt;&lt;br/&gt;
下图是 ParNew/CMS 和 G1 每秒服务的请求数：吞吐量较低的节点使用 G1 参数 -XX:G1RSetUpdatingPauseTimePercent=20&lt;br/&gt;
&lt;img src=&quot;https://a.perfma.net/img/233885&quot; alt=&quot;7.jpg&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;8. 为 GC 优化系统内存和 I/O 管理&lt;/h2&gt;
&lt;p&gt;通常来说，GC 停顿有两种特殊情况：(1) 低 user time，高 sys time 和高 real time (2) 低 user time，低 sys time 和高 real time。这意味着基础的进程/OS设置存在问题。情况 (1) 可能意味着 JVM 页面被 Linux 窃取；情况 (2) 可能意味着 GC 线程被 Linux 用于磁盘刷新，并卡在内核中等待 I/O。在这些情况下，如何设置参数可以参考该 PPT（参考 [8]）。&lt;/p&gt;
&lt;p&gt;另外，为了避免在运行时造成性能损失，我们可以使用 JVM 选项 -XX:+AlwaysPreTouch 在应用程序启动时先访问所有分配给它的内存，让操作系统把内存真正的分配给 JVM。我们还可以将 vm.swappability 设置为0，这样操作系统就不会交换页面到 swap（除非绝对必要）。&lt;/p&gt;
&lt;p&gt;可能你会使用 mlock 将 JVM 页固定到内存中，这样操作系统就不会将它们交换出去。但是，如果系统用尽了所有的内存和交换空间，操作系统将终止一个进程来回收内存。通常情况下，Linux 内核会选择具有高驻留内存占用但运行时间不长的进程（OOM 情况下杀死进程的工作流（参考[9]）进行终止。在我们的例子中，这个进程很有可能就是我们的应用程序。优雅的降级是服务优秀的属性之一，不过服务突然终止的可能性对于可操作性来说并不好 —— 因此，我们不使用 mlock，只是通过 vm.swapability 来尽可能避免交换内存页到 swap 的惩罚。&lt;/p&gt;
&lt;p&gt;LinkedIn 动态信息数据平台的 GC 优化&lt;/p&gt;
&lt;p&gt;对于该 Feed 平台原型系统，我们使用 Hotspot JVM 的两个 GC 算法优化垃圾回收：&lt;/p&gt;
&lt;p&gt;Young GC 使用 ParNew，Old GC 使用 CMS。&lt;br/&gt;
Young Gen 和 Old Gen 使用 G1。G1 试图解决堆大小为 6GB 或更大时，暂停时间稳定且可预测在 0.5 秒以下的问题。在我们用 G1 实验过程中，尽管调整了各种参数，但没有得到像 ParNew/CMS 一样的 GC 性能或停顿时间的可预测值。我们查询了使用 G1 发生内存泄漏相关的一个 bug（见解释[3]），但还不能确定根本原因。&lt;br/&gt;
使用 ParNew/CMS，应用每三秒进行一次 40-60ms 的 Young GC 和每小时一个 CMS GC。JVM 参数如下：&lt;/p&gt;
&lt;pre class=&quot;hljs&quot;&gt;&lt;code class=&quot;hljs-code-wrap&quot;&gt;// JVM sizing options
-server -Xms40g -Xmx40g -XX:MaxDirectMemorySize=4096m -XX:PermSize=256m -XX:MaxPermSize=256m   
// Young generation options
-XX:NewSize=6g -XX:MaxNewSize=6g -XX:+UseParNewGC -XX:MaxTenuringThreshold=2 -XX:SurvivorRatio=8 -XX:+UnlockDiagnosticVMOptions -XX:ParGCCardsPerStrideChunk=32768
// Old generation  options
-XX:+UseConcMarkSweepGC -XX:CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled -XX:+CMSClassUnloadingEnabled  -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCMSInitiatingOccupancyOnly   
// Other options
-XX:+AlwaysPreTouch -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:-OmitStackTraceInFastThrow
&lt;/code&gt;&lt;button class=&quot;pre-button&quot;&gt;复制&lt;/button&gt;&lt;/pre&gt;
&lt;p&gt;使用这些参数，对于成千上万读请求的吞吐量，我们应用程序的 999 线降低到 60ms。&lt;/p&gt;
&lt;p&gt;解释&lt;/p&gt;
&lt;p&gt;[1] -XX:+BindGCTaskThreadsToCPUs 参数似乎在Linux 系统上不起作用，因为 hotspot/src/os/linux/vm/oslinux.cpp 的 distributeprocesses 方法在 JDK7 或 JDK8 中没有实现。&lt;/p&gt;
&lt;p&gt;[2] -XX:+UseGCTaskAffinity 参数在 JDK7 和 JDK8 的所有平台似乎都不起作用，因为任务的亲缘性属性永远被设置为 sentinelworker = (uint) -1。源码见 hotspot/src/share/vm/gcimplementation/parallelScavenge/{gcTaskManager.cpp，gcTaskThread.cpp, gcTaskManager.cpp}。&lt;/p&gt;
&lt;p&gt;[3] G1 存在一些内存泄露的 bug，可能 Java7u51 没有修改。这个 bug 仅在 Java 8 修正了。&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>e1efcacbcec32a60205fcdb6d321062d</guid>
<title>一文读懂 Raft 一致性协议算法并理解其中的关键设计</title>
<link>https://toutiao.io/k/c68ccw6</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;h2&gt;&lt;span&gt;什么是一致性算法&lt;/span&gt;&lt;span/&gt;&lt;/h2&gt;&lt;p&gt;为了避免单个机器可能出现的数据丢失、单点故障等问题，人们想出了通过复制数据到多个机器上的方式来解决。但是有多个机器时，带来的另一个问题就是如何保证这些机器之间的数据是一致的呢？不能因为某个机器故障或错误，导致各个机器之前数据混乱或丢失。这就是分布式一致性算法要解决的问题。&lt;/p&gt;&lt;p&gt;业界比较有名的分布式算法是paxos，不过可惜的是它比较晦涩难懂，难懂的代价就是很少有人能掌握它然后基于它做出可靠的实现。不过幸好raft及时出现，raft的特点是易于理解，并且已经有非常多的实际系统是基于raft算法实现的了，比如tikv、etcd等。文章末尾我们还会解释一下raft的名字。&lt;/p&gt;&lt;h2&gt;&lt;span&gt;介绍raft&lt;/span&gt;&lt;span/&gt;&lt;/h2&gt;&lt;p&gt;raft的易懂性的一个重要方式就是对问题进行拆分，让大家能够独立理解每个子问题。raft的分布式一致性问题可以拆分成哪些子问题呢？&lt;/p&gt;&lt;p&gt;可以分为&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;leader选举&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;log复制&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;安全性保证&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;log compaction、membership change等优化&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;&lt;span&gt;leader选举&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;raft把分布式系统抽象成了分布式的状态机，每个状态机有一个log队列，log队列中存放的是状态机的指令，只要保证各个节点的log队列的数据顺序和值一致，就能保证状态机最终的状态是一致的。&lt;/p&gt;&lt;p&gt;在raft，节点一共有3种角色，leader、follower、candidate。在raft系统中，同一时间只会有一个leader，leader负责处理客户端的请求，并且同步log给follower, leader定期通过给follower发送心跳，保持自己的leader地位。&lt;/p&gt;&lt;p&gt;初始时所有的节点都处于follower状态，当follower一段时间(election timeout)内没有接收到leader的请求时(log复制或心跳)，就会把自己转变为candidate角色，candidate是成为leader前的准备状态，candidate会向其他节点发送RequestVote请求，请求其他节点为自己投票，如果某个candidate获得了半数以上（包括自己）的节点的投票支持后，就可以成为这一届的leader。这里为了防止节点可能同时唤醒成为candidate，会增加一个随机机制，让超时时间随机，减少冲突概率，即使出现了冲突（多个candiate且都获得相同的投票），会在一段时间后进行下一轮投票。每一轮的选举会有一个term（任期）的值，term会从1开始递增，每一个follower在一个任期内只会投票给一个节点，这样加上半数以上机制就能保证同一个term内最多只会出现一个leader。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;log复制(log replication)&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;成为leader之后，就可以接收客户端的请求，leader接收客户端请求后，会写入本地log，并同步给所有的follower(通过AppendEntries的RPC请求)，follower会写入本地log，返回给leader成功，leader接收到半数以上（包括自己）的写入成功后，会进行commit，commit后这个修改就会应用到状态机中，并且返回给客户端请求成功。AppendEntries请求中会包含当前leader的term和日志的index。每次AppendEntries请求中会带上leader的commitIndex，这样follower就知道哪些log可以被应用到状态机上了。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;raft的安全性关键设计-leader故障重新选举&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;如果某个时刻leader故障了，而leader刚刚同步的log并没有同步给全部follower，则这时一个没有完成同步的follower如果成为下一个leader，则会导致前一任leader已经commit的数据丢失，这是不能接受的。&lt;/p&gt;&lt;p&gt;在raft中是如何解决这个问题的呢？raft中巧妙的实现了必须拥有已经commit的log的节点才能成为下一个leader，当follower收到一个candidate的RequestVote时，RequestVote中包含term、lastLogTerm、lastLogIndex，term是这个candiate的发起新的leader选举的term，lastLogTerm是candidate上最后一个log的term，lastLogIndex是最后一个log的index，term和index能够共同定位出唯一一个log。如果candidate的lastLogTerm小于当前follower的最后一个log的term,则会拒绝这个RequestVote请求。如果candidate的lastLogIndex小于当前follower的最后一个log的index，则会拒绝这个RequsetVote的请求。&lt;/p&gt;&lt;p&gt;通过这个限制，就能够保证选举出的新leader，是不会丢掉已经commit的数据的。并且当leader给follower同步数据(AppendEntries请求)时，会带上上一个log的term和index，如果follower的上一个log的term和index不相同，follower会返回错误，leader会进行回溯，这样就可以将follower的数据和leader进行校准。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;raft优化-log compaction&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;系统持续运行log可能会持续增加，持续增加的log带来的问题有，1是占用过多的磁盘空间，2是如果有新节点加入，则需要同步的数据会非常多。因此raft中提出了log compaction的机制，就是做快照，这个与mysql中的redo log类似。程序在特定时间（比如定时或log到达一定长度）后会对状态机保存快照，这样这个状态机之前的log就可以丢弃了，只需要持久化、传输这个快照就可以了。一般快照都比log会小很多。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;raft优化-membership change&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;服务在运行过程中，可能出现要添加修改删除节点的情况，比如某个机器坏了，我们就需要给集群换一个机器，再比如我们想提高集群的故障容忍度，可能就需要添加节点。&lt;/p&gt;&lt;p&gt;raft中提出的修改membership的方法为joint（联合） consensus&lt;/p&gt;&lt;figure&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/figure&gt;&lt;img data-ratio=&quot;0.4781021897810219&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/bfAhkMkS9hOUibaABLlXG4gqsJnStia5y5m6drvBLVzV8sP8LiafJ9p64Pr35TOPwsNZCHTxnwgplfG1kU5O7CekA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1096&quot;/&gt;&lt;p&gt;我们把raft集群节点的列表称为配置，当要进行节点修改时，就是从一个旧的配置(C-old)修改到新的配置(C-new),当leader收到要进行集群配置修改的请求后，会创建一个C old-new的配置（旧集群和新集群合并），作为raft log发送给follower，当follower收到C old-new后，会立刻使用C new配置。一旦C old-new被commit后，Cold和Cnew必须联合起来做决定（包括log复制和leader选举)，两个集群配置不能做单边决定，然后leader会再发送一个C new的配置，C new的配置commit后，old配置就失效了，old配置中的节点也可以安全关闭了。这个机制能够保证C old和C new不会做单边决定，从而保证了安全性。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;raft的名字&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;最后，关于raft这个名字的理解, raft为什么叫raft呢? 官网和google上并没有明确的答案。下面是我的理解。&lt;/p&gt;&lt;img data-ratio=&quot;0.5846153846153846&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/bfAhkMkS9hOUibaABLlXG4gqsJnStia5y5JXczDIRfAmle8KvoaDyicwa3VqORxJJBQcUqGemibicdNyH7DKrvEVcRw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;390&quot;/&gt;&lt;p&gt;raft在英文中的名字是木筏的意思，从raft的logo可以看出，它是有三个木头组成，那么木头在英文中有什么名字呢，log的意思也是木头。3个木头代表3个log，分布式的replicate log，是不是很巧妙？&lt;/p&gt;&lt;h2&gt;&lt;span&gt;其他参考资料&lt;/span&gt;&lt;span/&gt;&lt;/h2&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;raft&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;深入浅出 Raft - 基本概念&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Making sense of the RAFT Distributed Consensus Algorithm&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>ce61b78e8c7d94ce75126b941f53eb1f</guid>
<title>KCP 1.4 源码分析</title>
<link>https://toutiao.io/k/vt7wvie</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;post-content&quot;&gt;
      

&lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/skywind3000/kcp&quot;&gt;KCP&lt;/a&gt;是基于UDP协议之上的ARQ协议实现。TCP虽然使用的更广泛，但是在某些实时性要求更高的领域（如实时音视频、即时在线游戏等），会更倾向于使用基于UDP的可靠传输协议。&lt;/p&gt;

&lt;p&gt;在项目的官网上，对KCP是这么介绍的：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;KCP是一个快速可靠协议，能以比 TCP 浪费 10%-20% 的带宽的代价，换取平均延迟降低 30%-40%，且最大延迟降低三倍的传输效果。纯算法实现，并不负责底层协议（如UDP）的收发，需要使用者自己定义下层数据包的发送方式，以 callback的方式提供给 KCP。 连时钟都需要外部传递进来，内部不会有任何一次系统调用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;UDP并不是一个可靠的传输协议，如果数据没有发送成功并不会自动重传，KCP基于UDP协议之上实现了自己的ARQ协议，所以在继续介绍KCP协议之前，先大体了解一下ARQ协议。&lt;/p&gt;

&lt;h2 id=&quot;arq的两种模式&quot;&gt;ARQ的两种模式&lt;/h2&gt;

&lt;p&gt;KCP在UDP之上，自己实现了可靠性的算法，即给UDP加上了诸如超时重传、流量控制等机制，这些都是为了保证ARQ协议的运作。&lt;/p&gt;

&lt;p&gt;ARQ协议(Automatic Repeat-reQuest)，即自动重传请求，是传输层的错误纠正协议之一，它通过使用确认和超时两个机制，在不可靠的网络上实现可靠的信息传输。&lt;/p&gt;

&lt;p&gt;ARQ的实现通常有如下两种模式。&lt;/p&gt;

&lt;h3 id=&quot;停等arq协议-stop-and-wait&quot;&gt;停等ARQ协议（stop-and-wait）&lt;/h3&gt;

&lt;p&gt;停等ARQ协议，意味着每个数据在发送出去之后，在没有收到对端的接收回复之前，将一直等待下去，而不会继续发送新的数据包。如果超时还未收到应答，就会自动重传数据包，以保证数据的可靠性。&lt;/p&gt;

&lt;p&gt;下图是两种情况下停等协议的示意图：&lt;/p&gt;

&lt;p/&gt;&lt;center&gt;
&lt;img src=&quot;https://cdn.jsdelivr.net/gh/lichuang/lichuang.github.io/media/imgs/20201105-kcp/stop-and-wait.png&quot; alt=&quot;stop-and-wait&quot; title=&quot;stop-and-wait&quot;/&gt;
&lt;/center&gt;

&lt;ul&gt;
&lt;li&gt;上图：正常不出错情况下运行的停等协议，消息2必须在发送方收到了消息1的对端确认回复之后才能发送出去。&lt;/li&gt;
&lt;li&gt;下图：出错情况下运行的停等协议，发送方发现消息1超时还未收到应答，就触发了针对消息1的重传机制。在这之前消息2都不会被发出去。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;协议栈如何确认这个“超时时间”呢？答案是根据数据往返时间动态估算出来的RTO（Retransmission TimeOut，重传超时时间）时间来确认的。&lt;/p&gt;

&lt;h3 id=&quot;连续arq协议-continuous-arq&quot;&gt;连续ARQ协议（Continuous ARQ）&lt;/h3&gt;

&lt;p&gt;可以看到，停等协议的机制是“一应一答”式的，对带宽的利用率不高，传输效率不高。&lt;/p&gt;

&lt;p&gt;连续ARQ协议，可以一次性发送多个数据，而不必像停等协议那样需要等待上一个数据包的确认回复才能继续发送数据。&lt;/p&gt;

&lt;p&gt;在使用连续ARQ协议的时候，接收方也并不会针对每一个收到的数据包进行确认应答，而只需应答确认最大的那个数据包，这时就认为在此之前的数据包都收到了。&lt;/p&gt;

&lt;p&gt;这种模式称为“UNA（unacknowledge，即第一个未应答数据包的序列号，小于该序列号的数据包都已经确认被接收到）”模式，与之对应的是，停等协议是ACK模式。&lt;/p&gt;

&lt;p&gt;然而，即便是可以一次发送多个数据包，也不意味着可以不受控制的发送数据，因为还要受到几种流量窗口的限制，这部分被称为“流量控制”。&lt;/p&gt;

&lt;h3 id=&quot;拥塞窗口&quot;&gt;拥塞窗口&lt;/h3&gt;

&lt;p&gt;拥塞窗口更多是对网络上经过的网络设备总体网络情况的一个预估。因为在真正发送数据时，并不清楚这时候的网络情况，因此启动时拥塞窗口会有一个初始值，然后根据以下几种算法进行动态的调整：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;慢启动：在启动时让拥塞窗口缓慢扩张。&lt;/li&gt;
&lt;li&gt;退半避让：在发生网络拥堵时让拥塞窗口大小减半。&lt;/li&gt;
&lt;li&gt;快重传：在网络恢复时尽快的将数据发送出去。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;滑动窗口&quot;&gt;滑动窗口&lt;/h3&gt;

&lt;p&gt;拥塞窗口是对外部网络情况的一种动态的检测，而滑动窗口则是进程本身接收缓冲区的控制，滑动窗口就是接收方用来通知发送方本方接收缓冲区大小的。由于一个网络进程分为协议层和应用层，如果协议层接收数据很快，但是应用层消费数据很慢，这个滑动窗口就会缩小，通过这种方式来通知对端放缓数据的发送，因为接收方已经忙不过来了。&lt;/p&gt;

&lt;p&gt;KCP作为一个ARQ协议，内部就是要实现对以上这些机制的支持。&lt;/p&gt;

&lt;p&gt;如果对TCP协议的实现有一些了解，可以看到上述的对端确认回复、超时重传、拥塞窗口、滑动窗口等概念，在TCP中就有，KCP自己实现的ARQ机制，与TCP对比起来有如下的不同点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在TCP中，超时之后的RTO时间直接翻倍（即RTO*2），而在KCP启用了快速模式之后，RTO的超时时间是*1.5，避免RTO时间的快速增长。&lt;/li&gt;
&lt;li&gt;TCP协议在丢包时会直接重传丢的那个包之后的所有数据包，KCP只会选择性的重传真正丢失的数据包。&lt;/li&gt;
&lt;li&gt;TCP为了充分利用带宽，会延迟发送ACK应答对端，这样会导致计算出来的RTT时间过大，KCP的ACK是否延迟发送则可以调节。&lt;/li&gt;
&lt;li&gt;KCP 正常模式同 TCP 一样使用公平退让法则，即发送窗口大小由：发送缓存大小、接收端剩余接收缓存大小、丢包退让及慢启动这四要素决定。但传送及时性要求很高的小数据时，可选择通过配置跳过后两步，仅用前两项来控制发送频率。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文基于KCP 1.4版本对其实现做分析。&lt;/p&gt;

&lt;h1 id=&quot;术语概念&quot;&gt;术语概念&lt;/h1&gt;

&lt;p&gt;在展开讨论之前，首先介绍相关的术语概念。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ARQ：Automatic Repeat-reQuest，自动重传请求协议。KCP就是其中一种ARQ协议的实现。&lt;/li&gt;
&lt;li&gt;MTU：Maximum Transmission Unit，最大传输单元，链路层规定的每一帧最大长度，通常为1500字节。&lt;/li&gt;
&lt;li&gt;MSS：Maximum Segment Size，最大分段大小。通常为MTU-协议头大小。&lt;/li&gt;
&lt;li&gt;RTT：Round-Trip Time，数据往返时间，即发出消息到接收到对端消息应答之间的时间差。&lt;/li&gt;
&lt;li&gt;RTO：Retransmission TimeOut，重传超时时间，根据收集到的RTT时间估算。&lt;/li&gt;
&lt;li&gt;rwnd：Receive Window，接收窗口大小，接收端通过该数据通知发送端本方接收窗口大小。&lt;/li&gt;
&lt;li&gt;cwnd：Congestion Window，拥塞窗口大小，影响发送方发送数据大小。&lt;/li&gt;
&lt;li&gt;ack：acknowledge，接收端接收到一个数据包之后，通过应答该数据包序列号来通知发送端接收成功。&lt;/li&gt;
&lt;li&gt;una：unacknowledge，即第一个未应答数据包的序列号，小于该序列号的数据包都已经确认被接收到。&lt;/li&gt;
&lt;li&gt;ssthresh：Slow Start threshold，慢启动阈值，用于在发生拥塞的情况下控制窗口的增长速度。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;数据结构&quot;&gt;数据结构&lt;/h1&gt;

&lt;h2 id=&quot;报文定义&quot;&gt;报文定义&lt;/h2&gt;

&lt;p&gt;每个KCP数据报文，其定义如下，注释中描述了每个字段的含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct IKCPSEG
{
struct IQUEUEHEAD node;
// 会话编号，两方一致才能通信
IUINT32 conv;
// 指令类型，可以同时有多个指令通过与操作设置进来
IUINT32 cmd;
// 分片编号，表示倒数第几个分片。
IUINT32 frg;
// 本方可用窗口大小
IUINT32 wnd;
// 当前时间
IUINT32 ts;
// 确认编号
IUINT32 sn;
// 代表编号前面的所有报都收到了的标志
IUINT32 una;
// 数据大小
IUINT32 len;
// 重传时间戳，超过这个时间重发这个包
IUINT32 resendts;
IUINT32 rto;
// 快速应答数量，记录被跳过的次数，统计在这个封包的序列号之前有多少报已经应答了。
// 比如1，2，3三个封包，收到2的时候知道1被跳过了，此时1的fastack加一，收到3的时候继续加一，超过一定阈值直接重传1这个封包。
// 该阈值由ikcp_nodelay函数设置，默认为0
IUINT32 fastack;
// 重传次数
IUINT32 xmit;
// 数据
char data[1];
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这里，挑其中几个重点的字段来展开说说，其他的字段已经在上面的注释中有描述。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;conv：该字段是会话编号，由于UDP协议不是基于链接的，因此通信的双方需要会话编号一致才能进行通信。&lt;/li&gt;
&lt;li&gt;cmd：指令类型，具体有以下这几种：

&lt;ul&gt;
&lt;li&gt;IKCP_CMD_PUSH：传送数据。&lt;/li&gt;
&lt;li&gt;IKCP_CMD_ACK：应答接收到数据包。&lt;/li&gt;
&lt;li&gt;IKCP_CMD_WASK：探测接收端接收窗口大小。&lt;/li&gt;
&lt;li&gt;IKCP_CMD_WINS：通知接收窗口大小。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;frg：分片编号，当发送的数据超过MTU大小时，就会将数据分片来发送，该字段就是用来存储分片编号，值从大到小，比如有4个分片，则从第一块分片到第四块分片的报文，该字段依次为3、2、1、0。&lt;/li&gt;
&lt;li&gt;fastack：用于快速重传的字段，具体的使用在后面展开详细的讨论。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要说明的是，以上只是报文在内存中的表示，当写入报文时报文的头部数据如下（由于KCP文档中有这部分的图示就直接引用了）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kcp协议头,共24个字节
|&amp;lt;&amp;lt;----------- 4 bytes -----------&amp;gt;&amp;gt;|
|--------|--------|--------|--------|
|conv   |
|--------|--------|--------|--------|
|  cmd   |  frg   |wnd   |
|--------|--------|--------|--------|
|ts       |
|--------|--------|--------|--------|
|sn   |
|--------|--------|--------|--------|
|una   |
|--------|--------|--------|--------|
|len   |
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;kcp结构体&quot;&gt;KCP结构体&lt;/h2&gt;

&lt;p&gt;除了上面定义每个报文的结构体之外，&lt;code&gt;kcp&lt;/code&gt;协议栈还有一个负责记录kcp协议栈信息的结构体&lt;code&gt;IKCPCB&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;其定义及成员的注释如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;struct IKCPCB
{
// mss：MSS（Maximum Segment Size），最大报文长度
IUINT32 conv, mtu, mss, state;
// snd_una：最小的未ack序列号，即这个编号前面的所有报都收到了的标志
// snd_nxt：下一个待发送的序列号
// rcv_nxt：下一个待接收的序列号，会通过包头中的una字段通知对端
IUINT32 snd_una, snd_nxt, rcv_nxt;
// ssthresh：slow start threshhold，慢启动阈值
IUINT32 ts_recent, ts_lastack, ssthresh;
// RTT：Round Trip Time，往返时间
// rx_rttval：RTT的平均偏差
// rx_srtt：RTT的一个加权RTT平均值，平滑值。
IINT32 rx_rttval, rx_srtt, rx_rto, rx_minrto;
// rmt_wnd：对端（rmt=remote）窗口
// probe：存储探测标志位
IUINT32 snd_wnd, rcv_wnd, rmt_wnd, cwnd, probe;
IUINT32 current, interval, ts_flush, xmit;
// 接收和发送缓冲区大小
IUINT32 nrcv_buf, nsnd_buf;
// 接收和发送队列大小
IUINT32 nrcv_que, nsnd_que;
IUINT32 nodelay, updated;
IUINT32 ts_probe, probe_wait;
IUINT32 dead_link, incr;
struct IQUEUEHEAD snd_queue;
struct IQUEUEHEAD rcv_queue;
struct IQUEUEHEAD snd_buf;
struct IQUEUEHEAD rcv_buf;
IUINT32 *acklist;
IUINT32 ackcount;
IUINT32 ackblock;
void *user;
char *buffer;
int fastresend;
int fastlimit;
// nocwnd：是否关闭流控，0表示不关闭，默认值为0
int nocwnd, stream;
int logmask;
int (*output)(const char *buf, int len, struct IKCPCB *kcp, void *user);
void (*writelog)(const char *log, struct IKCPCB *kcp, void *user);
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kcp库对外的接口中，首先需要调用&lt;code&gt;ikcp_create&lt;/code&gt;函数创建该结构体，才能继续后面的工作。&lt;/p&gt;

&lt;h2 id=&quot;几个队列&quot;&gt;几个队列&lt;/h2&gt;

&lt;p&gt;从上面定义的数据结构中，还看到了其中有队列指针，不难想象每个报文数据都是某种队列中的元素，确实也是这样，在KCP中定义了以下几个和报文相关的队列：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;snd_queue、nsnd_que：发送队列以及其大小。&lt;/li&gt;
&lt;li&gt;snd_buf、nsnd_buf：发送缓冲区及其大小。&lt;/li&gt;
&lt;li&gt;rcv_queue、nrcv_que：接收队列以及其大小。&lt;/li&gt;
&lt;li&gt;rcv_buf、nrcv_buf：接收缓冲区及其大小。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为什么发送和接收两端，既有缓冲区又有队列？在KCP中，队列是应用层可以直接进行读写的区域，而缓冲区则是KCP协议层接收和发送数据的区域了，如图所示：&lt;/p&gt;

&lt;p/&gt;&lt;center&gt;
&lt;img src=&quot;https://cdn.jsdelivr.net/gh/lichuang/lichuang.github.io/media/imgs/20201105-kcp/kcp-queue-buf.png&quot; alt=&quot;kcp-queue-buf&quot; title=&quot;kcp-queue-buf&quot;/&gt;
&lt;/center&gt;

&lt;p&gt;在发送报文时，用户层调用&lt;code&gt;ikcp_send&lt;/code&gt;函数，该函数最终会分配报文结构体指针，然后添加到发送队列&lt;code&gt;snd_queue&lt;/code&gt;的末尾；而在KCP协议栈真正调用系统接口发送数据出去的时候，将从&lt;code&gt;snd_queue&lt;/code&gt;队列中将报文取出，放入&lt;code&gt;snd_buf&lt;/code&gt;缓冲区中再进行发送。接收报文的流程反之，这里就不再阐述了。&lt;/p&gt;

&lt;h1 id=&quot;核心流程&quot;&gt;核心流程&lt;/h1&gt;

&lt;p&gt;了解了相关的数据结构，这里开始分析核心流程。先来看看整体的框架。&lt;/p&gt;

&lt;h2 id=&quot;概述-1&quot;&gt;概述&lt;/h2&gt;

&lt;p&gt;KCP的实现中，把两个部分留给应用层来做：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;具体收发数据的流程，通过将&lt;code&gt;ikcp_setoutput&lt;/code&gt;函数留给应用层注册来进行数据发送，KCP自己并不负责这一块。&lt;/li&gt;
&lt;li&gt;何时驱动KCP进行数据的收发，即KCP内部并没有实现一个定时器，定期检查条件来触发收发流程，而是提供了&lt;code&gt;ikcp_update&lt;/code&gt;函数给应用层，通过该函数来驱动KCP协议栈的运作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;除此以外，KCP提供另外几个函数的作用如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ikcp_input：当应用层接收到数据时，通过该函数通知KCP协议栈对接收到的数据进行解析，最终会生成报文存储到前面提到的接收队列&lt;code&gt;rcv_queue&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;ikcp_recv：上一步调用&lt;code&gt;ikcp_input&lt;/code&gt;函数完成对接收到的报文的解析之后，&lt;code&gt;ikcp_recv&lt;/code&gt;函数将解析完成的报文重新拼装到buffer中返回给用户层。&lt;/li&gt;
&lt;li&gt;ikcp_send：用户层发送数据，最终会将待发送的数据编码成一个个的KCP报文存入&lt;code&gt;snd_queue&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;ikcp_update：用户层调用该函数，来驱动KCP协议栈进行具体的协议收发、拥塞控制等流程，这些流程实际最终由函数&lt;code&gt;ikcp_flush&lt;/code&gt;完成，但是用户层并不会直接调用这个函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个流程中涉及到的函数及流程如下图：&lt;/p&gt;

&lt;p/&gt;&lt;center&gt;
&lt;img src=&quot;https://cdn.jsdelivr.net/gh/lichuang/lichuang.github.io/media/imgs/20201105-kcp/ikcp.png&quot; alt=&quot;ikcp&quot; title=&quot;ikcp&quot;/&gt;
&lt;/center&gt;&lt;p&gt;
（出自：&lt;/p&gt;&lt;a href=&quot;https://blog.csdn.net/yongkai0214/article/details/85156452&quot;&gt;KCP 协议与源码分析（一）_老雍的博客-CSDN博客_kcp&lt;/a&gt;&lt;p&gt;）

&lt;/p&gt;&lt;p&gt;在这里先对上图做简单的解释，下面再展开详细的分析：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;图中中轴的函数是&lt;code&gt;ikcp_create&lt;/code&gt;，负责创建kcp协议栈结构体指针；而真正需要发送数据时，需要用户层自己调用&lt;code&gt;ikcp_update&lt;/code&gt;函数驱动kcp协议栈工作。&lt;/li&gt;
&lt;li&gt;图的左边是用户层与协议栈的交互。用户调用&lt;code&gt;ikcp_send&lt;/code&gt;函数，将用户缓冲区的数据，根据KCP协议拼装成报文放到发送队列&lt;code&gt;snd_queue&lt;/code&gt;中。而当用户需要从协议栈接收数据时，会调用&lt;code&gt;ikcp_recv&lt;/code&gt;函数，该函数会将在接收队列&lt;code&gt;recv_queue&lt;/code&gt;中的报文反序列化成用户层缓冲数据，返回给应用层。&lt;/li&gt;
&lt;li&gt;图的右边是协议栈与网络之间的交互。首先&lt;code&gt;ikcp_flush&lt;/code&gt;函数，会将发送队列&lt;code&gt;snd_queue&lt;/code&gt;中的报文移动到发送缓冲区中，最终调用用户通过&lt;code&gt;ikcp_output&lt;/code&gt;函数注册发送函数发送出去；同时，当收到网络层的数据时，会调用&lt;code&gt;ikcp_input&lt;/code&gt;函数将这些数据以kcp协议的形式解析出来，放入到接收缓冲区&lt;code&gt;snd_buf&lt;/code&gt;中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下对其中的核心流程做分析。&lt;/p&gt;

&lt;h1 id=&quot;ikcp-input函数&quot;&gt;ikcp_input函数&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;ikcp_input&lt;/code&gt;函数是用户层接收到数据时调用的第一个函数，其传入的参数是收到数据的缓冲区。因为用户层接收到的数据，都没有经过KCP协议的解析，所以首先调用&lt;code&gt;ikcp_input&lt;/code&gt;函数进行协议解析。又由于一个报文中可能存在多个KCP协议包，所以会遍历这个用户层数据缓冲区进行多次的KCP协议解析。KCP协议，按照其包头中带的指令类型，可能有以下几种：&lt;/p&gt;

&lt;h2 id=&quot;ikcp-cmd-ack&quot;&gt;IKCP_CMD_ACK&lt;/h2&gt;

&lt;p&gt;对端应答ack报。处理流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;调用&lt;code&gt;ikcp_update_ack&lt;/code&gt;函数更新RTT估算值。&lt;/li&gt;
&lt;li&gt;由于收到了对端的ack，所以调用&lt;code&gt;ikcp_parse_ack&lt;/code&gt;函数，遍历当前的发送缓冲区&lt;code&gt;snd_buf&lt;/code&gt;删除对应该应答序列号的报文，因为该报文对端已经应答收到了，不需要再重发了。&lt;/li&gt;
&lt;li&gt;调用&lt;code&gt;ikcp_shrink_buf&lt;/code&gt;函数更新&lt;code&gt;snd_una&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;快速重传逻辑的处理，这部分在函数&lt;code&gt;ikcp_parse_fastack&lt;/code&gt;中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;ikcp_update_ack&lt;/code&gt;函数用于更新RTT相关的估算值，包括：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rx_rttval：rtt平均值，为最近4次rtt的平均值。&lt;/li&gt;
&lt;li&gt;rx_srtt：ack接收rtt平滑值为最近8次的平均值。&lt;/li&gt;
&lt;li&gt;rx_minrto：最小RTO，系统启动时配置，在nodelay的情况下值为&lt;code&gt;IKCP_RTO_NDL&lt;/code&gt;，否则就是&lt;code&gt;IKCP_RTO_MIN&lt;/code&gt;。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;rx_rto：估算出来的rto值，为平滑值+max（interval，平均值），在[rx_minrto,IKCP_RTO_MAX]之间。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;static void ikcp_update_ack(ikcpcb *kcp, IINT32 rtt)
{
IINT32 rto = 0;
if (kcp-&amp;gt;rx_srtt == 0) { // 当前没有rtt加权平均值
// 以这次RTT值来设置
kcp-&amp;gt;rx_srtt = rtt;
// 平均值要除以2
kcp-&amp;gt;rx_rttval = rtt / 2;
}else {
// 计算两者之差
long delta = rtt - kcp-&amp;gt;rx_srtt;
if (delta &amp;lt; 0) delta = -delta;
// 算平均值，可以看到平均值是最近4次的平均
kcp-&amp;gt;rx_rttval = (3 * kcp-&amp;gt;rx_rttval + delta) / 4;
// 算加权值，加权值是最近8次加权值的平均
kcp-&amp;gt;rx_srtt = (7 * kcp-&amp;gt;rx_srtt + rtt) / 8;
// 不能小于1
if (kcp-&amp;gt;rx_srtt &amp;lt; 1) kcp-&amp;gt;rx_srtt = 1;
}
// 计算RTO值：平滑值+max（interval，平均值）
rto = kcp-&amp;gt;rx_srtt + _imax_(kcp-&amp;gt;interval, 4 * kcp-&amp;gt;rx_rttval);
// 最终在[rx_minrto,IKCP_RTO_MAX]之间
kcp-&amp;gt;rx_rto = _ibound_(kcp-&amp;gt;rx_minrto, rto, IKCP_RTO_MAX);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可见，以上流程最终要算出来当前KCP协议栈的&lt;code&gt;rx_rto&lt;/code&gt;，这个值最终会影响每个报文的超时发送时间，这在后面的发送流程中再解释。&lt;/p&gt;

&lt;p&gt;另外还需要专门聊一下&lt;code&gt;ikcp_parse_fastack&lt;/code&gt;函数，以及快速重传的处理。快速重传的原理是这样的：假设当前有序列号&lt;code&gt;[1,2,3]&lt;/code&gt;的报文等待对端应答，当KCP协议栈收到报文2的ack时，知道报文1被跳过1次了；同样的，当收到报文3的ack时，报文1又被跳过1次。&lt;/p&gt;

&lt;p&gt;这里的“跳过次数”就存储在&lt;code&gt;IKCPSEG.fastack&lt;/code&gt;成员中，KCP协议栈提供&lt;code&gt;ikcp_nodelay&lt;/code&gt;函数可以配置快速重传值&lt;code&gt;resend&lt;/code&gt;，当报文的跳过次数超过&lt;code&gt;resend&lt;/code&gt;时，就马上重传该报文，不会等待报文超时，一定程度上加速报文的重传降低了延迟。&lt;/p&gt;

&lt;h2 id=&quot;ikcp-cmd-push&quot;&gt;IKCP_CMD_PUSH&lt;/h2&gt;

&lt;p&gt;传送数据的指令，此时解析最终会进入&lt;code&gt;ikcp_parse_data&lt;/code&gt;函数中，该函数流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先会通过报文序列号判断是否在当前接收窗口以内（&lt;code&gt;_itimediff(sn, kcp-&amp;gt;rcv_nxt + kcp-&amp;gt;rcv_wnd) &amp;gt;= 0&lt;/code&gt;），或者已经接收过了（&lt;code&gt;_itimediff(sn, kcp-&amp;gt;rcv_nxt) &amp;lt; 0&lt;/code&gt;），这两种情况都删除报文返回，不做进一步处理。&lt;/li&gt;
&lt;li&gt;根据报文序列号在&lt;code&gt;rcv_buf&lt;/code&gt;判断当前接收缓冲区中是否已经存在同序列号的报文，如果已经存在说明是重复接收的，也删除报文不再处理。&lt;/li&gt;
&lt;li&gt;以上两步都通过了，说明是首次接收该序列号的报文，将把该报文放入接收缓冲区&lt;code&gt;rcv_buf&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;由于&lt;code&gt;rcv_queue&lt;/code&gt;中的报文才是最终面向用户层的，而上面的操作可能让&lt;code&gt;rcv_buf&lt;/code&gt;接收缓冲区非空存在新的报文了，所以接下来将&lt;code&gt;rcv_buf&lt;/code&gt;中的报文移动到&lt;code&gt;rcv_queue&lt;/code&gt;中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ikcp-cmd-wask&quot;&gt;IKCP_CMD_WASK&lt;/h2&gt;

&lt;p&gt;对端请求探测窗口大小，此时会把探测标志位加上&lt;code&gt;IKCP_ASK_TELL&lt;/code&gt;，下一次发送数据时带上窗口大小通知对端。&lt;/p&gt;

&lt;h2 id=&quot;ikcp-cmd-wins&quot;&gt;IKCP_CMD_WINS&lt;/h2&gt;

&lt;p&gt;通知窗口大小。&lt;/p&gt;

&lt;h2 id=&quot;快速应答处理&quot;&gt;快速应答处理&lt;/h2&gt;

&lt;h2 id=&quot;更新参数&quot;&gt;更新参数&lt;/h2&gt;

&lt;p&gt;前面的处理完毕之后，可能会收到新的ack报文，这时就需要更新KCP协议栈的拥塞窗口大小。&lt;/p&gt;

&lt;p&gt;如果当前拥塞窗口大小小于对端的窗口大小（&lt;code&gt;kcp-&amp;gt;cwnd &amp;lt; kcp-&amp;gt;rmt_wnd&lt;/code&gt;），那么需要增加拥塞窗口大小，区分两种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果拥塞窗口大小小于慢启动阈值（&lt;code&gt;kcp-&amp;gt;cwnd &amp;lt; kcp-&amp;gt;ssthresh&lt;/code&gt;）：递增拥塞窗口大小。&lt;/li&gt;
&lt;li&gt;否则：

&lt;ul&gt;
&lt;li&gt;拥塞窗口增量递增1/16；&lt;/li&gt;
&lt;li&gt;如果当前拥塞窗口递增后小于增量的情况下才递增。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后，拥塞窗口不能超过对端窗口大小。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;// 前面处理完毕之后，最新的una更大，说明接收到了新的ack
if (_itimediff(kcp-&amp;gt;snd_una, prev_una) &amp;gt; 0) {
if (kcp-&amp;gt;cwnd &amp;lt; kcp-&amp;gt;rmt_wnd) {// 拥塞窗口小于对端窗口
IUINT32 mss = kcp-&amp;gt;mss;
if (kcp-&amp;gt;cwnd &amp;lt; kcp-&amp;gt;ssthresh) { // 拥塞窗口小于慢启动阈值
kcp-&amp;gt;cwnd++;// 递增拥塞窗口
kcp-&amp;gt;incr += mss;// 递增mss
}else { // 拥塞窗口大于等于慢启动阈值
// 不能小于mss了
if (kcp-&amp;gt;incr &amp;lt; mss) kcp-&amp;gt;incr = mss;
// 增加 mss + 1/16 mss
kcp-&amp;gt;incr += (mss * mss) / kcp-&amp;gt;incr + (mss / 16);
// 只有在拥塞窗口递增后不超过incr的情况下才允许加一
if ((kcp-&amp;gt;cwnd + 1) * mss &amp;lt;= kcp-&amp;gt;incr) {
kcp-&amp;gt;cwnd++;
}
}
if (kcp-&amp;gt;cwnd &amp;gt; kcp-&amp;gt;rmt_wnd) { // 拥塞窗口不能比对端窗口大
kcp-&amp;gt;cwnd = kcp-&amp;gt;rmt_wnd;
kcp-&amp;gt;incr = kcp-&amp;gt;rmt_wnd * mss;
}
}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ikcp-recv函数&quot;&gt;ikcp_recv函数&lt;/h1&gt;

&lt;p&gt;前面的&lt;code&gt;ikcp_input&lt;/code&gt;解析完毕之后，将用户缓冲区的数据解析到一个一个的报文放到了接收队列&lt;code&gt;rcv_queue&lt;/code&gt;中，&lt;code&gt;ikcp_recv&lt;/code&gt;函数就负责将这些报文重新组装起来放入用户缓冲区返回给用户层。&lt;/p&gt;

&lt;p&gt;之所以这里还需要“组装”，是因为对端发送的数据由于超过MTU所以被KCP协议栈分成多个报文发送了。所以这里需要兼容多个分片的情况，如果待接收报文的所有分片没有接收完毕，那么不能处理。接收完毕或者不分片的情况下，就遍历这些报文将数据拷贝到缓冲区中。&lt;/p&gt;

&lt;p&gt;上面的步骤完成之后，如果接收缓冲区&lt;code&gt;rcv_buf&lt;/code&gt;还有报文，那么依然是把这部分报文移动到接收队列中，等待下一次&lt;code&gt;ikcp_recv&lt;/code&gt;调用。&lt;/p&gt;

&lt;h1 id=&quot;ikcp-send&quot;&gt;ikcp_send&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;ikcp_send&lt;/code&gt;函数是用户层发送数据的接口，最终会将用户传入的缓冲区数据，组装成KCP报文，放入发送队列&lt;code&gt;snd_queue&lt;/code&gt;中。&lt;/p&gt;

&lt;p&gt;这里需要注意两种情况。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果是流模式，那么首先KCP会取出发送队列当前的最后报文的结构体，如果当前报文还有空间就将部分数据拷贝过去。&lt;/li&gt;
&lt;li&gt;如果数据超过MSS大小，那么需要对数据分片，即将数据分为多个KCP报文发送。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ikcp-update和ikcp-flush&quot;&gt;ikcp_update和ikcp_flush&lt;/h1&gt;

&lt;p&gt;前面的&lt;code&gt;ikcp_send&lt;/code&gt;只是将待发送数据组装成KCP报文放到发送队列中了，具体的发送流程由调用&lt;code&gt;ikcp_update&lt;/code&gt;函数来驱动完成的。&lt;/p&gt;

&lt;p&gt;KCP协议栈中，并没有任何的自定义定时器，即自己并不会主动来根据时间驱动来完成工作，这部分都留给了用户层，由用户层主动调用&lt;code&gt;ikcp_update&lt;/code&gt;来完成这些工作。&lt;code&gt;ikcp_update&lt;/code&gt;函数的处理其实很简单，会判断上一次刷新（flush）时间与这次的间隔，来判断是否调用&lt;code&gt;ikcp_flush&lt;/code&gt;函数来完成工作，所以这里真正工作的是&lt;code&gt;ikcp_flush&lt;/code&gt;函数，下面就来重点分析这个函数的实现。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ikcp_flush&lt;/code&gt;函数本质就是根据当前的情况，封装KCP报文，将这些报文放到发送缓冲区&lt;code&gt;snd_buf&lt;/code&gt;中，发送到对端。除此之外，还需要重新计算流控、拥塞窗口等参数。总体来看，需要完成以下的工作：&lt;/p&gt;

&lt;h2 id=&quot;处理ack应答&quot;&gt;处理ACK应答&lt;/h2&gt;

&lt;p&gt;首先，&lt;code&gt;ikcp_flush&lt;/code&gt;函数将编码&lt;code&gt;IKCP_CMD_ACK&lt;/code&gt;类型的指令，应答收到了对端那些报文。&lt;/p&gt;

&lt;h2 id=&quot;探测对端窗口&quot;&gt;探测对端窗口&lt;/h2&gt;

&lt;p&gt;在对端通知窗口为0（即&lt;code&gt;kcp-&amp;gt;rmt_wnd == 0&lt;/code&gt;）情况下，需要探测对端当前窗口大小，即需要发送&lt;code&gt;IKCP_ASK_SEND&lt;/code&gt;类型的报文。&lt;/p&gt;

&lt;p&gt;在这里，涉及到KCP协议栈&lt;code&gt;ikcpcb&lt;/code&gt;结构体的几个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;probe_wait：存储下一次探测窗口的时间间隔，该参数的初始值为&lt;code&gt;IKCP_PROBE_INIT&lt;/code&gt;，每次新的探测间隔时间将在当前基础上递增当前的1/2，但是最高不超过&lt;code&gt;IKCP_PROBE_LIMIT&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;ts_probe：存储下一次探测时间，不难知道这个值每次都是根据当前时间加上&lt;code&gt;probe_wait&lt;/code&gt;计算出来的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在当前时间超过&lt;code&gt;ts_probe&lt;/code&gt;（即&lt;code&gt;_itimediff(kcp-&amp;gt;current, kcp-&amp;gt;ts_probe) &amp;gt;= 0&lt;/code&gt;）的情况下，&lt;code&gt;probe&lt;/code&gt;探测标志位就要加上&lt;code&gt;IKCP_ASK_SEND&lt;/code&gt;，表示需要给对端发送探测窗口的报文了。&lt;/p&gt;

&lt;h2 id=&quot;流控&quot;&gt;流控&lt;/h2&gt;

&lt;p&gt;以上已经处理了&lt;code&gt;IKCP_CMD_ACK&lt;/code&gt;、&lt;code&gt;IKCP_ASK_SEND&lt;/code&gt;、&lt;code&gt;IKCP_ASK_TELL&lt;/code&gt;这三个类型的指令了，接下来就是处理&lt;code&gt;IKCP_CMD_PUSH&lt;/code&gt;类型的数据了，这部分数据都已经在前面的&lt;code&gt;ikcp_send&lt;/code&gt;由用户层传入的缓冲区解析拼装到发送队列&lt;code&gt;snd_queue&lt;/code&gt;中了。&lt;/p&gt;

&lt;p&gt;接下来，就可以遍历发送队列&lt;code&gt;snd_queue&lt;/code&gt;中的报文移动到发送队列&lt;code&gt;snd_buf&lt;/code&gt;中，进行实际的报文发送了。&lt;/p&gt;

&lt;p&gt;但是，并不是所有当前在发送队列中的报文都能在一次flush过程被发送出去，要考虑三个窗口的大小：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先不能超过发送窗口（snd_wnd）和对端窗口（rmt_wnd）的大小。&lt;/li&gt;
&lt;li&gt;在开启流控（kcp-&amp;gt;nocwnd == 0）的情况下，还不能超过当前流控窗口（cwnd）的大小。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前面的流程算出来发送时的窗口大小，接下来就按照这个窗口大小将&lt;code&gt;snd_queue&lt;/code&gt;的报文取下来放入&lt;code&gt;snd_buf&lt;/code&gt;中了。&lt;/p&gt;

&lt;h2 id=&quot;发送数据&quot;&gt;发送数据&lt;/h2&gt;

&lt;p&gt;以上已经根据流控窗口选出了待发送的报文放在发送缓冲区&lt;code&gt;snd_buf&lt;/code&gt;里了，接下来就是具体的发送流程了。&lt;/p&gt;

&lt;p&gt;针对每个报文，在发送之前要计算它的几个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;xmit：发送次数，每发送一次该计数递增，如果一个报文的发送次数超过了&lt;code&gt;dead_link&lt;/code&gt;，那么认为网络已经断了不再尝试发送。&lt;/li&gt;
&lt;li&gt;rto：用来计算重传超时时间的，初始值就是KCP协议栈当前估算出来的RTO值，在发生重传的情况下这个值会增加：

&lt;ul&gt;
&lt;li&gt;在非急速模式下，每次递增的值也是KCP协议栈估算出来的RTO值。（&lt;code&gt;segment-&amp;gt;rto += kcp-&amp;gt;rx_rto;&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;急速模式下，每次递增的值也是KCP协议栈估算出来的RTO值的二分之一。（&lt;code&gt;segment-&amp;gt;rto += kcp-&amp;gt;rx_rto / 2&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;resendts：根据当前时间加上rto时间计算出来的下次重传时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;来看看发送报文需要考虑的几种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首次发送（&lt;code&gt;segment-&amp;gt;xmit == 0&lt;/code&gt;）：设置&lt;code&gt;xmit&lt;/code&gt;为1，&lt;code&gt;rto&lt;/code&gt;为&lt;code&gt;kcp-&amp;gt;rx_rto&lt;/code&gt;，以及重传时间为当前时间加上rto（&lt;code&gt;segment-&amp;gt;resendts = current + segment-&amp;gt;rto + rtomin&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;因为超时发生的重传（&lt;code&gt;_itimediff(current, segment-&amp;gt;resendts) &amp;gt;= 0&lt;/code&gt;）：递增&lt;code&gt;xmit&lt;/code&gt;计数值，增加&lt;code&gt;rto&lt;/code&gt;时间，以及更新下次重传时间&lt;code&gt;resendts&lt;/code&gt;，并且标记发生了丢包（&lt;code&gt;lost=1&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;快速重传（&lt;code&gt;segment-&amp;gt;fastack &amp;gt;= resent&lt;/code&gt;）：前面已经分析过快速重传参数&lt;code&gt;fastack&lt;/code&gt;的作用，这里就不再阐述了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过以上分析，可以知道除了第一种情况是正常发送之外，还发生了超时重传以及快速重传，根据这些情况，需要更新一下KCP协议栈的参数。&lt;/p&gt;

&lt;h2 id=&quot;更新参数-1&quot;&gt;更新参数&lt;/h2&gt;

&lt;p&gt;分以下两种情况处理：&lt;/p&gt;

&lt;h3 id=&quot;快速重传&quot;&gt;快速重传&lt;/h3&gt;

&lt;p&gt;在发生快速重传的情况下，会挑战ssthresh为当前发送窗口的一半大小，同时拥塞窗口为&lt;code&gt;ssthresh + resent&lt;/code&gt;大小：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (change) { // 发生了快速重传，计算新的ssthresh
// 如果发生了快速重传，拥塞窗口阈值降低为当前未确认包数量的一半或最小值  
// 当前发送窗口大小
IUINT32 inflight = kcp-&amp;gt;snd_nxt - kcp-&amp;gt;snd_una;
// ssthresh为当前窗口大小的一半
kcp-&amp;gt;ssthresh = inflight / 2;
// 不能小于IKCP_THRESH_MIN
if (kcp-&amp;gt;ssthresh &amp;lt; IKCP_THRESH_MIN)
kcp-&amp;gt;ssthresh = IKCP_THRESH_MIN;
kcp-&amp;gt;cwnd = kcp-&amp;gt;ssthresh + resent;
kcp-&amp;gt;incr = kcp-&amp;gt;cwnd * kcp-&amp;gt;mss;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;超时丢包&quot;&gt;超时丢包&lt;/h3&gt;

&lt;p&gt;在发生超时丢包的情况下，慢启动阈值调整为旧的拥塞窗口的一半，但是不能小于&lt;code&gt;IKCP_THRESH_MIN&lt;/code&gt;，而新的拥塞窗口值变成1：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (lost) { // 发生了丢包
// 丢失则阈值减半, cwd 窗口保留为 1  
kcp-&amp;gt;ssthresh = cwnd / 2;
if (kcp-&amp;gt;ssthresh &amp;lt; IKCP_THRESH_MIN)
kcp-&amp;gt;ssthresh = IKCP_THRESH_MIN;
kcp-&amp;gt;cwnd = 1;
kcp-&amp;gt;incr = kcp-&amp;gt;mss;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;参考资料&quot;&gt;参考资料&lt;/h1&gt;



    &lt;/div&gt;

    
    &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>a9b2218bb1871dc5b7acfc3b5921bbc0</guid>
<title>细数 ThreadLocal 三大坑，内存泄露仅是小儿科</title>
<link>https://toutiao.io/k/gtp5csf</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;p class=&quot;original_area_primary&quot;&gt;
                                                                                                &lt;/p&gt;

                    
                                            &lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;pre data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-paragraph-type=&quot;ignored&quot;&gt;&lt;p&gt;&lt;span&gt;我在参加Code Review的时候不止一次听到有同学说：&lt;/span&gt;&lt;span&gt;我写的这个上下文工具没问题，在线上跑了好久了。&lt;/span&gt;&lt;span&gt;其实这种想法是有问题的，&lt;/span&gt;&lt;code&gt;ThreadLocal&lt;/code&gt;&lt;span&gt;写错难，但是用错就很容易，本文将会详细总结&lt;/span&gt;&lt;code&gt;ThreadLocal&lt;/code&gt;&lt;span&gt;容易用错的三个坑：&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/pre&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;内存泄露&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;线程池中线程上下文丢失&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;并行流中线程上下文丢失&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;内存泄露&lt;/h2&gt;&lt;section&gt;由于&lt;code&gt;ThreadLocal&lt;/code&gt;的&lt;code&gt;key&lt;/code&gt;是弱引用，因此如果使用后不调用&lt;code&gt;remove&lt;/code&gt;清理的话会导致对应的&lt;code&gt;value&lt;/code&gt;内存泄露。&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;@Test&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;testThreadLocalMemoryLeaks&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    ThreadLocal&amp;lt;List&amp;lt;Integer&amp;gt;&amp;gt; localCache = &lt;span&gt;new&lt;/span&gt; ThreadLocal&amp;lt;&amp;gt;();&lt;br/&gt;   List&amp;lt;Integer&amp;gt; cacheInstance = &lt;span&gt;new&lt;/span&gt; ArrayList&amp;lt;&amp;gt;(&lt;span&gt;10000&lt;/span&gt;);&lt;br/&gt;    localCache.set(cacheInstance);&lt;br/&gt;    localCache = &lt;span&gt;new&lt;/span&gt; ThreadLocal&amp;lt;&amp;gt;();&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;当&lt;code&gt;localCache&lt;/code&gt;的值被重置之后&lt;code&gt;cacheInstance&lt;/code&gt;被&lt;code&gt;ThreadLocalMap&lt;/code&gt;中的&lt;code&gt;value&lt;/code&gt;引用，无法被GC，但是其&lt;code&gt;key&lt;/code&gt;对&lt;code&gt;ThreadLocal&lt;/code&gt;实例的引用是一个弱引用，本来&lt;code&gt;ThreadLocal&lt;/code&gt;的实例被&lt;code&gt;localCache&lt;/code&gt;和&lt;code&gt;ThreadLocalMap&lt;/code&gt;的&lt;code&gt;key&lt;/code&gt;同时引用，但是当&lt;code&gt;localCache&lt;/code&gt;的引用被重置之后，则&lt;code&gt;ThreadLocal&lt;/code&gt;的实例只有&lt;code&gt;ThreadLocalMap&lt;/code&gt;的&lt;code&gt;key&lt;/code&gt;这样一个弱引用了，此时这个实例在GC的时候能够被清理。&lt;/section&gt;&lt;section&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.45454545454545453&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/4xfJbk4AmfjWImbjAuEDxAaUmzknXeLe7yjJLic9ZC5zYGPvHm204A89nib4TrE951uEicndeibiba8OK5CibPbeJ0BQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;715&quot;/&gt;&lt;/figure&gt;&lt;/section&gt;&lt;section&gt;其实看过&lt;code&gt;ThreadLocal&lt;/code&gt;源码的同学会知道，&lt;code&gt;ThreadLocal&lt;/code&gt;本身对于&lt;code&gt;key&lt;/code&gt;为&lt;code&gt;null&lt;/code&gt;的&lt;code&gt;Entity&lt;/code&gt;有自清理的过程，但是这个过程是依赖于后续对&lt;code&gt;ThreadLocal&lt;/code&gt;的继续使用，假如上面的这段代码是处于一个秒杀场景下，会有一个瞬间的流量峰值，这个流量峰值也会将集群的内存打到高位(或者运气不好的话直接将集群内存打满导致故障)，后面由于峰值流量已过，对&lt;code&gt;ThreadLocal&lt;/code&gt;的调用也下降，会使得&lt;code&gt;ThreadLocal&lt;/code&gt;的自清理能力下降，造成内存泄露。&lt;code&gt;ThreadLocal&lt;/code&gt;的自清理是锦上添花，千万不要指望他雪中送碳。&lt;/section&gt;&lt;section&gt;相比于&lt;code&gt;ThreadLocal&lt;/code&gt;中存储的&lt;code&gt;value&lt;/code&gt;对象泄露，&lt;code&gt;ThreadLocal&lt;/code&gt;用在&lt;code&gt;web&lt;/code&gt;容器中时更需要注意其引起的&lt;code&gt;ClassLoader&lt;/code&gt;泄露。&lt;/section&gt;&lt;section&gt;&lt;code&gt;Tomcat&lt;/code&gt;官网对在&lt;code&gt;web&lt;/code&gt;容器中使用&lt;code&gt;ThreadLocal&lt;/code&gt;引起的内存泄露做了一个总结，详见：https://cwiki.apache.org/confluence/display/tomcat/MemoryLeakProtection，这里我们列举其中的一个例子。&lt;/section&gt;&lt;section&gt;熟悉&lt;code&gt;Tomcat&lt;/code&gt;的同学知道，Tomcat中的web应用由&lt;code&gt;Webapp Classloader&lt;/code&gt;这个类加载器的，并且&lt;code&gt;Webapp Classloader&lt;/code&gt;是破坏双亲委派机制实现的，即所有的&lt;code&gt;web&lt;/code&gt;应用先由&lt;code&gt;Webapp classloader&lt;/code&gt;加载，这样的好处就是可以让同一个容器中的&lt;code&gt;web&lt;/code&gt;应用以及依赖隔离。&lt;/section&gt;&lt;section&gt;下面我们看具体的内存泄露的例子：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyCounter&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt; &lt;span&gt;private&lt;/span&gt; &lt;span&gt;int&lt;/span&gt; count = &lt;span&gt;0&lt;/span&gt;;&lt;br/&gt;&lt;br/&gt; &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;increment&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;  count++;&lt;br/&gt; }&lt;br/&gt;&lt;br/&gt; &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;int&lt;/span&gt; &lt;span&gt;getCount&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;  &lt;span&gt;return&lt;/span&gt; count;&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;MyThreadLocal&lt;/span&gt; &lt;span&gt;extends&lt;/span&gt; &lt;span&gt;ThreadLocal&lt;/span&gt;&amp;lt;&lt;span&gt;MyCounter&lt;/span&gt;&amp;gt; &lt;/span&gt;{&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;LeakingServlet&lt;/span&gt; &lt;span&gt;extends&lt;/span&gt; &lt;span&gt;HttpServlet&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt; &lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; MyThreadLocal myThreadLocal = &lt;span&gt;new&lt;/span&gt; MyThreadLocal();&lt;br/&gt;&lt;br/&gt; &lt;span&gt;&lt;span&gt;protected&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;doGet&lt;/span&gt;&lt;span&gt;(HttpServletRequest request,&lt;br/&gt;   HttpServletResponse response)&lt;/span&gt; &lt;span&gt;throws&lt;/span&gt; ServletException, IOException &lt;/span&gt;{&lt;br/&gt;&lt;br/&gt;  MyCounter counter = myThreadLocal.get();&lt;br/&gt;  &lt;span&gt;if&lt;/span&gt; (counter == &lt;span&gt;null&lt;/span&gt;) {&lt;br/&gt;   counter = &lt;span&gt;new&lt;/span&gt; MyCounter();&lt;br/&gt;   myThreadLocal.set(counter);&lt;br/&gt;  }&lt;br/&gt;&lt;br/&gt;  response.getWriter().println(&lt;br/&gt;    &lt;span&gt;&quot;The current thread served this servlet &quot;&lt;/span&gt; + counter.getCount()&lt;br/&gt;      + &lt;span&gt;&quot; times&quot;&lt;/span&gt;);&lt;br/&gt;  counter.increment();&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;需要注意这个例子中的两个非常关键的点：&lt;/section&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;MyCounter&lt;/code&gt;以及&lt;code&gt;MyThreadLocal&lt;/code&gt;必须放到&lt;code&gt;web&lt;/code&gt;应用的路径中，保被&lt;code&gt;Webapp Classloader&lt;/code&gt;加载&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;ThreadLocal&lt;/code&gt;类一定得是&lt;code&gt;ThreadLocal&lt;/code&gt;的继承类，比如例子中的&lt;code&gt;MyThreadLocal&lt;/code&gt;，因为&lt;code&gt;ThreadLocal&lt;/code&gt;本来被&lt;code&gt;Common Classloader&lt;/code&gt;加载，其生命周期与&lt;code&gt;Tomcat&lt;/code&gt;容器一致。&lt;code&gt;ThreadLocal&lt;/code&gt;的继承类包括比较常见的&lt;code&gt;NamedThreadLocal&lt;/code&gt;，注意不要踩坑。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;假如&lt;code&gt;LeakingServlet&lt;/code&gt;所在的&lt;code&gt;Web&lt;/code&gt;应用启动，&lt;code&gt;MyThreadLocal&lt;/code&gt;类也会被&lt;code&gt;Webapp Classloader&lt;/code&gt;加载，如果此时web应用下线，而线程的生命周期未结束(比如为&lt;code&gt;LeakingServlet&lt;/code&gt;提供服务的线程是一个线程池中的线程)，那会导致&lt;code&gt;myThreadLocal&lt;/code&gt;的实例仍然被这个线程引用，而不能被GC，期初看来这个带来的问题也不大，因为&lt;code&gt;myThreadLocal&lt;/code&gt;所引用的对象占用的内存空间不太多，问题在于&lt;code&gt;myThreadLocal&lt;/code&gt;间接持有加载web应用的&lt;code&gt;webapp classloader&lt;/code&gt;的引用（通过&lt;code&gt;myThreadLocal.getClass().getClassLoader()&lt;/code&gt;可以引用到），而加载web应用的&lt;code&gt;webapp classloader&lt;/code&gt;有持有它加载的所有类的引用，这就引起了&lt;code&gt;Classloader&lt;/code&gt;泄露，它泄露的内存就非常可观了。&lt;/section&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;线程池中线程上下文丢失&lt;/h2&gt;&lt;section&gt;&lt;code&gt;ThreadLocal&lt;/code&gt;不能在父子线程中传递，因此最常见的做法是把父线程中的&lt;code&gt;ThreadLocal&lt;/code&gt;值拷贝到子线程中，因此大家会经常看到类似下面的这段代码：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;for&lt;/span&gt;(value in valueList){&lt;br/&gt;     Future&amp;lt;?&amp;gt; taskResult = threadPool.submit(&lt;span&gt;new&lt;/span&gt; BizTask(ContextHolder.get()));&lt;span&gt;//提交任务，并设置拷贝Context到子线程&lt;/span&gt;&lt;br/&gt;     results.add(taskResult);&lt;br/&gt;}&lt;br/&gt;&lt;span&gt;for&lt;/span&gt;(result in results){&lt;br/&gt;    result.get();&lt;span&gt;//阻塞等待任务执行完成&lt;/span&gt;&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;提交的任务定义长这样：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;BizTask&lt;/span&gt;&amp;lt;&lt;span&gt;T&lt;/span&gt;&amp;gt; &lt;span&gt;implements&lt;/span&gt; &lt;span&gt;Callable&lt;/span&gt;&amp;lt;&lt;span&gt;T&lt;/span&gt;&amp;gt;  &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; String session = &lt;span&gt;null&lt;/span&gt;;&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;BizTask&lt;/span&gt;&lt;span&gt;(String session)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;this&lt;/span&gt;.session = session;&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;@Override&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; T &lt;span&gt;call&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;&lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;try&lt;/span&gt; {&lt;br/&gt;            ContextHolder.set(&lt;span&gt;this&lt;/span&gt;.session);&lt;br/&gt;            &lt;span&gt;// 执行业务逻辑&lt;/span&gt;&lt;br/&gt;        } &lt;span&gt;catch&lt;/span&gt;(Exception e){&lt;br/&gt;            &lt;span&gt;//log error&lt;/span&gt;&lt;br/&gt;        } &lt;span&gt;finally&lt;/span&gt; {&lt;br/&gt;            ContextHolder.remove(); &lt;span&gt;// 清理 ThreadLocal 的上下文，避免线程复用时context互串&lt;/span&gt;&lt;br/&gt;        }&lt;br/&gt;        &lt;span&gt;return&lt;/span&gt; &lt;span&gt;null&lt;/span&gt;;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;对应的线程上下文管理类为：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;ContextHolder&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; ThreadLocal&amp;lt;String&amp;gt; localThreadCache = &lt;span&gt;new&lt;/span&gt; ThreadLocal&amp;lt;&amp;gt;();&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;set&lt;/span&gt;&lt;span&gt;(String cacheValue)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        localThreadCache.set(cacheValue);&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; String &lt;span&gt;get&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;return&lt;/span&gt; localThreadCache.get();&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;remove&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        localThreadCache.remove();&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;这么写倒也没有问题，我们再看看线程池的设置：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;ThreadPoolExecutor executorPool = &lt;span&gt;new&lt;/span&gt; ThreadPoolExecutor(&lt;span&gt;20&lt;/span&gt;, &lt;span&gt;40&lt;/span&gt;, &lt;span&gt;30&lt;/span&gt;, TimeUnit.SECONDS, &lt;span&gt;new&lt;/span&gt; LinkedBlockingQueue&amp;lt;Runnable&amp;gt;(&lt;span&gt;40&lt;/span&gt;), &lt;span&gt;new&lt;/span&gt; XXXThreadFactory(), ThreadPoolExecutor.CallerRunsPolicy);&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;其中最后一个参数控制着当线程池满时，该如何处理提交的任务，内置有4种策略&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;ThreadPoolExecutor.AbortPolicy &lt;span&gt;//直接抛出异常&lt;/span&gt;&lt;br/&gt;ThreadPoolExecutor.DiscardPolicy &lt;span&gt;//丢弃当前任务&lt;/span&gt;&lt;br/&gt;ThreadPoolExecutor.DiscardOldestPolicy &lt;span&gt;//丢弃工作队列头部的任务&lt;/span&gt;&lt;br/&gt;ThreadPoolExecutor.CallerRunsPolicy &lt;span&gt;//转串行执行&lt;/span&gt;&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;可以看到，我们初始化线程池的时候指定如果线程池满，则新提交的任务转为串行执行，那我们之前的写法就会有问题了，串行执行的时候调用&lt;code&gt;ContextHolder.remove();&lt;/code&gt;会将主线程的上下文也清理，即使后面线程池继续并行工作，传给子线程的上下文也已经是&lt;code&gt;null&lt;/code&gt;了，而且这样的问题很难在预发测试的时候发现。&lt;/section&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;并行流中线程上下文丢失&lt;/h2&gt;&lt;section&gt;如果&lt;code&gt;ThreadLocal&lt;/code&gt;碰到并行流，也会有很多有意思的事情发生，比如有下面的代码：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;ParallelProcessor&lt;/span&gt;&amp;lt;&lt;span&gt;T&lt;/span&gt;&amp;gt; &lt;/span&gt;{&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;process&lt;/span&gt;&lt;span&gt;(List&amp;lt;T&amp;gt; dataList)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;// 先校验参数，篇幅限制先省略不写&lt;/span&gt;&lt;br/&gt;        dataList.parallelStream().forEach(entry -&amp;gt; {&lt;br/&gt;            doIt();&lt;br/&gt;        });&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;doIt&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        String session = ContextHolder.get();&lt;br/&gt;        &lt;span&gt;// do something&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;这段代码很容易在线下测试的过程中发现不能按照预期工作，因为并行流底层的实现也是一个&lt;code&gt;ForkJoin&lt;/code&gt;线程池，既然是线程池，那&lt;code&gt;ContextHolder.get()&lt;/code&gt;可能取出来的就是一个&lt;code&gt;null&lt;/code&gt;。我们顺着这个思路把代码再改一下：&lt;/section&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;ParallelProcessor&lt;/span&gt;&amp;lt;&lt;span&gt;T&lt;/span&gt;&amp;gt; &lt;/span&gt;{&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;private&lt;/span&gt; String session;&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;ParallelProcessor&lt;/span&gt;&lt;span&gt;(String session)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;this&lt;/span&gt;.session = session;&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;process&lt;/span&gt;&lt;span&gt;(List&amp;lt;T&amp;gt; dataList)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        &lt;span&gt;// 先校验参数，篇幅限制先省略不写&lt;/span&gt;&lt;br/&gt;        dataList.parallelStream().forEach(entry -&amp;gt; {&lt;br/&gt;            &lt;span&gt;try&lt;/span&gt; {&lt;br/&gt;                ContextHolder.set(session);&lt;br/&gt;                &lt;span&gt;// 业务处理&lt;/span&gt;&lt;br/&gt;                doIt();&lt;br/&gt;            } &lt;span&gt;catch&lt;/span&gt; (Exception e) {&lt;br/&gt;                &lt;span&gt;// log it&lt;/span&gt;&lt;br/&gt;            } &lt;span&gt;finally&lt;/span&gt; {&lt;br/&gt;                ContextHolder.remove();&lt;br/&gt;            }&lt;br/&gt;        });&lt;br/&gt;    }&lt;br/&gt;    &lt;br/&gt;    &lt;span&gt;&lt;span&gt;private&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; &lt;span&gt;doIt&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;        String session = ContextHolder.get();&lt;br/&gt;        &lt;span&gt;// do something&lt;/span&gt;&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;修改完后的这段代码可以工作吗？如果运气好，你会发现这样改又有问题，运气不好，这段代码在线下运行良好，这段代码就顺利上线了。不久你就会发现系统中会有一些其他很诡异的bug。原因在于并行流的设计比较特殊，父线程也有可能参与到并行流线程池的调度，那如果上面的&lt;code&gt;process&lt;/code&gt;方法被父线程执行，那么父线程的上下文会被清理。导致后续拷贝到子线程的上下文都为&lt;code&gt;null&lt;/code&gt;，同样产生丢失上下文的问题。&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;- EOF -&lt;/span&gt;&lt;/p&gt;&lt;section donone=&quot;shifuMouseDownCard(&#x27;shifu_c_030&#x27;)&quot; label=&quot;Copyright Reserved by PLAYHUDONG.&quot;&gt;&lt;section&gt;&lt;span&gt;推荐阅读&lt;/span&gt;  &lt;span&gt;点击标题可跳转&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;1、&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5NzMyMjAwMA==&amp;amp;mid=2651495981&amp;amp;idx=1&amp;amp;sn=f36af4c1f0a3873cb879c6f95c455bc8&amp;amp;chksm=bd25f2528a527b4413e40d35510260c7fae3350ec2bc2edb95fee1b78f14dcd084500f2d3a9f&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;Spring新版本抛弃JVM，可独立部署，网友：要自立门户？？？&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;2、&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5NzMyMjAwMA==&amp;amp;mid=2651495976&amp;amp;idx=1&amp;amp;sn=c9fd8885a1a31b7e3415dde818291c53&amp;amp;chksm=bd25f2578a527b4102c332229cd8f1c8020a1089be23a93e5782f618c49c462293cec9306ef5&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;面试题：a==1 &amp;amp;&amp;amp; a==2 &amp;amp;&amp;amp; a==3 是 true 还是 false？&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;3、&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MjM5NzMyMjAwMA==&amp;amp;mid=2651495976&amp;amp;idx=2&amp;amp;sn=c2f18cab0069840902df30fab279a80d&amp;amp;chksm=bd25f2578a527b418b576e7cb5849091de8e753bd6c2f2b2193d46f525bae931f9d7330bd83d&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;MySQL 中的反斜杠 \\，真是太坑了！！&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看完本文有收获？请转发分享给更多人&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;关注「ImportNew」，提升Java技能&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img data-ratio=&quot;0.9166666666666666&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/2A8tXicCG8ylbWIGfdoDED35IRRySQZTXUkJ1eop9MHApzFibKnOo0diboXpl0rmS5mH78YJhsWQv0dhv718A6kUA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;span&gt;点赞和在看就是最大的支持&lt;/span&gt;&lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>64aa8d173630bfee2d2b98c23687b48a</guid>
<title>分布式系统之 leader-followers Replication 深入介绍</title>
<link>https://toutiao.io/k/2fa1y2w</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;entry-inner&quot;&gt;
                                                    
&lt;p&gt;我们在前面有简单讲过Replication的作用，简单说就是为在多个机器上保存同样的拷贝来服务的。有了这个拷贝之后我们就可以做很多事情，比如说它可以成为一个读的源从而分散读的压力，它可以在原来数据机器出问题（或者deploy）等的时候作为一个backup等等。&lt;/p&gt;



&lt;p&gt;这个想法其实很简单，但真正在我们做这个拷贝的时候，会遇到很多问题，比如说我们是使用同步还是使用异步来进行同步多个拷贝，如何保证多个拷贝之间的一致性等等。那么本文就来从各个方面详细介绍这些内容。&lt;/p&gt;



&lt;h2&gt;Leaders和Followers&lt;/h2&gt;



&lt;p&gt;我们把每一个保存数据的节点称之为replica，当我们有多个节点的时候，最明显的一个问题就是怎么去保证每个节点的内容都是一样的呢？其中最常见的方法就是基于leader的模式（也成为master-slave模式或者active/passive模式）。总得来说，它的工作方法如下：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;一个节点是leader。所有的写操作都必须经过leader。&lt;/li&gt;&lt;li&gt;其他的节点我们成为follower，每次leader写数据的时候，也把相关的内容发送到每一个follower（replication log），然后每个follower根据这些log来更新本地的数据。&lt;/li&gt;&lt;li&gt;当有读的操作的时候，既可以从leader读也可以从follower读。&lt;/li&gt;&lt;/ol&gt;



&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img src=&quot;https://donggeitnote.com/wp-content/uploads/2021/05/image-29.png&quot; alt=&quot;&quot; class=&quot;wp-image-642&quot; srcset=&quot;https://donggeitnote.com/wp-content/uploads/2021/05/image-29.png 624w, https://donggeitnote.com/wp-content/uploads/2021/05/image-29-300x100.png 300w&quot; sizes=&quot;(max-width: 624px) 100vw, 624px&quot;/&gt;&lt;/figure&gt;



&lt;h3&gt;同步VS.异步Replication&lt;/h3&gt;



&lt;p&gt;上面这种方法其实在很多关系型数据库中很常见。这里遇到的一个最大的问题就是使用同步还是异步来进行replication。&lt;/p&gt;



&lt;p&gt;就比如上面的这个case，它实现的功能很简单。就是把user id 1234这个用户的picture更新一下，它首先发送了update的请求到了leader的replica上。在这之后，leader会把这个更新的请求分别发送给两个follower的replica，从而最终达到所有的replica上这个用户的picture都更新了的效果。&lt;/p&gt;



&lt;p&gt;我们就以这个例子来分析一下同步和异步在这两个上面的差别。如下图所示，我们看到leader的返回是在Follower1 更新完成之后，也就是说follower1是一个同步的更新。而leader并没有等待follower2完成更新再返回，这也就意味着follower2的更新是一个异步的。&lt;/p&gt;



&lt;figure class=&quot;wp-block-image size-large&quot;&gt;&lt;img src=&quot;https://donggeitnote.com/wp-content/uploads/2021/05/image-30.png&quot; alt=&quot;&quot; class=&quot;wp-image-643&quot; srcset=&quot;https://donggeitnote.com/wp-content/uploads/2021/05/image-30.png 624w, https://donggeitnote.com/wp-content/uploads/2021/05/image-30-300x122.png 300w&quot; sizes=&quot;(max-width: 624px) 100vw, 624px&quot;/&gt;&lt;/figure&gt;



&lt;p&gt;从这个图中可以看到follower1和follower2的更新其实都是由一个延时的，尽管大多数时候这个延时都很小，但是当我们遇到网络问题或者别的情况的时候（CPU占用率很高，内存不够等等），这个延时有可能会很大，谁也不能保证什么时候能被更新。&lt;/p&gt;



&lt;p&gt;所以，同步更新的好处就是follower其实和leader是同步的，当leader有问题的时候我们甚至可以直接切换到同步更新的follower，当然它的问题也很明显，就是每一个更新（写操作）都需要等待这个follower的更新完成才行，这有可能会导致整个request延迟很久，甚至超时。所以，假如你想做同步的更新，一般来说也不会想让所有的节点都同步，而是选择leader和一个节点是同步的，这样就可以在leader出问题的切换和同步的效率之间达到一个很好的trade off。&lt;/p&gt;



&lt;p&gt;当然，事实情况下基于leader的replication基本都是完全使用异步。这样的问题就是leader出问题，那些没有sync到别的node的数据就会丢失。好处就是leader完全不收别的节点的影响，哪怕别的节点都出问题也可以正常独立运行（通常这个时候就会出alert，然后oncall就到了表现的时候了，哈哈）。生产环境中这样的选择其实有很多原因，比如说为了保证随时都有节点是健康的，这些节点通常会分布在不同的地区（数据中心），所以他们之间的通信通常不是那么可靠，如果使用同步就会很容易出现问题等等。&lt;/p&gt;



&lt;h3&gt;Follower的建立&lt;/h3&gt;



&lt;p&gt;有时我们需要从无到有建立一个新的follower，这种情况在生产环境中特别常见，比如说机器的磁盘坏了，原有follower的数据就都不能用了，这个时候就需要从头开始建立，或者说某个时刻你没有足够的follower了（比如说数据中心出问题了，或者机器出问题了），你需要在一个新的机器上建立一个 follower，那么如何从头开始来建立一个节点呢？&lt;/p&gt;



&lt;p&gt;首先想到的就是从leader上拷贝过来就好了，但是我们知道leader其实是在不停改变的，也就是说随时都在不停地被写入，而数据的拷贝速度不会很快（主要瓶颈在HDD的写入速度上，目前基本在200MB/s左右），所以在这个过程中不允许写可能也不是一个很好的方法。那么一般来说是怎么做的呢？大概会有以下的步骤：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;得到一个数据库某一个节点的snapshot（这个snapshot不影响写）。大多数数据库都支持这样的功能。&lt;/li&gt;&lt;li&gt;拷贝这个snapshot到新的follower&lt;/li&gt;&lt;li&gt;Follower联系leader去得到所有的在这个snapshot之后的操作（log sequence）。&lt;/li&gt;&lt;li&gt;然后follower根据得到的信息来apply snapshot之后 的操作，我们称这个过程为catch up。&lt;/li&gt;&lt;/ol&gt;



&lt;h3&gt;处理节点的中断&lt;/h3&gt;



&lt;p&gt;我们上文中也提到了任何节点在任何时候都有可能出问题。那么要是真的出问题了，我们一般会怎么处理呢？&lt;/p&gt;



&lt;p&gt;假如这个出问题的节点是follower，假如这个问题只是service crash或者server reboot，也就是说磁盘上的数据还是好的，那么我们常见的处理方法就是等到系统恢复过来之后，看一下磁盘中的数据库上次replay到了那个地方（通常会保存在数据库的头信息中），然后根据收到的log（以及沟通leader得到server reboot这段时间的log）来继续replay，就和上面提到的catch up是一样的。&lt;/p&gt;



&lt;p&gt;假如出问题的这个节点是leader，那么情况就稍微有点复杂了。因为这个时候需要找一个follower来成为新的leader，所有的写操作需要指向这个新的leader，别的follower也需要从这个新的leader来获取信息。我们称这个过程为failover。Failover可以是手动也可以是自动的，自动的failover有以下这些过程：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;判断leader是真的有问题了。这就需要一些node down detection的机制，比如类似的心跳机制来判断leader是不是真的有问题，等到确定有问题了才来继续下面的机制。其实这里如何进行detection是一个很好玩的话题，笔者在现实生产环境中也曾为这个问题伤过脑筋，比如你怎么判断是leader的问题还是你本身的问题，谁来决定leader有问题等等。&lt;/li&gt;&lt;li&gt;选择一个新的leader。这个通常有一个选举的过程，就是谁是最好的候选人，需要大家一起来决定，这里就有很多机制值得探讨，比如majority的election等等。&lt;/li&gt;&lt;li&gt;重新配置系统来使用新的leader。当新的leader选择好了之后就需要通知所有的traffic切换到新的leader来处理数据流了。&lt;/li&gt;&lt;/ol&gt;



&lt;p&gt;Failover的过程中其实会遇到很多问题：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;假如我们使用的是async的方法来进行同步数据，那么新的leader可能是有数据丢失的。这个时候假如旧的leader回来了，他们之间就会有冲突了，如何处理这些冲突呢？一种常见的处理方法就是把旧leader里面的数据丢失掉，不过这样就相当于我们丢失了用户的数据。&lt;/li&gt;&lt;li&gt;丢失掉写的数据有时会有很大的问题，比如说我们是和别的系统相关联的，而这个丢失的写正好的别的系统很在意的，这样就会出现数据的冲突。&lt;/li&gt;&lt;li&gt;多个leader的情况，比如说原来的leader还认为它自己的leader，而新的leader也觉得自己是leader。这种情况我们称之为split brain。这就很危险了，因为两个leader都在接受写操作，然后还没有解决写冲突的机制，那么显然情况就会很糟糕。这个时候你就需要一个leader的monitor机制来确保不要同时有两个leader。&lt;/li&gt;&lt;li&gt;如何来决定leader有问题。这个是一个很难的问题，因为生产换机的网络其实有时很不稳定，你10s收不到心跳有可能只是一个网络的波动。假如你觉得10s是一个不能接受的阈值，那么你可能会一直在切换leader，假如你时间设置长了，就意味着真的出问题的时候，你在这段长的时间内就没法操作了。这里就是一个trade off有时需要结合现实情况来处理。&lt;/li&gt;&lt;/ol&gt;



&lt;h3&gt;如何实现replication的log&lt;/h3&gt;



&lt;p&gt;我们在上面一直说可以根据leader的log来catch up，那么这个log怎么产生呢？一个简单地的实现就是数据库的每一个操作产生一条log，这样follower就可以根据这个log来重复相应的操作，比如INSERT,UPDATE等。这个实现乍一听起来还不错，但一细想就会发现好像有点问题：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;任何调用不确定的数值都会有问题，比如说NOW()或者RAND()函数，假如你只是传递相关操作，那么在两个机器上执行的结果就会有差别。&lt;/li&gt;&lt;li&gt;假如操作取决于别的条件，比如UPDATE..WHEN，那么就需要所有的log执行的顺序是一模一样的，不能有任何差别&lt;/li&gt;&lt;li&gt;状态相关的内容可能会在不同的server有不能表现。&lt;/li&gt;&lt;/ol&gt;



&lt;p&gt;当然这些问题都是可以解决，比如说我们可以把RAND()替换中真正得到的数等等。&lt;/p&gt;



&lt;h3&gt;往前写的log&lt;/h3&gt;



&lt;p&gt;我们在之前有聊到过，很多时候数据库本身把数据写到磁盘，还会实现一个一直往前写的log，用来防止磁盘写发生问题，从而可以恢复。这里我们其实也可以把这种log发送给follower，这样每一个follower就可以根据这个log来catch up。&lt;/p&gt;



&lt;h3&gt;逻辑log的replication&lt;/h3&gt;



&lt;p&gt;还有一种常见的log就是逻辑log，它和数据库引擎使用的log不同，基本实现如下：&lt;/p&gt;



&lt;ol type=&quot;1&quot;&gt;&lt;li&gt;假如insert一行，log就包含插入行的所有列的数据。&lt;/li&gt;&lt;li&gt;假如delete一行，log就包含足够的区分行的信息。&lt;/li&gt;&lt;li&gt;假如更新一样，log包含做够区分行的信息以及需要所有的更新后的列信息（或者至少修改的列信息）&lt;/li&gt;&lt;/ol&gt;



&lt;h2&gt;总结&lt;/h2&gt;



&lt;p&gt;至此，本文就总结了leader-follower这种形式的结构是如何运行，以及错误发生时的处理机制，最后还介绍了几种常见的replication的方法。&lt;/p&gt;
                                                    &lt;nav class=&quot;pagination group&quot;&gt;
                      &lt;/nav&gt;
        &lt;/div&gt;

        
        &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>