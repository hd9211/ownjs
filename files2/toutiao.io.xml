<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>5562370abedb19ef10cba80377e171ac</guid>
<title>Java 知识点整理：Spring、MySQL</title>
<link>https://toutiao.io/k/j4kd9qh</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;post-topic-des nc-post-content&quot;&gt;
&lt;h2 id=&quot;spring-10&quot;&gt;Spring 10&lt;/h2&gt; 
&lt;h3&gt;P1：Spring 框架&lt;/h3&gt; 
&lt;p&gt;Spring 是分层的企业级应用轻量级开源框架，以 IoC 和 AOP为内核。Spring 可以降低企业级应用开发的复杂性，对此主要采取了四个关键策略：基于 POJO 的轻量级和最小侵入性编程、通过依赖注入和面向接口实现松耦合、基于切面和惯性进行声明式编程、通过切面和模板减少样板式代码。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;好处&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;降低代码耦合度、简化开发。通过 Spring 提供的 IoC 容器可以将对象间的依赖关系交由 Spring 进行控制，避免硬编码所造成的过度程序耦合。用户也不必再为单例模式类、属性文件解析等这些底层的需求编写代码，可以更专注于上层的应用。&lt;/p&gt; 
&lt;p&gt;AOP 编程以及声明式事务的支持。通过 Spring 的 AOP 功能可以方便进行面向切面的编程，通过声明式事务可以灵活进行事务管理，提高开发效率和质量。&lt;/p&gt; 
&lt;p&gt;方便程序的测试和集成各种框架。可以用非容器依赖的编程方式进行几乎所有的测试工作，可以降低各种框架的使用难度，提供了对 Mybatis 和 Hibernate 等框架的直接支持。&lt;/p&gt; 
&lt;p&gt;降低了 JavaEE API 的使用难度。Spring 对 JDBC、JavaMail、远程调用等 API 进行了封装，使这些 API 的使用难度大幅降低。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;核心容器&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;核心容器由 spring-beans、spring-core、spring-context 和 spring-expression 四个模块组成。&lt;/p&gt; 
&lt;p&gt;spring-beans 和 spring-core 模块是 Spring 的核心模块，包括了控制反转和依赖注入。BeanFactory 使用控制反转对应用程序的配置和依赖性规范与实际的应用代码进行分离，BeanFactory 实例化后并不会自动实例化 Bean，只有当 Bean 被使用时才会对其进行实例化与依赖关系的装配。&lt;/p&gt; 
&lt;p&gt;spring-context 模块构架于核心模块之上，扩展了 BeanFactory，为它添加了 Bean 的生命周期控制、框架事件体系及资源透明化加载等功能。ApplicationConext 是该模块的核心接口，它是 BeanFactory 的子接口，它实例化后会自动对所有单例 Bean 进行实例化与依赖关系的装配，使之处于待用状态。&lt;/p&gt; 
&lt;p&gt;spring-expression 是 EL 语言的扩展模块，可以查询、管理运行中的对象，同时也可以方便地调用对象方法，以及操作数组、集合等。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P2：IoC 控制反转&lt;/h3&gt; 
&lt;p&gt;IoC 即控制反转，是一种给予应用程序中目标组件更多控制的设计范式，简单来说就是把原来代码里需要实现的对象创建、依赖反转给容器来帮忙实现，需要创建一个容器并且需要一种描述来让容器知道要创建的对象之间的关系，在 Spring 框架中管理对象及其依赖关系是通过 Spring 的 IoC 容器实现的，IoC 的作用是降低代码耦合度。&lt;/p&gt; 
&lt;p&gt;IoC 的实现方式有依赖注入和依赖查找，由于依赖查找使用的很少，因此 IoC 也叫做依赖注入。依赖注入指对象被动地接受依赖类而不用自己主动去找，对象不是从容器中查找它依赖的类，而是在容器实例化对象时主动将它依赖的类注入给它。假设一个 Car 类需要一个 Engine 的对象，那么一般需要需要手动 new 一个 Engine，利用 IoC 就只需要定义一个私有的 Engine 类型的成员变量，容器会在运行时自动创建一个 Engine 的实例对象并将引用自动注入给成员变量。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;基于 XML 的容器初始化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;当创建一个 ClassPathXmlApplicationContext 时，构造器做了两件事：首先调用父容器的构造器为容器设置好 Bean 资源加载器，然后调用父类的 setConfigLocations 方法设置 Bean 配置信息的定位路径。&lt;/p&gt; 
&lt;p&gt;ClassPathXmlApplicationContext 通过调用父类 AbstractApplicationContext 的 refresh 方法启动整个 IoC 容器对 Bean 定义的载入过程，refresh 是一个模板方法，规定了 IoC 容器的启动流程。refresh 方法的主要作用是：在创建 IoC 容器之前如果已有容器存在，需要把已有的容器销毁和关闭，以保证在 refresh 方法之后使用的是新创建的 IoC 容器。&lt;/p&gt; 
&lt;p&gt;容器创建后通过 loadBeanDefinitions 方法加载 Bean 配置资源，该方***做两件事：首先调用资源加载器的方法获取要加载的资源，其次真正执行加载功能，由子类 XmlBeanDefinitionReader 实现。在加载资源时，首先会解析配置文件路径，读取配置文件的内容，然后通过 XML 解析器将 Bean 配置信息转换成文档对象，之后再按照 Spring Bean 的定义规则对文档对象进行解析。&lt;/p&gt; 
&lt;p&gt;Spring IoC 容器中注册解析的 Bean 信息存放在一个 HashMap 集合中，key 是 String 字符串，值是 BeanDefinition，在注册过程中需要使用 synchronized 同步块保证线程安全。当 Bean 配置信息中配置的 Bean 被解析后且被注册到 IoC 容器中，初始化就算真正完成了，Bean 定义信息已经可以使用，并且可以被检索。Spring IoC 容器的作用就是对这些注册的 Bean 定义信息进行处理和维护，注册的 Bean 定义信息是控制反转和依赖注入的基础。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;基于注解的容器初始化&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Spring 对注解的处理分为两种方式：① 直接将注解 Bean 注册到容器中，可以在初始化容器时注册，也可以在容器创建之后手动注册，然后刷新容器使其对注册的注解 Bean 进行处理。② 通过扫描指定的包及其子包的所有类处理，在初始化注解容器时指定要自动扫描的路径。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P3：DI 依赖注入&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;可注入的数据类型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;基本数据类型和 String、集合类型、Bean 类型。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;实现方式&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;构造器注入：IoC Service Provider 会检查被注入对象的构造器，取得它所需要的依赖对象列表，进而为其注入相应的对象。这种方法的优点是在对象构造完成后就处于就绪状态，可以马上使用。缺点是当依赖对象较多时，构造器的参数列表会比较长，构造器无法被继承，无法设置默认值。对于非必需的依赖处理可能需要引入多个构造器，参数数量的变动可能会造成维护的困难。&lt;/p&gt; 
&lt;p&gt;setter 方法注入：当前对象只需要为其依赖对象对应的属性添加 setter 方法，就可以通过 setter 方法将依赖对象注入到被依赖对象中。setter 方法注入在描述性上要比构造器注入强，并且可以被继承，允许设置默认值。缺点是无法在对象构造完成后马上进入就绪状态。&lt;/p&gt; 
&lt;p&gt;接口注入：必须实现某个接口，这个接口提供一个方法来为其注入依赖对象。使用较少，因为它强制要求被注入对象实现不必要的接口，侵入性强。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关注解&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Autowired&lt;/code&gt;：自动按类型注入，如果有多个匹配则按照指定 Bean 的 id 查找，查找不到会报错。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Qualifier&lt;/code&gt;：在自动按照类型注入的基础上再按照 Bean 的 id 注入，给变量注入时必须搭配 &lt;code&gt;@Autowired&lt;/code&gt;，给方法注入时可单独使用。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Resource&lt;/code&gt; ：直接按照 Bean 的 id 注入，只能注入 Bean 类型。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Value&lt;/code&gt; ：用于注入基本数据类型和 String 类型。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;依赖注入的过程&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;getBean 方法是获取 Bean 实例的方法，该方***调用 doGetBean 方法，doGetBean 真正实现向 IoC 容器获取 Bean 的功能，也是触发依赖注入的地方。如果 Bean 定义为单例模式，容器在创建之前先从缓存中查找以确保整个容器中只存在一个实例对象。如果 Bean 定义为原型模式，则容器每次都会创建一个新的实例。&lt;/p&gt; 
&lt;p&gt;具体创建 Bean 实例对象的过程由 ObjectFactory 的 createBean 方法完成，该方法主要通过 createBeanInstance 方法生成 Bean 包含的 Java 对象实例和 populateBean 方法对 Bean 属性的依赖注入进行处理。&lt;/p&gt; 
&lt;p&gt;在 createBeanInstance 方法中根据指定的初始化策略，通过简单工厂、工厂方法或容器的自动装配特性生成 Java 实例对象，对工厂方法和自动装配特性的 Bean，调用相应的工厂方法或参数匹配的构造器即可完成实例化对象的工作，但最常用的默认无参构造器需要使用 JDK 的反射或 CGLib 来进行初始化。&lt;/p&gt; 
&lt;p&gt;在 populateBean 方法中，注入过程主要分为两种情况：① 属性值类型不需要强制转换时，不需要解析属性值，直接进行依赖注入。② 属性值类型需要强制转换时，首先需要解析属性值，然后对解析后的属性值进行依赖注入。依赖注入的过程就是将 Bean 对象实例设置到它所依赖的 Bean 对象属性上，真正的依赖注入是通过 setPropertyValues 方法实现的，该方法使用了委派模式。&lt;/p&gt; 
&lt;p&gt;BeanWrapperImpl 类负责对容器完成初始化的 Bean 实例对象进行属性的依赖注入，对于非集合类型的属性，大量使用 JDK 的反射机制，通过属性的 getter 方法获取指定属性注入前的值，同时调用属性的 setter 方法为属性设置注入后的值。对于集合类型的属性，将属性值解析为目标类型的集合后直接赋值给属性。&lt;/p&gt; 
&lt;p&gt;当 Spring IoC 容器对 Bean 定义资源的定位、载入、解析和依赖注入全部完成后，就不再需要我们手动创建所需的对象，Spring IoC 容器会自动为我们创建对象并且注入好相关依赖。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P4：Bean 对象&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;生命周期&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 IoC 容器的初始化过程中会对 Bean 定义完成资源定位，加载读取配置并解析，最后将解析的 Bean 信息放在一个 HashMap 集合中。当 IoC 容器初始化完成后，会进行对 Bean 实例的创建和依赖注入过程，注入对象依赖的各种属性值，在初始化时可以指定自定义的初始化方法。经过这一系列初始化操作后 Bean 达到可用状态，接下来就可以使用 Bean 了，当使用完成后会调用 destroy 方法进行销毁，此时也可以指定自定义的销毁方法，最终 Bean 被销毁且从容器中移除。&lt;/p&gt; 
&lt;p&gt;指定 Bean 初始化和销毁的方法：&lt;/p&gt; 
&lt;p&gt;XML 方式通过配置 bean 标签中的 init-Method 和 destory-Method 指定自定义初始化和销毁方法。 &lt;/p&gt; 
&lt;p&gt;注解方式通过 &lt;code&gt;@PreConstruct&lt;/code&gt; 和 &lt;code&gt;@PostConstruct&lt;/code&gt; 注解指定自定义初始化和销毁方法。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;作用范围&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过 scope 属性指定 bean 的作用范围，包括：① singleton：单例模式，是默认作用域，不管收到多少 Bean 请求每个容器中只有一个唯一的 Bean 实例。② prototype：原型模式，和 singleton 相反，每次 Bean 请求都会创建一个新的实例。③ request：每次 HTTP 请求都会创建一个新的 Bean 并把它放到 request 域中，在请求完成后 Bean 会失效并被垃圾收集器回收。④ session：和 request 类似，确保每个 session 中有一个 Bean 实例，session 过期后 bean 会随之失效。⑤ global session：当应用部署在 Portlet 容器中时，如果想让所有 Portlet 共用全局存储变量，那么这个变量需要存储在 global session 中。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;创建方式&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;XML&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过默认无参构造器，只需要指明 bean 标签中的 id 和 class 属性，如果没有无参构造器会报错。&lt;/p&gt; 
&lt;p&gt;使用静态工厂方法，通过 bean 标签中的 class 属性指明静态工厂，factory-method 属性指明静态工厂方法。&lt;/p&gt; 
&lt;p&gt;使用实例工厂方法，通过 bean 标签中的 factory-bean 属性指明实例工厂，factory-method 属性指明实例工厂方法。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;注解&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Component&lt;/code&gt; 把当前类对象存入 Spring 容器中，相当于在 xml 中配置一个 bean 标签。value 属性指定 bean 的 id，默认使用当前类的首字母小写的类名。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Controller&lt;/code&gt;，&lt;code&gt;@Service&lt;/code&gt;，&lt;code&gt;@Repository&lt;/code&gt; 三个注解都是 &lt;code&gt;@Component&lt;/code&gt; 的衍生注解，作用及属性都是一模一样的。只是提供了更加明确语义，&lt;code&gt;@Controller&lt;/code&gt; 用于表现层，&lt;code&gt;@Service&lt;/code&gt;用于业务层，&lt;code&gt;@Repository&lt;/code&gt;用于持久层。如果注解中有且只有一个 value 属性要赋值时可以省略 value。&lt;/p&gt; 
&lt;p&gt;如果想将第三方的类变成组件又没有源代码，也就没办法使用 &lt;code&gt;@Component&lt;/code&gt; 进行自动配置，这种时候就要使用 &lt;code&gt;@Bean&lt;/code&gt; 注解。被 &lt;code&gt;@Bean&lt;/code&gt; 注解的方法返回值是一个对象，将会实例化，配置和初始化一个新对象并返回，这个对象由 Spring 的 IoC 容器管理。name 属性用于给当前 &lt;code&gt;@Bean&lt;/code&gt; 注解方法创建的对象指定一个名称，即 bean 的 id。当使用注解配置方法时，如果方法有参数，Spring 会去容器查找是否有可用 bean对象，查找方式和 &lt;code&gt;@Autowired&lt;/code&gt; 一样。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Configuration&lt;/code&gt; 用于指定当前类是一个 spring 配置类，当创建容器时会从该类上加载注解，value 属性用于指定配置类的字节码。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@ComponentScan&lt;/code&gt; 用于指定 Spring 在初始化容器时要扫描的包。basePackages 属性用于指定要扫描的包。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@PropertySource&lt;/code&gt; 用于加载 &lt;code&gt;.properties&lt;/code&gt; 文件中的配置。value 属性用于指定文件位置，如果是在类路径下需要加上 classpath。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Import&lt;/code&gt; 用于导入其他配置类，在引入其他配置类时可以不用再写 &lt;code&gt;@Configuration&lt;/code&gt; 注解。有 &lt;code&gt;@Import&lt;/code&gt; 的是父配置类，引入的是子配置类。value 属性用于指定其他配置类的字节码。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;BeanFactory、FactoryBean 和 ApplicationContext 的区别&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;BeanFactory 是一个 Bean 工厂，实现了工厂模式，是 Spring IoC 容器最顶级的接口，可以理解为含有 Bean 集合的工厂类，它的作用是管理 Bean，包括实例化、定位、配置应用程序中的对象及建立这些对象之间的依赖。BeanFactory 实例化后并不会自动实例化 Bean，只有当 Bean 被使用时才会对其进行实例化与依赖关系的装配，属于延迟加载，适合多例模式。&lt;/p&gt; 
&lt;p&gt;FactoryBean 是一个工厂 Bean，作用是生产其他 Bean 实例，可以通过实现该接口，提供一个工厂方法来自定义实例化 Bean 的逻辑。&lt;/p&gt; 
&lt;p&gt;ApplicationConext 是 BeanFactory 的子接口，扩展了 BeanFactory 的功能，提供了支持国际化的文本消息，统一的资源文件读取方式，事件传播以及应用层的特别配置等。容器会在初始化时对配置的 Bean 进行预实例化，Bean 的依赖注入在容器初始化时就已经完成，属于立即加载，适合单例模式，一般推荐使用 ApplicationContext。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P5：AOP 面向切面编程&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;概念和原理&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;AOP 即面向切面编程，简单地说就是将代码中重复的部分抽取出来，在需要执行的时候使用动态代理的技术，在不修改&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%BA%90%E7%A0%81&quot; target=&quot;_blank&quot;&gt;源码&lt;/a&gt;的基础上对方法进行增强。优点是可以减少代码的冗余，提高开发效率，维护方便。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;Spring 会根据类是否实现了接口来判断动态代理的方式，如果实现了接口会使用 JDK 的动态代理，核心是 InvocationHandler 接口和 Proxy 类，如果没有实现接口会使用 CGLib 动态代理，CGLib 是在运行时动态生成某个类的子类，如果某一个类被标记为 final，是不能使用 CGLib 动态代理的。&lt;/p&gt; 
&lt;p&gt;JDK 动态代理主要通过重组字节码实现，首先获得被代理对象的引用和所有接口，生成新的类必须实现被代理类的所有接口，动态生成Java 代码后编译新生成的 &lt;code&gt;.class&lt;/code&gt; 文件并重新加载到 JVM 运行。JDK 代理直接写 Class 字节码，CGLib是采用ASM框架写字节码，生成代理类的效率低。但是CGLib调用方法的效率高，因为 JDK 使用反射调用方法，CGLib 使用 FastClass 机制为代理类和被代理类各生成一个类，这个类会为代理类或被代理类的方法生成一个 index，这个 index 可以作为参数直接定位要调用的方法。&lt;/p&gt; 
&lt;p&gt;常用场景包括权限认证、自动缓存、错误处理、日志、调试和事务等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关注解&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Aspect&lt;/code&gt;：声明被注解的类是一个切面 Bean。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Before&lt;/code&gt;：前置通知，指在某个连接点之前执行的通知。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@After&lt;/code&gt;：后置通知，指某个连接点退出时执行的通知（不论正常返回还是异常退出）。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@AfterReturning&lt;/code&gt;：返回后通知，指某连接点正常完成之后执行的通知，返回值使用returning属性接收。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@AfterThrowing&lt;/code&gt;：异常通知，指方法抛出异常导致退出时执行的通知，和&lt;code&gt;@AfterReturning&lt;/code&gt;只会有一个执行，异常使用throwing属性接收。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;相关术语&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Aspect&lt;/code&gt;：切面，一个关注点的模块化，这个关注点可能会横切多个对象。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Joinpoint&lt;/code&gt;：连接点，程序执行过程中的某一行为，即业务层中的所有方法。。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Advice&lt;/code&gt;：通知，指切面对于某个连接点所产生的动作，包括前置通知、后置通知、返回后通知、异常通知和环绕通知。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Pointcut&lt;/code&gt;：切入点，指被拦截的连接点，切入点一定是连接点，但连接点不一定是切入点。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Proxy&lt;/code&gt;：代理，Spring AOP 中有 JDK 动态代理和 CGLib 代理，目标对象实现了接口时采用 JDK 动态代理，反之采用 CGLib 代理。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Target&lt;/code&gt;：代理的目标对象，指一个或多个切面所通知的对象。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Weaving&lt;/code&gt; ：织入，指把增强应用到目标对象来创建代理对象的过程。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AOP 的过程&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Spring AOP 是由 BeanPostProcessor 后置处理器开始的，这个后置处理器是一个***，可以监听容器触发的 Bean 生命周期事件，向容器注册后置处理器以后，容器中管理的 Bean 就具备了接收 IoC 容器回调事件的能力。BeanPostProcessor 的调用发生在 Spring IoC 容器完成 Bean 实例对象的创建和属性的依赖注入之后，为 Bean 对象添加后置处理器的入口是 initializeBean 方法。&lt;/p&gt; 
&lt;p&gt;Spring 中 JDK 动态代理生通过 JdkDynamicAopProxy 调用 Proxy 的 newInstance 方法来生成代理类，JdkDynamicAopProxy 也实现了 InvocationHandler 接口，invoke 方法的具体逻辑是先获取应用到此方法上的拦截器链，如果有拦截器则创建 MethodInvocation 并调用其 proceed 方法，否则直接反射调用目标方法。因此 Spring AOP 对目标对象的增强是通过拦截器实现的。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P6：Spring MVC 核心组件&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;DispatcherServlet&lt;/code&gt;&lt;span&gt;：SpringMVC 中的&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%89%8D%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;前端&lt;/a&gt;控制器，是整个流程控制的核心，负责接收请求并转发给对应的处理组件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Handler&lt;/code&gt;：处理器，完成具体业务逻辑，相当于 Servlet 或 Action。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HandlerMapping&lt;/code&gt;：完成URL 到 Controller映射的组件，DispatcherServlet 接收到请求之后，通过 HandlerMapping 将不同的请求映射到不同的 Handler。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HandlerInterceptor&lt;/code&gt;：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HandlerExecutionChain&lt;/code&gt;：处理器执行链，包括两部分内容：Handler 和 HandlerInterceptor。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HandlerAdapter&lt;/code&gt;：处理器适配器，Handler执行业务方法前需要进行一系列操作，包括表单数据验证、数据类型转换、将表单数据封装到JavaBean等，这些操作都由 HandlerAdapter 完成。DispatcherServlet 通过 HandlerAdapter 来执行不同的 Handler。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ModelAndView&lt;/code&gt;：装载了模型数据和视图信息，作为 Handler 的处理结果返回给 DispatcherServlet。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;ViewResolver&lt;/code&gt;&lt;span&gt;：视图解析器，DispatcherServlet 通过它将逻辑视图解析为物理视图，最终将渲染的结果响应给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P7：Spring MVC 处理流程&lt;/h3&gt; 
&lt;p&gt;Web 容器启动时会通知 Spring 初始化容器，加载 Bean 的定义信息并初始化所有单例 Bean，然后遍历容器中的 Bean，获取每一个 Controller 中的所有方法访问的 URL，将 URL 和对应的 Controller 保存到一个 Map 集合中。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;所有的请求会转发给 DispatcherServlet &lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%89%8D%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;前端&lt;/a&gt;处理器处理，DispatcherServlet 会请求 HandlerMapping 找出容器中被 &lt;/span&gt;&lt;code&gt;@Controler&lt;/code&gt; 注解修饰的 Bean 以及被 &lt;code&gt;@RequestMapping&lt;/code&gt; 修饰的方法和类，生成 Handler 和 HandlerInterceptor 并以一个 HandlerExcutionChain 处理器执行链的形式返回。&lt;/p&gt; 
&lt;p&gt;之后 DispatcherServlet 使用 Handler 找到对应的 HandlerApapter，通过 HandlerApapter 调用 Handler 的方法，将请求参数绑定到方法的形参上，执行方法处理请求并得到 ModelAndView。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;最后 DispatcherServlet 根据使用 ViewResolver 试图解析器对得到的 ModelAndView 逻辑视图进行解析得到 View 物理视图，然后对视图渲染，将数据填充到视图中并返回给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;注解&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Controller&lt;/code&gt;：在类定义处添加，将类交给IoC容器管理。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@RequtestMapping&lt;/code&gt;：将URL请求和业务方法映射起来，在类和方法定义上都可以添加该注解。&lt;code&gt;value&lt;/code&gt; 属性指定URL请求的实际地址，是默认值。&lt;code&gt;method&lt;/code&gt; 属性限制请求的方法类型，包括GET、POST、PUT、DELETE等。如果没有使用指定的请求方法请求URL，会报405 Method Not Allowed 错误。&lt;code&gt;params&lt;/code&gt; 属性限制必须提供的参数，如果没有会报错。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@RequestParam&lt;/code&gt;：如果 Controller 方法的形参和 URL 参数名一致可以不添加注解，如果不一致可以使用该注解绑定。&lt;code&gt;value&lt;/code&gt; 属性表示HTTP请求中的参数名。&lt;code&gt;required&lt;/code&gt; 属性设置参数是否必要，默认false。&lt;code&gt;defaultValue&lt;/code&gt; 属性指定没有给参数赋值时的默认值。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@PathVariable&lt;/code&gt;：Spring MVC 也支持 RESTful 风格的 URL，通过 &lt;code&gt;@PathVariable&lt;/code&gt; 完成请求参数与形参的绑定。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P8：Spring Data JPA 框架&lt;/h3&gt; 
&lt;p&gt;Spring Data JPA 是 Spring 基于 ORM 框架、JPA 规范的基础上封装的一套 JPA 应用框架，可使开发者用极简的代码实现对数据库的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展，可以极大提高开发效率。&lt;/p&gt; 
&lt;p&gt;ORM 即 Object-Relational Mapping ，表示对象关系映射，映射的不只是对象的值还有对象之间的关系，通过 ORM 就可以把对象映射到关系型数据库中。操作实体类就相当于操作数据库表，可以不再重点关注 SQL 语句。&lt;/p&gt; 
&lt;p&gt;使用时只需要持久层接口继承 JpaRepository 即可，泛型参数列表中第一个参数是实体类类型，第二个参数是主键类型。运行时通过 &lt;code&gt;JdkDynamicAopProxy&lt;/code&gt; 的 &lt;code&gt;invoke&lt;/code&gt; 方法创建了一个动态代理对象 &lt;code&gt;SimpleJpaRepository&lt;/code&gt;，&lt;code&gt;SimpleJpaRepository&lt;/code&gt; 中封装了 JPA 的操作，通过 &lt;code&gt;hibernate&lt;/code&gt;（封装了JDBC）完成数据库操作。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;注解&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Entity&lt;/code&gt;：表明当前类是一个实体类。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Table&lt;/code&gt; ：关联实体类和数据库表。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Column&lt;/code&gt; ：关联实体类属性和数据库表中字段。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@Id&lt;/code&gt; ：声明当前属性为数据库表主键对应的属性。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@GeneratedValue&lt;/code&gt;： 配置主键生成策略。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@OneToMany&lt;/code&gt; ：配置一对多关系，mappedBy 属性值为主表实体类在从表实体类中对应的属性名。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@ManyToOne&lt;/code&gt; ：配置多对一关系，targetEntity 属性值为主表对应实体类的字节码。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;@JoinColumn&lt;/code&gt;：配置外键关系，name 属性值为外键名称，referencedColumnName 属性值为主表主键名称。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;对象导航查询&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过 get 方法查询一个对象的同时，通过此对象可以查询它的关联对象。&lt;/p&gt; 
&lt;p&gt;对象导航查询一到多默认使用延迟加载的形式， 关联对象是集合，因此使用立即加载可能浪费资源。&lt;/p&gt; 
&lt;p&gt;对象导航查询多到一默认使用立即加载的形式， 关联对象是一个对象，因此使用立即加载。&lt;/p&gt; 
&lt;p&gt;如果要改变加载方式，在实体类注解配置加上 fetch 属性即可，LAZY 表示延迟加载，EAGER 表示立即加载。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P9：Mybatis 框架&lt;/h3&gt; 
&lt;p&gt;Mybatis 是一个实现了数据持久化的 ORM 框架，简单理解就是对 JDBC 进行了封装。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;相比 JDBC 减少了大量代码量，减少冗余代码。&lt;/p&gt; 
&lt;p&gt;使用灵活，SQL 语句写在 XML 里，从程序代码中彻底分离，降低了耦合度，便于管理。&lt;/p&gt; 
&lt;p&gt;提供 XML 标签，支持编写动态 SQL 语句。&lt;/p&gt; 
&lt;p&gt;提供映射标签，支持对象与数据库的 ORM 字段映射关系。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;SQL 语句编写工作量较大，尤其是字段和关联表多时。&lt;/p&gt; 
&lt;p&gt;SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;映射文件标签&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;select&lt;/code&gt;、&lt;code&gt;insert&lt;/code&gt;、&lt;code&gt;update&lt;/code&gt;、&lt;code&gt;delete&lt;/code&gt; 标签分别对应查询、添加、更新、删除操作。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;parameterType&lt;/code&gt; 属性表示参数的数据类型，包括基本数据类型和对应的包装类型、String 和 Java Bean 类型，当有多个参数时可以使用 &lt;code&gt;#{argn}&lt;/code&gt; 的形式表示第 n 个参数。除了基本数据类型都要以全限定类名的形式指定参数类型。&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;resultType&lt;/code&gt; 表示返回的结果类型，包括基本数据类型和对应的包装类型、String 和 Java Bean 类型。还可以使用把返回结果封装为复杂类型的 &lt;code&gt;resultMap&lt;/code&gt; 。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;缓存&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;使用缓存可以减少程序和数据库交互的次数，从而提高程序的运行效率。第一次查询后会自动将结果保存到缓存中，下一次查询时直接从缓存中返回结果无需再次查询数据库。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;一级缓存&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SqlSession 级别，默认开启且不能关闭。&lt;/p&gt; &lt;p&gt;操作数据库时需要创建 SqlSession 对象，在对象中有一个 HashMap 用于存储缓存数据，不同 SqlSession 之间缓存数据区域互不影响。&lt;/p&gt; &lt;p&gt;一级缓存的作用域是 SqlSession 范围的，在同一个 SqlSession 中执行两次相同的 SQL 语句时，第一次执行完毕会将结果保存在缓存中，第二次查询直接从缓存中获取。&lt;/p&gt; &lt;p&gt;如果 SqlSession 执行了 DML 操作（insert、update、delete），Mybatis 必须将缓存清空以保证数据的有效性。 &lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;二级缓存&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mapper 级别，默认关闭。&lt;/p&gt; &lt;p&gt;使用二级缓存时多个 SqlSession 使用同一个 Mapper 的 SQL 语句操作数据库，得到的数据会存在二级缓存区，同样使用 HashMap 进行数据存储，相比于一级缓存，二级缓存范围更大，多个 SqlSession 可以共用二级缓存，作用域是 Mapper 的同一个 namespace，不同 SqlSession 两次执行相同的 namespace 下的 SQL 语句，参数也相等，则第一次执行成功后会将数据保存在二级缓存中，第二次可直接从二级缓存中取出数据。&lt;/p&gt; &lt;p&gt;要使用二级缓存，先在在全局配置文件中配置：&lt;/p&gt; &lt;pre class=&quot;prettyprint lang-xml&quot; from-niu=&quot;default&quot;&gt;&amp;lt;!-- 开启二级缓存 --&amp;gt;
&amp;lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&amp;gt;&lt;/pre&gt; &lt;p&gt;再在对应的映射文件中配置一个 cache 标签即可。&lt;/p&gt; &lt;pre class=&quot;prettyprint lang-xml&quot; from-niu=&quot;default&quot;&gt;&amp;lt;cache/&amp;gt;&lt;/pre&gt; &lt;/li&gt;
&lt;/ul&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P10：Spring Cloud 框架&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;单体应用存在的问题&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;随着业务发展，开发越来越复杂。&lt;/p&gt; 
&lt;p&gt;修改、新增某个功能，需要对整个系统进行测试、重新部署。&lt;/p&gt; 
&lt;p&gt;一个模块出现问题，可能导致整个系统崩溃。&lt;/p&gt; 
&lt;p&gt;多个开发团队同时对数据进行管理，容易产生安全漏洞。&lt;/p&gt; 
&lt;p&gt;各个模块使用同一种技术开发，各个模块很难根据实际情况选择更合适的技术框架，局限性很大。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;分布式和集群的区别&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;集群：一台服务器无法负荷高并发的数据访问量，就设置多台服务器一起分担压力，是在物理层面解决问题。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;分布式：将一个复杂的问题拆分成若干简单的小问题，将一个大型的&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E9%A1%B9%E7%9B%AE&quot; target=&quot;_blank&quot;&gt;项目&lt;/a&gt;架构拆分成若干个微服务来协同完成，在软件设计层面解决问题。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;微服务的优点&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;各个服务的开发、测试、部署都相互独立，用户服务可以拆分为独立服务，如果用户量很大，可以很容易对其实现负载。&lt;/p&gt; 
&lt;p&gt;当新需求出现时，使用微服务不再需要考虑各方面的问题，例如兼容性、影响度等。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;使用微服务拆分&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E9%A1%B9%E7%9B%AE&quot; target=&quot;_blank&quot;&gt;项目&lt;/a&gt;后，各个服务之间消除了很多限制，只需要保证对外提供的接口正常可用，而不限制语言和框架等选择。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;服务治理 Eureka&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;服务治理的核心由三部分组成：&lt;strong&gt;服务提供者&lt;/strong&gt;、&lt;strong&gt;服务消费者&lt;/strong&gt;、&lt;strong&gt;注册中心&lt;/strong&gt;。&lt;/p&gt; 
&lt;p&gt;服务注册：在分布式系统架构中，每个微服务在启动时，将自己的信息存储在注册中心。&lt;/p&gt; 
&lt;p&gt;服务发现：服务消费者从注册中心获取服务提供者的网络信息，通过该信息调用服务。&lt;/p&gt; 
&lt;p&gt;Spring Cloud 的服务治理使用 Eureka 实现，Eureka 是 Netflix 开源的基于 REST 的服务治理解决方案，Spring Cloud 集成了 Eureka，提供服务注册和服务发现的功能，可以和基于 Spring Boot 搭建的微服务应用轻松完成整合，将 Eureka 二次封装为 Spring Cloud Eureka。&lt;strong&gt;Eureka Server&lt;/strong&gt; 是注册中心，所有要进行注册的微服务通过 Eureka Client 连接到 Eureka Server 完成注册。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;服务网关 Zuul&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Spring Cloud 集成了 Zuul 组件，实现服务网关。Zuul 是 Netflix 提供的一个开源的 API 网关服务器，是&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;和网站后端所有请求的中间层，对外开放一个 API，将所有请求导入统一的入口，屏蔽了服务端的具体实现逻辑，可以实现方向代理功能，在网关内部实现动态路由、身份认证、IP过滤、数据监控等。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;负载均衡 Ribbon&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Spring Cloud Ribbon 是一个负载均衡的解决方案，Ribbon 是 Netflix 发布的均衡负载器，Spring Cloud Ribbon是基于 Netflix Ribbon 实现的，是一个用于对 HTTP 请求进行控制的负载均衡&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;在注册中心对 Ribbon 进行注册之后，Ribbon 就可以基于某种负载均衡算***循、随机、加权轮询、加权随机等）自动帮助服务消费者调用接口，开发者也可以根据具体需求自定义 Ribbon 负载均衡&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E7%AE%97%E6%B3%95&quot; target=&quot;_blank&quot;&gt;算法&lt;/a&gt;。实际开发中 Spring Clooud Ribbon 需要结合 Spring Cloud Eureka 使用，Eureka 提供所有可以调用的服务提供者列表，Ribbon 基于特定的负载均衡&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E7%AE%97%E6%B3%95&quot; target=&quot;_blank&quot;&gt;算法&lt;/a&gt;从这些服务提供者中选择要调用的具体实例。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;声明式接口调用 Feign&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Feign 与 Ribbon 一样也是 Netflix 提供的，Feign 是一个声明式、模板化的 Web Service &lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;，简化了开发者编写 Web 服务&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;的操作，开发者可以通过简单的接口和注解来调用 HTTP API，Spring Cloud Feign 整合了 Ribbon 和 Hystrix，具有可插拔、基于注解、负载均衡、服务熔断等一系列功能。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;相比于 Ribbon + RestTemplate 的方式，Feign 可以大大简化代码开发，支持多种注解，包括 Feign 注解、JAX-RS 注解、Spring MVC 注解等。RestTemplate 是 Spring 框架提供的基于 REST 的服务组件，底层是对 HTTP 请求及响应进行了封装，提供了很多访问 REST 服务的方法，可以简化代码开发。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;服务熔断 Hystrix&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;熔断器的作用是在不改变各个微服务调用关系的前提下，针对错误情况进行预先处理。&lt;/p&gt; 
&lt;p&gt;设计原则：服务隔离机制、服务降级机制、熔断机制、提供实时监控和报警功能和提供实时配置修改功能&lt;/p&gt; 
&lt;p&gt;Hystrix 数据监控需要结合 &lt;code&gt;Spring Boot Actuator&lt;/code&gt; 使用，Actuator 提供了对服务的数据监控、数据统计，可以通过 &lt;code&gt;hystirx-stream&lt;/code&gt; 节点获取监控的请求数据，同时提供了可视化监控界面。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;服务配置 Config&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Spring Cloud Config 通过服务端可以为多个&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;提供配置服务，既可以将配置文件存储在本地，也可以将配置文件存储在远程的 Git 仓库，创建 Config Server，通过它管理所有的配置文件。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;服务跟踪 Zipkin&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Spring Cloud Zipkin 是一个可以采集并跟踪分布式系统中请求数据的组件，让开发者更直观地监控到请求在各个微服务耗费的时间，Zipkin 包括两部分 Zipkin Server 和 Zipkin Client。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h2 id=&quot;mysql-15&quot;&gt;MySQL 15&lt;/h2&gt; 
&lt;h3&gt;P1：逻辑架构&lt;/h3&gt; 
&lt;p&gt;第一层是服务器层，主要提供连接处理、授权认证、安全等功能，该层的服务不是 MySQL 独有的，大多数基于网络的 C/S 服务都有类似架构。&lt;/p&gt; 
&lt;p&gt;第二层实现了 MySQL 核心服务功能，包括查询解析、分析、优化、缓存以及日期、时间等所有内置函数，所有跨存储引擎的功能都在这一层实现，例如存储过程、触发器、视图等。&lt;/p&gt; 
&lt;p&gt;第三层是存储引擎层，存储引擎负责 MySQL 中数据的存储和提取。服务器通过API 与存储引擎通信，这些接口屏蔽了不同存储引擎的差异，使得差异对上层查询过程透明。除了会解析外键定义的 InnoDB 外，存储引擎不会解析SQL，不同存储引擎之间也不会相互通信，只是简单响应上层服务器请求。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P2：锁&lt;/h3&gt; 
&lt;p&gt;当有多个查询需要在同一时刻修改数据时就会产生并发控制的问题，MySQL 在两个层面进行并发控制：服务器层与存储引擎层。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;读写锁&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在处理并发读或写时，可以通过实现一个由两种类型组成的锁系统来解决问题。这两种类型的锁通常被称为共享锁和排它锁，也叫读锁和写锁。读锁是共享的，或者说相互不阻塞的，多个客户在同一时刻可以同时读取同一个资源而不相互干扰。写锁则是排他的，也就是说一个写锁会阻塞其他的写锁和读锁，只有如此才能确保在给定时间内只有一个用户能执行写入并防止其他用户读取正在写入的同一资源。在实际的数据库系统中，每时每刻都在发生锁定，当某个用户在修改某一部分数据时，MySQL 会通过锁定防止其他用户读取同一数据。写锁比读锁有更高的优先级，一个写锁请求可能会被插入到读锁队列的前面，但是读锁不能插入到写锁前面。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;锁策略&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一种提高共享资源并发性的方法就是让锁定对象更有选择性，尽量只锁定需要修改的部分数据而不是所有资源，更理想的方式是只对会修改的数据进行精确锁定。任何时刻在给定的资源上，锁定的数据量越少，系统的并发程度就越高，只要不发生冲突即可。&lt;/p&gt; 
&lt;p&gt;锁策略就是在锁的开销和数据安全性之间寻求平衡，这种平衡也会影响性能。大多数商业数据库系统没有提供更多选择，一般都是在表上加行锁，而 MySQL 提供了多种选择，每种MySQL存储引擎都可以实现自己的锁策略和锁粒度。MySQL最重要的两种锁策略是：&lt;/p&gt; 
 
&lt;p&gt;&lt;strong&gt;死锁&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;死锁是指两个或者多个事务在同一资源上相互占用并请求锁定对方占用的资源，从而导致恶性循环的现象。当多个事务试图以不同顺序锁定资源时就可能会产生死锁，多个事务同时锁定同一个资源时也会产生死锁。&lt;/p&gt; 
&lt;p&gt;为了解决死锁问题，数据库系统实现了各种死锁检测和死锁超时机制。越复杂的系统，例如InnoDB 存储引擎，越能检测到死锁的循环依赖，并立即返回一个错误。这种解决方式很有效，否则死锁会导致出现非常慢的查询。还有一种解决方法，就是当查询的时间达到锁等待超时的设定后放弃锁请求，这种方式通常来说不太好。InnoDB 目前处理死锁的方法是将持有最少行级排它锁的事务进行回滚。&lt;/p&gt; 
&lt;p&gt;锁的行为与顺序是和存储引擎相关的，以同样的顺序执行语句，有些存储引擎会产生死锁有些则不会。死锁的产生有双重原因：有些是真正的数据冲突，这种情况很难避免，有些则完全是由于存储引擎的实现方式导致的。&lt;/p&gt; 
&lt;p&gt;死锁发生之后，只有部分或者完全回滚其中一个事务，才能打破死锁。对于事务型系统这是无法避免的，所以应用程序在设计时必须考虑如何处理死锁。大多数情况下只需要重新执行因死锁回滚的事务即可。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;&lt;font&gt;P3：事务&lt;/font&gt;&lt;/h3&gt; 
&lt;p&gt;事务就是一组原子性的 SQL 查询，或者说一个独立的工作单元。如果数据库引擎能够成功地对数据库应用该组查询的全部语句，那么就执行该组查询。如果其中有任何一条语句因为崩溃或其他原因无法执行，那么所有的语句都不会执行。也就是说事务内的语句要么全部执行成功，要么全部执行失败。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ACID 特性&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一个运行良好的事务处理系统必须具备 ACID 特性，实现了 ACID 的数据库需要更强的CPU处理能力、更大的内存和磁盘空间。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;原子性 atomicity&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;一个事务在逻辑上是必须不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说不可能只执行其中的一部分。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;一致性 consistency&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;数据库总是从一个一致性的状态转换到另一个一致性的状态。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;隔离性 isolation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;针对并发事务而言，隔离性就是要隔离并发运行的多个事务之间的相互影响，一般来说一个事务所做的修改在最终提交以前，对其他事务是不可见的。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;持久性 durability&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;一旦事务提交成功，其修改就会永久保存到数据库中，此时即使系统崩溃，修改的数据也不会丢失。&lt;/p&gt; &lt;/li&gt;
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;隔离级别&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在 SQL 标准中定义了四种隔离级别，每一种隔离级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;未提交读 READ UNCOMMITTED&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;在该级别事务中的修改即使没有被提交，对其他事务也是可见的。事务可以读取其他事务修改完但未提交的数据，这种问题称为脏读。这个级别还会导致不可重复读和幻读，从性能上说也没有比其他级别好很多，因此很少使用。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;提交读 READ COMMITTED&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;大多数数据库系统默认的隔离级别就是提交读，但 MySQL 不是。提交读满足了隔离性的简单定义：一个事务开始时只能&quot;看见&quot;已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前的任何修改对其他事务都是不可见的。这个级别有时也叫不可重复读，因为两次执行同样的查询可能会得到不同结果。提交读存在不可重复读和幻读的问题。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;可重复读 REPEATABLE READ&lt;/strong&gt;（MySQL默认的隔离级别）&lt;/p&gt; &lt;p&gt;可重复读解决了不可重复读的问题，该级别保证了在同一个事务中多次读取同样的记录结果是一致的。但可重复读隔离级别还是无法解决幻读的问题，所谓幻读，指的是当某个事务在读取某个范围内的记录时，会产生幻行。InnoDB 存储引擎通过多版本并发控制MVCC 解决幻读的问题。&lt;/p&gt; &lt;/li&gt;
 &lt;li&gt;&lt;p&gt;&lt;strong&gt;可串行化 SERIALIZABLE&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;该级别是最高的隔离级别，通过强制事务串行执行，避免了幻读的问题。可串行化会在读取的每一行数据上都加锁，可能导致大量的超时和锁争用的问题。实际应用中很少用到这个隔离级别，只有非常需要确保数据一致性且可以接受没有并发的情况下才考虑该级别。&lt;/p&gt; &lt;/li&gt;
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;MySQL 中的事务&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MySQL 提供了两种事务型的存储引擎：InnoDB 和 NDB Cluster。&lt;/p&gt; 
&lt;p&gt;MySQL 事务默认采用自动提交模式，如果不是显式地开始一个事务，则每个查询都将被当作一个事务执行提交操作。在当前连接中，可以通过设置 AUTOCOMMIT 变量来启用或禁用自动提交模式。&lt;/p&gt; 
&lt;p&gt;1 或 ON 表示启用，0 或 OFF表示禁用，当禁用自动提交时，所有的查询都是在一个事务中，直到显式地执行 COMMIT 或 ROLLBACK 后该事务才会结束，同时又开始了一个新事务。修改 AUTOCOMMIT 对非事务型表，例如 MyISAM 或内存表不会有任何影响，对这类表来说没有 COMMIT 或 ROLLBACK 的概念，也可以理解为一直处于启用自动提交的模式&lt;/p&gt; 
&lt;p&gt;有一些命令在执行之前会强制执行提交当前的活动事务，例如&lt;code&gt;ALTER TABLE&lt;/code&gt;和&lt;code&gt;LOCK TABLES&lt;/code&gt;等。&lt;/p&gt; 
&lt;p&gt;MySQL能够识别所有的 4个 ANSI 隔离级别，InnoDB 引擎也支持所有隔离级别。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P4：MVCC 多版本并发控制&lt;/h3&gt; 
&lt;p&gt;可以认为 MVCC 是行级锁的一个变种，但它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。&lt;/p&gt; 
&lt;p&gt;MVCC 的实现，是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。&lt;/p&gt; 
&lt;p&gt;不同的存储引擎的 MVCC 实现是不同的，典型的有乐观并发控制和悲观并发控制。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;InnoDB 的 MVCC 实现&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;InnoDB 的MVCC 通过在每行记录后面保存两个隐藏的列来实现，这两个列一个保存了行的创建时间，一个保存行的过期时间间。不过存储的不是实际的时间值而是系统版本号，每开始一个新的事务系统版本号都会自动递增，事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;REPEATABLE READ 级别下 MVCC 的具体实现&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;SELECT：InnoDB 会根据以下两个条件检查每行记录：&lt;/p&gt; 
 
&lt;p&gt;INSERT ：为新插入的每一行保存当前系统版本号作为行版本号。&lt;/p&gt; 
&lt;p&gt;DELETE：为删除的每一行保存当前系统版本号作为行删除标识。&lt;/p&gt; 
&lt;p&gt;UPDATE：为插入的每一行新记录保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。&lt;/p&gt; 
&lt;p&gt;保存这两个额外系统版本号使大多数读操作都可以不用加锁。这样设计使读数据操作简单且高效，并且能保证只会读取到符合标准的行。不足之处是每行记录都需要额外存储空间，需要做更多行检查工作以及一些额外维护工作。&lt;/p&gt; 
&lt;p&gt;MVCC 只能在 &lt;code&gt;READ COMMITTED&lt;/code&gt; 和 &lt;code&gt;REPEATABLE READ&lt;/code&gt; 两个隔离级别下工作，因为 &lt;code&gt;READ UNCOMMITTED&lt;/code&gt; 总是读取最新的数据行，而不是符合当前事务版本的数据行，而 &lt;code&gt;SERIALIZABLE&lt;/code&gt; 则会对所有读取的行都加锁。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P5：InnoDB 存储引擎&lt;/h3&gt; 
&lt;p&gt;InnoDB 是 MySQL 的默认事务型引擎，它被设计用来处理大量的短期事务。InnoDB 的性能和自动崩溃恢复特性，使得它在非事务型存储需求中也很流行，除非有特别原因否则应该优先考虑 InnoDB 引擎。&lt;/p&gt; 
&lt;p&gt;InnoDB 的数据存储在表空间中，表空间由一系列数据文件组成。MySQL4.1 后 InnoDB 可以将每个表的数据和索引放在单独的文件中。&lt;/p&gt; 
&lt;p&gt;InnoDB 采用 MVCC 来支持高并发，并且实现了四个标准的隔离级别。其默认级别是 &lt;code&gt;REPEATABLE READ&lt;/code&gt;，并且通过间隙锁策略防止幻读，间隙锁使 InnoDB 不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定防止幻行的插入。&lt;/p&gt; 
&lt;p&gt;InnoDB 表是基于聚簇索引建立的，InnoDB 的索引结构和其他存储引擎有很大不同，聚簇索引对主键查询有很高的性能，不过它的二级索引中必须包含主键列，所以如果主键很大的话其他所有索引都会很大，因此如果表上索引较多的话主键应当尽可能小。&lt;/p&gt; 
&lt;p&gt;InnoDB 的存储格式是平***立的，可以将数据和索引文件从一个平台复制到另一个平台。&lt;/p&gt; 
&lt;p&gt;InnoDB 内部做了很多优化，包括从磁盘读取数据时采用的可预测性预读，能够自动在内存中创建加速读操作的自适应哈希索引，以及能够加速插入操作的插入缓冲区等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;选择合适的存储引擎&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MySQL5.5 将 InnoDB 作为默认存储引擎，除非需要用到某些 InnoDB 不具备的特性，并且没有其他方法可以代替，否则都应该优先选用InnoDB。&lt;/p&gt; 
&lt;p&gt;如果应用需要事务支持，那么 InnoDB 是目前最稳定并且经过验证的选择。如果不需要事务并且主要是 SELECT 和 INSERT 操作，那么MyISAM 是不错的选择。相对而言，MyISAM 崩溃后发生损坏的概率要比 InnoDB 大很多而且恢复速度也要慢，因此即使不需要事务支持，也可以选择InnoDB。&lt;/p&gt; 
&lt;p&gt;如果可以定期地关闭服务器来执行备份，那么备份的因素可以忽略。反之如果需要在线热备份，那么 InnoDB 就是基本的要求。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P6：MyISAM 存储引擎&lt;/h3&gt; 
&lt;p&gt;在 MySQL5.1及之前，MyISAM 是默认的存储引擎，MyISAM 提供了大量的特性，包括全文索引、压缩、空间函数等，但不支持事务和行锁，最大的缺陷就是崩溃后无法安全恢复。对于只读的数据或者表比较小、可以忍受修复操作的情况仍然可以使用 MyISAM。&lt;/p&gt; 
&lt;p&gt;MyISAM 将表存储在数据文件和索引文件中，分别以 &lt;code&gt;.MYD&lt;/code&gt; 和 &lt;code&gt;.MYI&lt;/code&gt; 作为扩展名。MyISAM 表可以包含动态或者静态行，MySQL 会根据表的定义决定行格式。MyISAM 表可以存储的行记录数一般受限于可用磁盘空间或者操作系统中单个文件的最大尺寸。&lt;/p&gt; 
&lt;p&gt;MyISAM 对整张表进行加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但是在表有读取查询的同时，也支持并发往表中插入新的记录。&lt;/p&gt; 
&lt;p&gt;对于MyISAM 表，MySQL 可以手动或自动执行检查和修复操作，这里的修复和事务恢复以及崩溃恢复的概念不同。执行表的修复可能导致一些数据丢失，而且修复操作很慢。&lt;/p&gt; 
&lt;p&gt;对于 MyISAM 表，即使是 BLOB 和 TEXT 等长字段，也可以基于其前 500 个字符创建索引。MyISAM 也支持全文索引，这是一种基于分词创建的索引，可以支持复杂的查询。&lt;/p&gt; 
&lt;p&gt;创建 MyISAM 表时如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时不会立刻将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理缓冲区或关闭表的时候才会将对应的索引库写入磁盘。这种方式可以极大提升写性能，但在数据库或主机崩溃时会造成索引损坏，需要执行修复。延迟更新索引键的特性可以在全局设置也可以单个表设置。&lt;/p&gt; 
&lt;p&gt;MyISAM 设计简单，数据以紧密格式存储，所以在某些场景下性能很好。MyISAM 最典型的性能问题还是表锁问题，如果所有的查询长期处于 Locked 状态，那么原因毫无疑问就是表锁。&lt;/p&gt; 
&lt;h3&gt;P7：Memory 存储引擎&lt;/h3&gt; 
&lt;p&gt;如果需要快速访问数据，并且这些数据不会被修改，重启以后丢失也没有关系，那么使用 Memory 表是非常有用的。Memory 表至少要比 MyISAM 表快一个数量级，因为所有的数据都保存在内存中，不需要进行磁盘 IO，Memory 表的结构在重启以后还会保留，但数据会丢失。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;Memory 表适合的场景：查找或者映射表、缓存周期性聚合数据的结果、保存&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&quot; target=&quot;_blank&quot;&gt;数据分析&lt;/a&gt;中产生的中间数据。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;Memory 表支持哈希索引，因此查找速度极快。虽然速度很快但还是无法取代传统的基于磁盘的表，Memory 表使用表级锁，因此并发写入的性能较低。它不支持 BLOB 和 TEXT 类型的列，并且每行的长度是固定的，所以即使指定了 VARCHAR 列，实际存储时也会转换成CHAR，这可能导致部分内存的浪费。&lt;/p&gt; 
&lt;p&gt;如果 MySQL 在执行查询的过程中需要使用临时表来保持中间结果，内部使用的临时表就是 Memory 表。如果中间结果太大超出了Memory 表的限制，或者含有 BLOB 或 TEXT 字段，临时表会转换成 MyISAM 表。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P8：数据类型&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;整数类型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果存储整数可以使用这几种整数类型：TINYINT、SMALLINT、MEDIUMINT、INT，BIGINT，它们分别使用8、16、24、32、64 位存储空间。&lt;/p&gt; 
&lt;p&gt;整数类型有可选的 UNSIGNED 属性，表示不允许负值，可以使整数的上限提高一倍。有符号和无符号类型使用相同的存储空间并具有相同的性能，可以根据实际情况选择合适的类型。&lt;/p&gt; 
&lt;p&gt;MySQL 可以为整数类型指定宽度，例如 INT(11)，这对大多数应用没有意义，不会限制值的范围，只是规定了 MySQL 的交互工具显示字符的个数，对于存储和计算来说 INT(1) 和 INT(11) 是相同的。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;实数类型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;实数是带有小数部分的数字，但它们不只是为了存储小数，也可以使用 DECIMAL 存储比 BIGINT 还大的整数。MySQL既支持精确类型，也支持不精确类型。&lt;/p&gt; 
&lt;p&gt;FLOAT 和 DOUBLE 支持使用标准的浮点运算进行近似运算，DECIMAL 用于存储精确的小数。&lt;/p&gt; 
&lt;p&gt;浮点类型在存储同样范围的值时，通常比 DECIMAL 使用更少的空间。FLOAT 使用 4 字节存储，DOUBLE 占用8字节，MySQL 内部使用DOUBLE 作为内部浮点计算的类型。&lt;/p&gt; 
&lt;p&gt;因为需要额外空间和计算开销，所以应当尽量只在对小数进行精确计算时才使用 DECIMAL。在数据量较大时可以考虑 BIGINT 代替DECIMAL，将需要存储的货币单位根据小数的位数乘以相应的倍数即可。假设要存储的数据精确到万分之一分，则可以把所有金额乘以一百万将结果存储在 BIGINT 中，这样可以同时避免浮点存储计算不精确和 DECIMAL 精确计算代价高的问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;VARCHAR&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;VARCHAR 用于存储可变字符串，是最常见的字符串数据类型。它比定长字符串更节省空间，因为它仅使用必要的空间。VARCHAR 需要 1或 2 个额外字节记录字符串长度，如果列的最大长度不大于 255 字节则只需要1 字节。VARCHAR 不会删除末尾空格。&lt;/p&gt; 
&lt;p&gt;VARCHAR 节省了存储空间，但由于行是变长的，在 UPDATE 时可能使行变得比原来更长，这就导致需要做额外的工作。如果一个行占用的空间增长并且页内没有更多的空间可以存储，这种情况下不同存储引擎处理不同，InnoDB 会分裂页而 MyISAM 会将行拆分成不同片。&lt;/p&gt; 
&lt;p&gt;适用场景：字符串列的最大长度比平均长度大很多、列的更新很少、使用了 UTF8 这种复杂字符集，每个字符都使用不同的字节数存储。&lt;/p&gt; 
&lt;p&gt;InnoDB 可以把过长的 VARCHAR 存储为 BLOB。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CHAR&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;CHAR 是定长的，根据定义的字符串长度分配足够的空间。CHAR 会删除末尾空格。&lt;/p&gt; 
&lt;p&gt;CHAR 适合存储很短的字符串，或所有值都接近同一个长度，例如存储密码的 MD5 值。对于经常变更的数据，CHAR 也比 VARCHAR更好，因为定长的 CHAR 不容易产生碎片。对于非常短的列，CHAR 在存储空间上也更有效率，例如用 CHAR 来存储只有 Y 和 N 的值只需要一个字节，但是 VARCHAR 需要两个字节，因为还有一个记录长度的额外字节。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;BLOB 和 TEXT 类型&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;BLOB 和TEXT 都是为了存储大数据而设计的字符串数据类型，分别采用二进制和字符串方式存储。MySQL会把每个 BLOB 和 TEXT 值当作一个独立的对象处理，存储引擎在存储时通常会做特殊处理。当值太大时，InnoDB 会使用专门的外部存储区来进行存储。BLOB 和TEXT 仅有的不同是 BLOB 存储的是二进制数据，没有&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;规则或字符集，而 TEXT 有字符集和&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;规则。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;MySQL 对 BLOB 和TEXT 列进行&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;与其他类型不同：它只对每个列最前 &lt;/span&gt;&lt;code&gt;max_sort_length&lt;/code&gt;&lt;span&gt; 字节而不是整个字符串做&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;，如果只需要&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;前面一小部分字符，则可以减小 &lt;/span&gt;&lt;code&gt;max_sort_length&lt;/code&gt;&lt;span&gt; 的配置。MySQL 不能将 BLOB 和 TEXT 列全部长度的字符串进行索引，也不能使用这些索引消除&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DATETIME&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;这个类型能保存大范围的值，从 1001 年到 9999 年，精度为秒。它把日期和时间封装到了一个整数中，与时区无关，使用 8 字节的存储空间。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TIMESTAMP&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;它和 UNIX 时间戳相同。TIMESTAMP 只使用 4 字节的存储空间，因此它的范围比DATETIME 小得多，只能表示1970年到2038年，并且依赖于时区。通常应该选择 TIMESTAMP，因为它比 DATETIME 空间效率更高。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P9：索引的分类&lt;/h3&gt; 
&lt;p&gt;索引在也叫做键，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能很关键，尤其是当表中数据量越来越大时，索引对性能的影响愈发重要。在数据量较小且负载较低时，不恰当的索引对性能的影响可能还不明显，但数据量逐渐增大时，性能会急剧下降。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;索引大大减少了服务器需要扫描的数据量、可以帮助服务器避免&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;和临时表、可以将随机 IO 变成顺序 IO。但索引并不总是最好的工具，对于非常小的表，大部分情况下会采用全表扫描。对于中到大型的表，索引就非常有效。但对于特大型的表，建立和使用索引的代价也随之增长，这种情况下应该使用分区技术。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;在MySQL中，首先在索引中找到对应的值，然后根据匹配的索引记录找到对应的数据行。索引可以包括一个或多个列的值，如果索引包含多个列，那么列的顺序也十分重要，因为 MySQL 只能高效地使用索引的最左前缀列。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;B-Tree 索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;大多数 MySQL 引擎都支持这种索引，使用术语 B-Tree 是因为 MySQL 在 CREATE TABLE 和其他语句中也使用该关键字。不过底层的存储引擎可能使用不同的存储结构，例如 NDB 集群实际使用 T-Tree，而 InnoDB 则使用 B+Tree。&lt;/p&gt; 
&lt;p&gt;存储引擎以不同方式使用 B-Tree 索引，性能也不同。例如 MyISAM 使用前缀压缩技术使得索引更小，但 InnoDB 则按照原数据格式进行存储。再例如 MyISAM 索引通过数据的物理位置引用被索引的行，而 InnoDB 则根据主键引用被索引的行。&lt;/p&gt; 
&lt;p&gt;B-Tree 通常意味着所有的值都是按顺序存储的，并且每个叶子页到根的距离相同。B-Tree 索引能够加快访问数据的速度，因为存储引擎不再需要进行全表扫描来获取需要的数据，取而代之的是从索引的根节点开始进行搜索。根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这些指针实际上定义了子节点页中值的上限和下限。最终存储引擎要么找到对应的值，要么该记录不存在。叶子节点的指针指向的是被索引的数据，而不是其他的节点页。&lt;/p&gt; 
&lt;p&gt;B-Tree索引适用于全键值、键值范围或键前缀查找，其中键前缀查找只适用于最左前缀查找。索引对如下类型的查询有效：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;全值匹配：全值匹配指的是和索引中的所有列进行匹配。 &lt;/li&gt;
 &lt;li&gt;匹配最左前缀：只使用索引的第一列。 &lt;/li&gt;
 &lt;li&gt;匹配列前缀：只匹配某一列的值的开头部分。 &lt;/li&gt;
 &lt;li&gt;匹配范围值：查找某两个值之间的范围。 &lt;/li&gt;
 &lt;li&gt;精确匹配某一列并范围匹配另一列：有一列全匹配而另一列范围匹配。 &lt;/li&gt;
 &lt;li&gt;只访问索引的查询：B-Tree 通常可以支持只访问索引的查询，即查询只需要访问索引而无需访问数据行。 &lt;/li&gt;
&lt;/ul&gt; 
&lt;p&gt;&lt;span&gt;因为索引树中的节点有序，所以除了按值查找之外索引还可以用于查询中的 ORDER BY 操作。一般如果 B-Tree 可以按照某种方式查找到值，那么也可以按照这种方式&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;B-Tree索引的限制：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果不是按照索引的最左列开始查找，则无法使用索引。 &lt;/li&gt;
 &lt;li&gt;不能跳过索引中的列，例如索引为 (id,name,sex)，不能只使用 id 和 sex 而跳过 name。 &lt;/li&gt;
 &lt;li&gt;如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引。 &lt;/li&gt;
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;哈希索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;哈希索引基于&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%93%88%E5%B8%8C%E8%A1%A8&quot; target=&quot;_blank&quot;&gt;哈希表&lt;/a&gt;实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值的行计算出的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%93%88%E5%B8%8C%E8%A1%A8&quot; target=&quot;_blank&quot;&gt;哈希表&lt;/a&gt;中保存指向每个数据行的指针。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;只有 Memory 引擎显式支持哈希索引，这也是 Memory 引擎的默认索引类型。&lt;/p&gt; 
&lt;p&gt;因为索引自身只需存储对应的哈希值，所以索引的结构十分紧凑，这让哈希索引的速度非常快，但它也有一些限制：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;哈希索引只包含哈希值和行指针而不存储字段值，所以不能使用索引中的值来避免读取行。 &lt;/li&gt;
 &lt;li&gt;&lt;span&gt;哈希索引数据并不是按照索引值顺序存储的，因此无法用于&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;。 &lt;/span&gt;&lt;/li&gt;
 &lt;li&gt;哈希索引不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。例如在数据列(a,b)上建立哈希索引，如果查询的列只有a就无法使用该索引。 &lt;/li&gt;
 &lt;li&gt;哈希索引只支持等值比较查询，不支持任何范围查询。 &lt;/li&gt;
 &lt;li&gt;&lt;span&gt;访问哈希索引的数据非常快，除非有很多哈希冲突。当出现哈希冲突时，存储引擎必须遍历&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E9%93%BE%E8%A1%A8&quot; target=&quot;_blank&quot;&gt;链表&lt;/a&gt;中所有的行指针，逐行进行比较直到找到所有符合条件的行。 &lt;/span&gt;&lt;/li&gt;
 &lt;li&gt;如果哈希冲突很高的话，索引维护的代价也会很高。 &lt;/li&gt;
&lt;/ul&gt; 
&lt;p&gt;自适应哈希索引是 InnoDB 引擎的一个特殊功能，当它注意到某些索引值被使用的非常频繁时，会在内存中基于 B-Tree 索引之上再创键一个哈希索引，这样就让 B-Tree 索引也具有哈希索引的一些优点，比如快速哈希查找。这是一个完全自动的内部行为，用户无法控制或配置，但如果有必要可以关闭该功能。&lt;/p&gt; 
&lt;p&gt;如果存储引擎不支持哈希索引，可以创建自定义哈希索引，在 B-Tree基础 上创建一个伪哈希索引，它使用哈希值而不是键本身进行索引查找，需要在查询的 WHERE 子句中手动指定哈希函数。当数据表非常大时，CRC32 会出现大量的哈希冲突，可以考虑自己实现 64 位哈希函数，或者使用 MD5 函数返回值的一部分作为自定义哈希函数。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;空间索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MyISAM 表支持空间索引，可以用作地理数据存储。和 B-Tree 索引不同，这类索引无需前缀查询。空间索引会从所有维度来索引数据，查询时可以有效地使用任意维度来组合查询。必须使用 MySQL 的 GIS 即地理信息系统的相关函数来维护数据，但 MySQL 对 GIS 的支持并不完善，因此大部分人都不会使用这个特性。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;全文索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通过数值比较、范围过滤等就可以完成绝大多数需要的查询，但如果希望通过关键字的匹配进行查询过滤，那么就需要基于相似度的查询，而不是精确的数值比较，全文索引就是为这种场景设计的。全文索引有自己独特的语法，没有索引也可以工作，如果有索引效率会更高。&lt;/p&gt; 
&lt;p&gt;全文索引可以支持各种字符内容的搜索，包括 CHAR、VARCHAR 和 TEXT 类型，也支持自然语言搜索和布尔搜索。在 MySQL 中全文索引有很多限制，例如表锁对性能的影响、数据文件的崩溃恢复等，这使得 MyISAM 的全文索引对很多应用场景并不合适。MyISAM 的全文索引作用对象是一个&quot;全文集合&quot;，可能是某个数据表的一列，也可能是多个列。具体的对某一条记录，MySQL 会将需要索引的列全部拼接成一个字符串然后进行索引。&lt;/p&gt; 
&lt;p&gt;MyISAM 的全文索引是一种特殊的 B-Tree 索引，一共有两层。第一层是所有关键字，然后对于每一个关键字的第二层，包含的是一组相关的&quot;文档指针&quot;。全文索引不会索引文档对象中的所有词语，它会根据规则过滤掉一些词语，例如停用词列表中的词都不会被索引。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;聚簇索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。InnoDB 的聚簇索引实际上在同一个结构中保存了 B-Tree 索引和数据行。当表有聚餐索引时，它的行数据实际上存放在索引的叶子页中，因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。&lt;/p&gt; 
&lt;p&gt;优点：① 可以把相关数据保存在一起，例如实现电子邮箱时可以根据用户 ID 聚集数据，这样只需要从磁盘读取少数数据页就能获取某个用户的全部邮件，如果没有使用聚簇索引，每封邮件可能都导致一次磁盘 IO。② 数据访问更快，聚簇索引将索引和数据保存在同一个 B-Tree 中，因此获取数据比非聚簇索引要更快。③ 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。&lt;/p&gt; 
&lt;p&gt;缺点：① 聚簇索引最大限度提高了 IO 密集型应用的性能，如果数据全部在内存中将会失去优势。② 插入速度验证依赖于插入顺序，按照主键的顺序插入是加载数据到 InnoDB 引擎最快的方式。③ 更新聚簇索引列的代价很高，因为会强制每个被更新的行移动到新位置。④ 基于聚簇索引的表插入新行或主键被更新导致行移动时，可能导致页分裂，表会占用更多磁盘空间。④ 当行稀疏或由于页分裂导致数据存储不连续时，全表扫描可能很慢。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;覆盖索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;覆盖索引指一个索引包含或覆盖了所有需要查询的字段的值，不再需要根据索引回表查询数据。覆盖索引必须要存储索引列的值，因此 MySQL 只能使用 B-Tree 索引做覆盖索引。&lt;/p&gt; 
&lt;p&gt;优点：① 索引条目通常远小于数据行大小，可以极大减少数据访问量。② 因为索引按照列值顺序存储，所以对于 IO 密集型防伪查询回避随机从磁盘读取每一行数据的 IO 少得多。③ 由于 InnoDB 使用聚簇索引，覆盖索引对 InnoDB 很有帮助。InnoDB 的二级索引在叶子节点保存了行的主键值，如果二级主键能覆盖查询那么可以避免对主键索引的二次查询。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;&lt;font&gt;P10：索引使用原则&lt;/font&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;建立索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;对查询频次较高，且数据量比较大的表建立索引。索引字段的选择，最佳候选列应当从 WHERE 子句的条件中提取，如果 WHERE 子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;使用前缀索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;索引列开始的部分字符，索引创建后也是使用硬盘来存储的，因此短索引可以提升索引访问的 IO 效率。对于 BLOB、TEXT 或很长的 VARCHAR 列必须使用前缀索引，MySQL 不允许索引这些列的完整长度。前缀索引是一种能使索引更小更快的有效方法，但缺点是 MySQL 无法使用前缀索引做 ORDER BY 和 GROUP BY，也无法使用前缀索引做覆盖扫描。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;选择合适的索引顺序&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;当不需要考虑&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;和分组时，将选择性最高的列放在前面。索引的选择性是指不重复的索引值和数据表的记录总数之比，索引的选择性越高则查询效率越高，唯一索引的选择性是 1，因此也可以使用唯一索引提升查询效率。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;删除无用索引&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MySQL 允许在相同列上创建多个索引，重复的索引需要单独维护，并且优化器在优化查询时也需要逐个考虑，这会影响性能。重复索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应该避免创建重复索引。如果创建了索引 (A,B) 再创建索引 (A) 就是冗余索引，因为这只是前一个索引的前缀索引，对于 B-Tree 索引来说是冗余的。解决重复索引和冗余索引的方法就是删除这些索引。除了重复索引和冗余索引，可能还会有一些服务器永远不用的索引，也应该考虑删除。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;减少碎片&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;B-Tree 索引可能会碎片化，碎片化的索引可能会以很差或无序的方式存储在磁盘上，这会降低查询的效率。表的数据存储也可能碎片化，包括行碎片、行间碎片、剩余空间碎片，对于 MyISAM 这三类碎片化都有可能发生，对于 InnoDB 不会出现短小的行碎片，它会移动短小的行重写到一个片段中。可以通过执行 OPTIMIZE TABLE 或者导出再导入的方式重新整理数据，对于 MyISAM 可以通过&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;重建索引消除碎片。InnoDB 可以通过先删除再重新创建索引的方式消除索引碎片。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;索引失效情况&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果索引列出现了隐式类型转换，则 MySQL 不会使用索引。常见的情况是在 SQL 的 WHERE 条件中字段类型为字符串，其值为数值，如果没有加引号那么 MySQL 不会使用索引。&lt;/p&gt; 
&lt;p&gt;如果 WHERE 条件中含有 OR，除非 OR 前使用了索引列而 OR 之后是非索引列，索引会失效。&lt;/p&gt; 
&lt;p&gt;MySQL 不能在索引中执行 LIKE 操作，这是底层存储引擎 API 的限制，最左匹配的 LIKE 比较会被转换为简单的比较操作，但如果是以通配符开头的 LIKE 查询，存储引擎就无法做笔记。这种情况下 MySQL 服务器只能提取数据行的值而不是索引值来做比较。&lt;/p&gt; 
&lt;p&gt;如果查询中的列不是独立的，则 MySQL 不会使用索引。独立的列是指索引列不能是表达式的一部分，也不能是函数的参数。&lt;/p&gt; 
&lt;p&gt;对于多个范围条件查询，MySQL 无法使用第一个范围列后面的其他索引列，对于多个等值查询则没有这种限制。&lt;/p&gt; 
&lt;p&gt;如果 MySQL 判断全表扫描比使用索引查询更快，则不会使用索引。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P11：优化数据类型&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;更小的通常更好&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;一般情况下尽量使用可以正确存储数据的最小数据类型，更小的数据类型通常也更快，因为它们占用更少的磁盘、内存和 CPU 缓存。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;尽可能简单&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;简单数据类型的操作通常需要更少的 CPU 周期，例如整数比字符操作代价更低，因为字符集和校对规则使字符相比整形更复杂。应该使用 MySQL 的内建类型 date、time 和 datetime 而不是字符串来存储日期和时间，另一点是应该使用整形存储 IP 地址。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;尽量避免 NULL&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;通常情况下最好指定列为 NOT NULL，除非需要存储 NULL值。因为如果查询中包含可为 NULL 的列对 MySQL 来说更难优化，可为 NULL 的列使索引、索引统计和值比较都更复杂，并且会使用更多存储空间。当可为 NULL 的列被索引时，每个索引记录需要一个额外字节，在MyISAM 中还可能导致固定大小的索引变成可变大小的索引。&lt;/p&gt; 
&lt;p&gt;通常把可为 NULL 的列设置为 NOT NULL 带来的性能提升较小，因此调优时没必要首先查找并修改这种情况。但如果计划在列上建索引，就应该尽量避免设计成可为 NULL 的列。&lt;/p&gt; 
&lt;p&gt;在为列选择数据类型时，第一步需要确定合适的大类型：数字、字符串、时间等。下一步是选择具体类型，很多 MySQL 数据类型可以存储相同类型的数据，只是存储的长度和范围不一样，允许的精度不同或需要的物理空间不同。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P12：优化查询概述&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;优化数据访问&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果把查询看作一个任务，那么它由一系列子任务组成，每个子任务都会消耗一定时间。如果要优化查询，要么消除一些子任务，要么减少子任务的执行次数。查询性能低下最基本的原因是访问的数据太多，大部分性能低下的查询都可以通过减少访问的数据量进行优化。可以通过以下两个步骤分析。&lt;/p&gt; 
&lt;p&gt;是否向数据库请求了不需要的数据：有些查询会请求超过实际需要的数据，然后这些多余的数据会被应用程序丢弃，这会给 MySQL 服务器造成额外负担并增加网络开销，另外也会消耗应用服务器的 CPU 和内存资源。例如多表关联时返回全部列，取出全部列会让优化器无法完成索引覆盖扫描这类优化，还会为服务器带来额外的 IO、内存和 CPU 的消耗，因此使用 SELECT * 时需要仔细考虑是否真的需要返回全部列。再例如总是重复查询相同的数据，比较好的解决方案是初次查询时将数据缓存起来，需要的时候从缓存中取出。&lt;/p&gt; 
&lt;p&gt;MySQL 是否在扫描额外的记录：在确定查询只返回需要的数据后，应该看看查询为了返回结果是否扫描了过多的数据，最简单的三个衡量指标时响应时间、扫描的行数和返回的行数。如果发现查询需要扫描大量数据但只返回少数的行，可以使用以下手动优化：① 使用覆盖索引扫描，把所有需要用的列都放到索引中，这样存储引擎无需回表查询对应行就可以返回结果。② 改变库表结构。 ③ 重写这个复杂的查询，让 MySQL 优化器能够以更优化的方式执行这个查询。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;重构查询方式&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在优化有问题的查询时，目标应该是找到一个更优的方法获取实际需要的结果，而不一定总是需要从 MySQL 获取一模一样的结果集。&lt;/p&gt; 
&lt;p&gt;切分查询：有时候对于一个大查询可以将其切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分查询结果。例如删除旧数据，定期清除大量数据时，如果用一个大的语句一次性完成的话可能需要一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。将一个大的 DELETE 语句切分成多个较小的查询可以尽可能小地影响 MySQL 的性能，同时还可以减少MySQL 复制的延迟。&lt;/p&gt; 
&lt;p&gt;分解关联查询：很多高性能应用都会对关联查询进行分解，可以对每一个表进行单表查询，然后将结果在应用程序中进行关联。分解关联查询可以让缓存的效率更高、减少锁的竞争、提升查询效率、还可以减少冗余记录的查询。&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P13：查询执行流程&lt;/h3&gt; 
&lt;p&gt;&lt;span&gt;简单来说分为五步：① &lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;发送一条查询给服务器。② 服务器先检查查询缓存，如果命中了缓存则立刻返回存储在缓存中的结果，否则进入下一阶段。③ 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划。④ MySQL 根据优化器生成的执行计划，调用存储引擎的 API 来执行查询。⑤ 将结果返回给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;。 &lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;查询缓存&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;在解析一个查询语句之前，如果查询缓存是打开的，那么 MySQL 会优先检查这个查询是否命中查询缓存中的数据。这个检查是通过一个对大小写敏感的哈希查找实现的。查询和缓存中的查询即使只有一个字节不同，也不会匹配缓存结果，这种情况下会进行下一个阶段的处理。如果当前的查询恰好命中了查询缓存，那么在返回查询结果之前 MySQL 会检查一次用户权限。如果权限没有问题，MySQL 会跳过其他阶段，直接从缓冲中拿到结果并返回给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;，这种情况下查询不会被解析，不用生成执行计划，不会被执行。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;查询优化处理&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;该阶段包括多个子阶段：解析 SQL、预处理、优化 SQL 执行计划。首先 MySQL 通过关键字将 SQL 语句进行解析，并生成一颗对应的解析树，MySQL 解析器将使用 MySQL 语法规则验证和解析查询。例如它将验证是否使用了错误的关键字，或者使用关键字的顺序是否正确等。预处理器则根据一些 MySQL 规则进一步检查解析树是否合法，例如检查数据表和数据列是否存在，还会解析名字和别名看它们是否有歧义。下一步预处理器会验证权限，这一步通常很快，除非服务器上有非常多的权限配置。&lt;/p&gt; 
&lt;p&gt;语法树被认为合法后，查询优化器将其转成执行计划。一条查询可以有多种查询方式，最后都返回相同的结果，优化器的作用就是找到这其中最好的执行计划。MySQL 使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。优化策略可以简单分为两种，一种是静态优化，可以直接对解析树分析并完成优化，不依赖于特别的数值，可以认为是一种编译时优化。另一种是动态优化，和查询的上下文有关，每次查询时都需要重新评估。&lt;/p&gt; 
&lt;p&gt;MySQL 可以处理的优化类型包括：重新定义表的关联顺序、将外连接转化成内连接、使用等价变换规则、优化 COUNT() 和 MIN() 以及 MAX() 函数、预估并转为常数表达式、覆盖索引扫描、子查询优化等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;查询执行引擎&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在解析和优化阶段，MySQL 将生成查询对应的执行计划，MySQL 的查询执行引擎则根据这个计划来完成整个查询。执行计划是一个数据结构，而不是其他关系型数据库那样会生成对应的字节码。查询执行阶段并不复杂，MySQL 只是简单的根据执行计划给出的指令逐步执行，再根据执行计划执行的过程中，有大量操作需要通过调用存储引擎实现的接口来完成。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;span&gt;返回结果给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;查询执行的最后一个阶段是将结果返回给&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;，即使查询不需要返回结果集，MySQL 仍然会返回这个查询的一些信息，如该查询影响到的行数。如果查询可以被缓存，那么 MySQL 会在这个阶段将结果存放到查询缓冲中。MySQL 将结果集返回&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;是一个增量、逐步返回的过程，这样做的好处是服务器无需存储太多的结果，减少内存消耗，也可以让&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;第一时间获得响应结果。结果集中的每一行给都会以一个满足 MySQL &lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;/服务器通信协议的包发送，再通过 TCP 协议进行传输，在 TCP 传输过程中可能对包进行缓存然后批量传输。&lt;/span&gt;&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;&lt;font&gt;P14：优化 SQL&lt;/font&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;优化 COUNT 查询&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;COUNT 是一个特殊的函数，它可以统计某个列值的数量，在统计列值时要求列值是非空的，不会统计 NULL 值。如果在 COUNT 中指定了列或列的表达式，则统计的就是这个表达式有值的结果数，而不是 NULL。&lt;/p&gt; 
&lt;p&gt;COUNT 的另一个作用是统计结果集的行数，当 MySQL 确定括号内的表达式不可能为 NULL 时，实际上就是在统计行数。当使用 COUNT(&lt;em&gt;) 时，\&lt;/em&gt; 不会扩展成所有列，它会忽略所有的列而直接统计所有的行数。&lt;/p&gt; 
&lt;p&gt;某些业务场景并不要求完全精确的 COUNT 值，此时可以使用近似值来代替，EXPLAIN 出来的优化器估算的行数就是一个不错的近似值，因为执行 EXPLAIN 并不需要真正地执行查询。&lt;/p&gt; 
&lt;p&gt;通常来说 COUNT 都需要扫描大量的行才能获取精确的结果，因此很难优化。在 MySQL 层还能做的就只有覆盖扫描了，如果还不够就需要修改应用的架构，可以增加汇总表或者外部缓存系统。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化关联查询&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;确保 ON 或 USING 子句中的列上有索引，在创建索引时就要考虑到关联的顺序。&lt;/p&gt; 
&lt;p&gt;确保任何 GROUP BY 和 ORDER BY 的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化这个过程。&lt;/p&gt; 
&lt;p&gt;在 MySQL 5.5 及以下版本尽量避免子查询，可以用关联查询代替，因为执行器会先执行外部的 SQL 再执行内部的 SQL。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化 GROUP BY&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;如果没有通过 ORDER BY 子句显式指定要&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;的列，当查询使用 GROUP BY 子句的时候，结果***自动按照分组的字段进行&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;，如果不关心结果集的顺序，可以使用 ORDER BY NULL 禁止&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化 LIMIT 分页&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在偏移量非常大的时候，需要查询很多条数据再舍弃，这样的代价非常高。要优化这种查询，要么是在页面中限制分页的数量，要么是优化大偏移量的性能。最简单的办法是尽可能地使用覆盖索引扫描，而不是查询所有的列，然后根据需要做一次关联操作再返回所需的列。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;还有一种方法是从上一次取数据的位置开始扫描，这样就可以避免使用 OFFSET。其他优化方法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表只包含主键列和需要做&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E6%8E%92%E5%BA%8F&quot; target=&quot;_blank&quot;&gt;排序&lt;/a&gt;的数据列。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化 UNION 查询&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;MySQL 通过创建并填充临时表的方式来执行 UNION 查询，除非确实需要服务器消除重复的行，否则一定要使用 UNION ALL，如果没有 ALL 关键字，MySQL 会给临时表加上 DISTINCT 选项，这会导致对整个临时表的数据做唯一性检查，这样做的代价非常高。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;使用用户自定义变量&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;在查询中混合使用过程化和关系化逻辑的时候，自定义变量可能会非常有用。用户自定义变量是一个用来存储内容的临时容器，在连接 MySQL 的整个过程中都存在，可以在任何可以使用表达式的地方使用自定义变量。例如可以使用变量来避免重复查询刚刚更新过的数据、统计更新和插入的数量等。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;优化 INSERT&lt;/strong&gt; &lt;/p&gt; 
&lt;p&gt;&lt;span&gt;需要对一张表插入很多行数据时，应该尽量使用一次性插入多个值的 INSERT 语句，这种方式将缩减&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;与数据库之间的连接、关闭等消耗，效率比多条插入单个值的 INSERT 语句高。也可以关闭事务的自动提交，在插入完数据后提交。当插入的数据是按主键的顺序插入时，效率更高。&lt;/span&gt;&lt;/p&gt; 
&lt;hr/&gt; 
&lt;h3&gt;P15：复制&lt;/h3&gt; 
&lt;p&gt;复制解决的基本问题是让一台服务器的数据与其他服务器保持同步，一台主库的数据可以同步到多台备库上，备库本身也可以被配置成另外一台服务器的主库。主库和备库之间可以有多种不同的组合方式。&lt;/p&gt; 
&lt;p&gt;MySQL 支持两种复制方式：基于行的复制和基于语句的复制，基于语句的复制也称为逻辑复制，从 MySQL 3.23 版本就已存在，基于行的复制方式在 5.1 版本才被加进来。这两种方式都是通过在主库上记录二进制日志、在备库重放日志的方式来实现异步的数据复制。因此同一时刻备库的数据可能与主库存在不一致，并且无法包装主备之间的延迟。&lt;/p&gt; 
&lt;p&gt;MySQL 复制大部分是向后兼容的，新版本的服务器可以作为老版本服务器的备库，但是老版本不能作为新版本服务器的备库，因为它可能无法解析新版本所用的新特性或语法，另外所使用的二进制文件格式也可能不同。&lt;/p&gt; 
&lt;p&gt;复制解决的问题：数据分布、负载均衡、备份、高可用性和故障切换、MySQL 升级测试。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;复制步骤&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;概述：① 在主库上把数据更改记录到二进制日志中。② 备库将主库的日志复制到自己的中继日志中。 ③ 备库读取中继日志中的事件，将其重放到备库数据之上。&lt;/p&gt; 
&lt;p&gt;第一步是在主库上记录二进制日志，每次准备提交事务完成数据更新前，主库将数据更新的事件记录到二进制日志中。MySQL 会按事务提交的顺序而非每条语句的执行顺序来记录二进制日志，在记录二进制日志后，主库会告诉存储引擎可以提交事务了。&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;下一步，备库将主库的二进制日志复制到其本地的中继日志中。备库首先会启动一个工作的 IO 线程，IO 线程跟主库建立一个普通的&lt;a class=&quot;content-link  js-post-content-keyword&quot; href=&quot;/jump/super-jump/word?word=%E5%AE%A2%E6%88%B7%E7%AB%AF&quot; target=&quot;_blank&quot;&gt;客户端&lt;/a&gt;连接，然后在主库上启动一个特殊的二进制转储线程，这个线程会读取主库上二进制日志中的事件。它不会对事件进行轮询。如果该线程追赶上了主库将进入睡眠状态，直到主库发送信号量通知其有新的事件产生时才会被唤醒，备库 IO 线程会将接收到的事件记录到中继日志中。&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;备库的 SQL 线程执行最后一步，该线程从中继日志中读取事件并在备库执行，从而实现备库数据的更新。当 SQL 线程追赶上 IO 线程时，中继日志通常已经在系统缓存中，所以中继日志的开销很低。SQL 线程执行的时间也可以通过配置选项来决定是否写入其自己的二进制日志中。&lt;/p&gt; 
&lt;p&gt;这种复制架构实现了获取事件和重放事件的解耦，允许这两个过程异步进行，也就是说 IO 线程能够独立于 SQL 线程工作。但这种架构也限制了复制的过程，在主库上并发允许的查询在备库只能串行化执行，因为只有一个 SQL 线程来重放中继日志中的事件。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>21c0ce50b926c4a26c67c88209ba8007</guid>
<title>[推荐] Elasticsearch 架构选型指南：不止是搜索引擎，还有......</title>
<link>https://toutiao.io/k/j0yifcz</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;最近被咨询到“ETC 卡口数据的存储以及车流量分析、车路线分析业务场景是否适合 Elasticsearch 去做”的问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这个问题涉及 Elasticsearch 架构选型的问题，而追根究底是：Elasticsearch 适合的业务场景的问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;记得我在做内部技术分享的时候，讲解 Elasticsearch 应用场景，就铺了3 页 PPT：搜索服务场景、日志实时分析场景、商业智能 BI 场景。我拿着初稿找同事讨论，我心想：“没毛病啊？！Elastic 官方、阿里云、腾讯云、金山云、京东云、百度云等介绍 Elasticsearch 都是这么说的。”&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;同事一句话点醒了我：“是不是应该加上 Elasticsearch 不适合做什么？”，“我大呼：搜戴斯乃（原来如此）”。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1、Elasticsearch 不适合做什么？&lt;/span&gt;&lt;/h1&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1.1 不支持事务&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;不支持：ACID（在写入或更新数据的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability））。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果要有强一致场景，如：银行业务，还得采购 Oracle 等大型商业数据库。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;据我不完整了解：银行使用 Elasticsearch 一般和系统日志统一规范、存储和检索、自动化运维相关，而非存储核心金融交易数据。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1.2 多表关联有限&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;因为习惯了：Mysql 多表通过外键关联检索，不免会将 Mysql 思维平移到 Elasticsearch。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;常见的实战多表关联问题如下：“
一般大in的场景有好的解决方案么？&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;比如：连锁 一个人管理了1000家门店 要去查自己管辖的商品。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;比如：企微 一个人维护了 1000个员工 要去查自己管辖的员工。”&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Elasticsearch 多表关联的解决方案一般概括如下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;宽表：适合增加冗余存储、空间换时间场景。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;nested 类型：适合子文档偶尔更新、查询频繁场景。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Join 父子文档类型：适合子文档频繁更新频繁场景。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;正如官方文档所说：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;“In particular, joins should be avoided. nested can make queries several times slower and parent-child relations can make queries hundreds of times slower.  ”&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;官方在数据建模部分强调：“应该避免关联。嵌套可以使查询速度慢几倍，父子关系可以使查询速度慢数百倍。因此，若能使用宽表，可以明显加速。”&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1.3 不支持准实时&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;refresh_interval 刷新频率决定近实时而非准实时。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;腾讯云黄华老师的分享《腾讯Elasticsearch海量规模背后的内核优化剖析》中强调：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;“ES 是一个实时的分布式搜索分析引擎，目前很多用户对 ES 的印象还是准实时，实际上在6.8版本之后官方文档已经将 near real-time 改为了 real-time”。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们直接看一下6.8版本——7.6版本官方文档的说法，的确如黄老师所说：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&quot;Elasticsearch provides real-time search and analytics for all types of data.&quot;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;ES 在写入完毕刷新之前，是可以通过 getById 的方式实时获取文档的，只是在刷新之前 FST 还没有构建，还不能提供搜索的能力。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但是，7.7版本——7.13版本（截止：2021-06-17最新版本）又改成：“Elasticsearch provides &lt;span&gt;&lt;strong&gt;&lt;span&gt;near&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt; real-time search and analytics for all types of data. ”&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;再一次改回原来的说法，说明：官方文档严谨性拿捏的很到位！&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;近实时而非准实时说法没毛病。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;7.13 版本官方文档地址：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;https://www.elastic.co/guide/en/elasticsearch/reference/7.13/elasticsearch-intro.html#elasticsearch-intro&lt;/span&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;1.4 高阶功能收费&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;比如：机器学习、Kerberos 安全认证、JDBC 客户端、ODBC客户端、Graph 探索、威胁猎捕等高阶功能。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这是主流开源上市公司的收费模式，MongoDB、Confluent、Elastic 皆是如此。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;开源版、基础版、黄金版、白金版、企业版的区别和差异，查看地址（要做到必知必会）：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;https://www.elastic.co/cn/subscriptions&lt;/span&gt;&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;2、Elasticsearch 认知升级&lt;/span&gt;&lt;/h1&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;2.1 Elasticsearch 早已不仅是搜索引擎了&lt;/span&gt;&lt;/h1&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.2418230563002681&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mjl8GCpsL9ZcNaUFJicbnBibErEsESYE1DU8Dx0xArHVA7eJNazNror4sLKr8udMbnvXHZQ6M5Tbqj8z5RbuC0UQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1865&quot;/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以 Elasticsearch 起家，发展到：集成 Elasticsearch、Logstash、Beats、Kibana 四位一体的“全家桶”。&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;Elasticsearch：搜索和分析引擎。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Logstash：数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Beats：一系列轻量型的单一功能数据采集器（包含：Metricbeat、Filebeat等100多种）。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Kibana ：使用图形和图表对Elasticsearch数据进行可视化和探索分析。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;2.2 Elasticsearch 早已不单单局限搜索场景&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Elasticsearch 发展战略也在随&lt;span&gt;势&lt;/span&gt;而变。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6293103448275862&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mjl8GCpsL9ZcNaUFJicbnBibErEsESYE1D6icbh5scF8qyNsBgH0n1jDW0KsL3icVNIfa9hs0licYBFQGnCh5jf1hibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1044&quot;/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;“3 + 1战略”——是指在Elastic Stack 基础上在Elastic 企业搜索、Elastic 全观察、Elastic 安全三个核心业务场景发力。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Elastic 已经由原来的 Elasticsearch 检索工具转变为 Elastic Stack 解决方案提供商。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;https://www.elastic.co/cn/products/&lt;/span&gt;&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3、Elasticsearch 业务场景&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;还是建议过一下官方文档关于 “ &lt;span&gt;What is Elasticsearch ? &lt;/span&gt;”的介绍。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;需要强调的点如下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;slogon 加了分析：由原来的“You know, for search” 改成 “You know, for search (&lt;span&gt;&lt;strong&gt;&lt;span&gt;and analysis&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;)”。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Elasticsearch 是位于 Elastic Stack 核心的分布式搜索和分析引擎。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Elasticsearch 为所有类型的数据提供高效存储和索引、近乎实时的搜索和分析。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所有都指哪些？主要指：结构化文本、非结构化文本、数值数据、地理空间数据等。&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Elasticsearch 的分布式特性、横向扩展能力可以应对数据、查询量的增长。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;Elasticsearch 提供了在各种用例中处理数据的速度和灵活性，尽管并非所有问题都是搜索问题。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;官方明确点出的几种场景总结如下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;（1）支持各类应用、网站等的全文搜索。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;（2）存储和分析日志、指标和安全事件数据。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;（3）使用机器学习实时自动建模数据的行为。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;（4）使用 Elasticsearch 作为存储引擎自动化业务工作流。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;（5）使用 Elasticsearch 作为地理信息系统 (GIS) 管理、集成和分析空间信息。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我也不免俗，将开头提及的：搜索服务场景、日志实时分析 场景、商业智能 BI 场景以截图的形式提供给大家，更直观一些。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以下三页 PPT 截图是基于腾讯云的分享结合了：官方文档、各大云厂商介绍结合我的理解进行了整合梳理。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.1 场景1：搜索服务&lt;/span&gt;&lt;/h2&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.1.1 典型场景&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.1.2 主要特性&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;高性能：高并发、低延迟的搜索体验&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;强相关：自定义打分、排序机制&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;高可用：机房、机架感知，异地容灾&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.1.3 相关公司&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;腾讯健康码、腾讯文档全文检索、携程、拼多多、蘑菇街、滴滴、今日头条、贝壳找房…….&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5484546883184913&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mjl8GCpsL9ZcNaUFJicbnBibErEsESYE1DA1Qibc3JxrE5TgNoklzfpIRNicuQI5bDmqaEAAicS1NnaxohoAuvIR4qw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1909&quot;/&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;3.2 场景2：日志实时分析&lt;/span&gt;&lt;/h2&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.2.1 典型场景&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;业务日志：用户行为日志、应用日志&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;状态日志：慢查询、异常探测&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;系统日志：debug、info、warn、error、fatal&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.2.2 主要特性&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;实时性：从日志产生到可访问，秒级&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;全文搜索：基于倒排索引，支持灵活的搜索分析&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;交互式分析：万亿级日志，搜索秒级响应&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.2.3 相关公司&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;日志易等&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5450755601875977&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mjl8GCpsL9ZcNaUFJicbnBibErEsESYE1Dlp2XofmibFeEfeF0ExspcLD1MNiaNzV4JDiaBTXib0k0PtIh9rd5PjSR3Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1919&quot;/&gt;&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;3.3 场景3：商业智能BI&lt;/span&gt;&lt;/h2&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.3.1 典型场景&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;电子商务、移动应用、广告媒体等业务都需要借助数据分析和数据挖掘来辅助商业决策，而规模庞大的业务数据对数据的统计分析造成了很大的挑战。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.3.2 主要特性&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;ES 拥有结构化查询的能力，支持复杂的过滤和聚合统计功能。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;帮助客户对海量数据进行高效地个性化统计分析、发现问题与机会、辅助商业决策，让数据产生真正的价值。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;3.3.3 相关公司&lt;/span&gt;&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;睿思BI 等。&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5483028720626631&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mjl8GCpsL9ZcNaUFJicbnBibErEsESYE1DkwtiaUcT04SmoO8Mmib8JHhibGCBr0iavcJNsMpWYKQbPP8tGbTJariczsw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1915&quot;/&gt;&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;4、小结&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我的座右铭之一：“自由不是你想干什么就干什么，而是你不想干什么就有能力不干什么！”。可见：不想干什么较想干什么更为牛逼。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而选型 Elasticsearch 的时候也要注意：它不能干什么或者它不擅长需要优先考虑，而将它擅长的、能干什么的方面发挥到极致是我们架构选型及后续实战方面要多考虑的因素。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你在架构选型方面还考虑哪些因素？欢迎留言交流分享实战心得。&lt;/p&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>2bd1853983d81bad07bef6afff30b30b</guid>
<title>[推荐] 徒手用 Go 写个 Redis 服务器</title>
<link>https://toutiao.io/k/t9ic9wp</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot; data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.66640625&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/xBgIbW1vdNPWkD6xmM3YDpLKRk9icwRvyvo6aWc1Pz3duHADKROA748khObVndvFBt9tZwTqMfIKZ876WHEDibww/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;span&gt;作者：HDT3213&lt;br/&gt;&lt;/span&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;今天给大家带来的开源项目是 Godis：一个用 Go 语言实现的 Redis 服务器。支持：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;5 种数据结构（string、list、hash、set、sortedset）&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;自动过期（TTL）&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;发布订阅、地理位置、持久化等功能&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.2294736842105263&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/xBgIbW1vdNPWkD6xmM3YDpLKRk9icwRvyyIuib3RuttVFnsljC8XwPgicGkL1zibBcKKAhTERZRwKnzEaesYZhcfXQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1900&quot;/&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你或许不需要自己实现 Redis 服务，但你是否厌烦了每天都是写增删改查的业务代码，想提高编程水平试图从零写个项目打开 IDE 却发现无从下手？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;动手造轮子一定是提高编程能力的好办法，下面就带大家用 Go 从零开始写一个 Redis 服务器（Godis），从中你将学到：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;如何编写 Go 语言 TCP 服务器&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;设计并实现安全可靠的通信协议（redis 协议）&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;如何使用 Go 语言开发高并发程序&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;设计和实现分布式集群以及分布式事务&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;熟悉链表、哈希表、跳表以及时间轮等常用数据结构&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;千万不要担心内容太难！！虽然示例代码是 Go，但就算你不会 Go 语言也不会影响你理解 Redis 的原理和底层协议以及高性能的秘密。而且作者为了照顾到广大读者，对技术的讲解做了优化。示例代码在原项目基础上做了简化，并逐行地加了注释。如果是高级玩家，请直接访问项目阅读源码：&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;https://github.com/HDT3213/godis&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面让我们一起拨开 Redis 的迷雾。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;/&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;一、写个 TCP 服务器&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;众所周知 Redis 是 C/S 模型，使用 TCP 协议进行通信。接下来就从实现 TCP 服务端开始。作为广泛用于服务端的编程语言 Golang 提供了非常简洁的 TCP 接口，所以实现起来十分方便。示例代码：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;ListenAndServe&lt;/span&gt;&lt;span&gt;(address &lt;span&gt;string&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    &lt;span&gt;// 绑定监听地址&lt;/span&gt;&lt;br/&gt;    listener, err := net.Listen(&lt;span&gt;&quot;tcp&quot;&lt;/span&gt;, address)&lt;br/&gt;    &lt;span&gt;if&lt;/span&gt; err != &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;        log.Fatal(fmt.Sprintf(&lt;span&gt;&quot;listen err: %v&quot;&lt;/span&gt;, err))&lt;br/&gt;    }&lt;br/&gt;    &lt;span&gt;defer&lt;/span&gt; listener.Close()&lt;br/&gt;    log.Println(fmt.Sprintf(&lt;span&gt;&quot;bind: %s, start listening...&quot;&lt;/span&gt;, address))&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;for&lt;/span&gt; {&lt;br/&gt;        &lt;span&gt;// Accept 会一直阻塞直到有新的连接建立或者listen中断才会返回&lt;/span&gt;&lt;br/&gt;        conn, err := listener.Accept()&lt;br/&gt;        &lt;span&gt;if&lt;/span&gt; err != &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;            &lt;span&gt;// 通常是由于listener被关闭无法继续监听导致的错误&lt;/span&gt;&lt;br/&gt;            log.Fatal(fmt.Sprintf(&lt;span&gt;&quot;accept err: %v&quot;&lt;/span&gt;, err))&lt;br/&gt;        }&lt;br/&gt;        &lt;span&gt;// 开启新的 goroutine 处理该连接&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;go&lt;/span&gt; Handle(conn)&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;Handle&lt;/span&gt;&lt;span&gt;(conn net.Conn)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    reader := bufio.NewReader(conn)&lt;br/&gt;    &lt;span&gt;for&lt;/span&gt; {&lt;br/&gt;        &lt;span&gt;// ReadString 会一直阻塞直到遇到分隔符 &#x27;\n&#x27;&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 遇到分隔符后 ReadString 会返回上次遇到分隔符到现在收到的所有数据&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 若在遇到分隔符之前发生异常, ReadString 会返回已收到的数据和错误信息&lt;/span&gt;&lt;br/&gt;        msg, err := reader.ReadString(&lt;span&gt;&#x27;\n&#x27;&lt;/span&gt;)&lt;br/&gt;        &lt;span&gt;if&lt;/span&gt; err != &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;            &lt;span&gt;// 通常遇到的错误是连接中断或被关闭，用io.EOF表示&lt;/span&gt;&lt;br/&gt;            &lt;span&gt;if&lt;/span&gt; err == io.EOF {&lt;br/&gt;                log.Println(&lt;span&gt;&quot;connection close&quot;&lt;/span&gt;)&lt;br/&gt;            } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;                log.Println(err)&lt;br/&gt;            }&lt;br/&gt;            &lt;span&gt;return&lt;/span&gt;&lt;br/&gt;        }&lt;br/&gt;        b := []&lt;span&gt;byte&lt;/span&gt;(msg)&lt;br/&gt;        &lt;span&gt;// 将收到的信息发送给客户端&lt;/span&gt;&lt;br/&gt;        conn.Write(b)&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;main&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    ListenAndServe(&lt;span&gt;&quot;:8000&quot;&lt;/span&gt;)&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;👌 至此只用了 40 行代码就搞定服务端啦！启动上面的 TCP 服务后，在终端中输入 &lt;code&gt;telnet 127.0.0.1 8000&lt;/code&gt; 就可以连接到刚写好的服务器，它会将你发送的消息原样返回给你（所以请不要骂它）：&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.2675925925925926&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/xBgIbW1vdNPWkD6xmM3YDpLKRk9icwRvyuibu0TPqyuw0zws5DwMnC0m4dRic0QxoiaqrxVsC3ZxU93z9qGVx6licKA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这个 TCP 服务器的非常简单，主协程调用 accept 函数来监听端口，接受新连接后开启一个 Goroutine 来处理它。这种简单的阻塞 IO 模型有些类似于早期的 Tomcat/Apache 服务器。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;阻塞 IO 模型是使用&lt;strong&gt;一个线程处理一个连接&lt;/strong&gt;，在没有收到新数据时监听线程处于阻塞状态，直到数据就绪后线程被唤醒进行处理。因为阻塞 IO 模型需要开启大量线程并且频繁地进行上下文切换，所以它的效率很低。而 Redis 使用的 epoll 技术（IO 多路复用）用&lt;strong&gt;一个线程处理大量连接&lt;/strong&gt;，极大地提高了吞吐量。那么我们的 TCP 服务器会比 Redis 慢很多吗？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当然不会，Golang 利用 Goroutine 调度开销远远小于线程调度开销的优势封装出 &lt;code&gt;goroutine-per-connection&lt;/code&gt; 风格的极简接口，而且 net/tcp 库将 epoll 封装成了阻塞 IO 的样子，在享受 epoll 高性能的同时避免了原生 epoll 接口所需的复杂异步代码。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在作者的电脑上 Redis 每秒可以响应 10.6k 个 PING 命令，而 Godis（完整代码） 的吞吐量为 9.2 kqps 相差并不大。想了解更多 Golang 高性能的㊙️密，可以搜索 &lt;code&gt;go netpoller&lt;/code&gt; 或者 &lt;code&gt;go 语言 网络轮询器&lt;/code&gt; 关键字&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;另外，合格的 TCP 的服务器在关闭的时候不应该一停了之，而需要完成响应已接收的请求、释放 TCP 连接等必要的清理工作。这个功能我们一般称为 &lt;code&gt;优雅关闭&lt;/code&gt; 或者 &lt;code&gt;graceful shutdown&lt;/code&gt;，优雅关闭步骤：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;首先，关闭 listener 停止接受新连接&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;然后，遍历所有存活连接逐个关闭&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;优雅关闭的代码比较多，这里就不完整贴出了。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;二、透视 Redis 协议&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在解决完通信后，下一步就是搞清楚 Redis 的协议，其实就是一套序列化协议类似 JSON、Protocol Buffers，你看底层其实也就是一些基础的知识。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;自 Redis 2.0 以后的通信统一为 RESP 协议（REdis Serialization Protocol)，该协议易于实现不仅可以高效的被程序解析，还能够被人类读懂容易调试。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;RESP 是一个二进制安全的文本协议，工作于 TCP 协议上。RESP 以行作为单位，客户端和服务器发送的命令或数据一律以 &lt;code&gt;\r\n&lt;/code&gt;（CRLF）作为换行符。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;二进制安全是指允许协议中出现任意字符而不会导致故障。比如 C 语言的字符串以 &lt;code&gt;\0&lt;/code&gt; 作为结尾不允许字符串中间出现 &lt;code&gt;\0&lt;/code&gt;，而 Go 语言的 string 则允许出现 &lt;code&gt;\0&lt;/code&gt;，我们说 Go 语言的 string 是二进制安全的，而 C 语言字符串不是二进制安全的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;RESP 的二进制安全性允许我们在 key 或者 value 中包含 &lt;code&gt;\r&lt;/code&gt; 或者 &lt;code&gt;\n&lt;/code&gt; 这样的特殊字符。在使用 Redis 存储 protobuf、msgpack 等二进制数据时，二进制安全性尤为重要。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;RESP 定义了 5 种格式：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;简单字符串（Simple String）：服务器用来返回简单的结果，比如 &quot;OK&quot; 非二进制安全，且不允许换行&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;错误信息（Error）：服务器用来返回简单的错误信息，比如 &quot;ERR Invalid Synatx&quot; 非二进制安全，且不允许换行&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;整数（Integer）：llen、scard 等命令的返回值，64 位有符号整数&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;字符串（Bulk String）：二进制安全字符串，比如 get 等命令的返回值&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;数组（Array，又称 Multi Bulk Strings）：Bulk String 数组，客户端发送指令以及 lrange 等命令响应的格式&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;RESP 通过第一个字符来表示格式：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;简单字符串：以&quot;+&quot; 开始， 如：&quot;+OK\r\n&quot;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;错误：以&quot;-&quot; 开始，如：&quot;-ERR Invalid Synatx\r\n&quot;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;整数：以&quot;:&quot;开始，如：&quot;:1\r\n&quot;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;字符串：以 &lt;code&gt;$&lt;/code&gt; 开始&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;数组：以 &lt;code&gt;*&lt;/code&gt; 开始&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面让我们通过一些实际例子来理解协议。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.1 字符串&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;字符串（Bulk String）有两行，第一行为 &lt;code&gt;$&lt;/code&gt;+正文长度，第二行为实际内容。如：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;$3&lt;/span&gt;\r\nSET\r\n&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;字符串（Bulk String）是二进制安全的，就是说可以在 Bulk String 内部包含 &quot;\r\n&quot; 字符（行尾的 CRLF 被隐藏）：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;$4&lt;/span&gt;&lt;br/&gt;a\r\nb&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.2 空&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;$-1&lt;/code&gt; 表示 nil，比如使用 get 命令查询一个不存在的 key 时，响应即为 &lt;code&gt;$-1&lt;/code&gt;。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.3 数组&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;数组（Array）格式第一行为 &quot;*&quot;+数组长度，其后是相应数量的 字符串（Bulk String）。比如 &lt;code&gt;[&quot;foo&quot;, &quot;bar&quot;]&lt;/code&gt; 的报文（传输时的内容）：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;*2&lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;foo&lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;bar&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;客户端也使用 数组（Array）格式向服务端发送指令。命令本身将作为第一个参数，比如 &lt;code&gt;SET key value&lt;/code&gt; 指令的 RESP 报文：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;*3&lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;SET&lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;key&lt;br/&gt;&lt;span&gt;$5&lt;/span&gt;&lt;br/&gt;value&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;将换行符打印出来：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;*3\r\n$3\r\nSET\r\n$3\r\nkey\r\n$5\r\nvalue\r\n&lt;/code&gt;&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.4 解析预备&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;知道常用的 RESP 报文内容后，就可以开始着手解析了。但需要注意的是 RESP 是 &lt;code&gt;二进制安全&lt;/code&gt; 的协议，它允许在正文中使用 &lt;code&gt;\r\n&lt;/code&gt; 字符。举例来说 Redis 可以正确接收并执行 &lt;code&gt;SET &quot;a\r\nb&quot; hellogithub&lt;/code&gt; 指令，这条指令的正确报文是这样的：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;*3  &lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;SET&lt;br/&gt;&lt;span&gt;$4&lt;/span&gt;&lt;br/&gt;a\r\nb &lt;br/&gt;&lt;span&gt;$11&lt;/span&gt;&lt;br/&gt;hellogithub&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当 &lt;code&gt;ReadBytes&lt;/code&gt; 读取到第五行 &quot;a\r\nb\r\n&quot; 时会将其误认为两行：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;*3  &lt;br/&gt;&lt;span&gt;$3&lt;/span&gt;&lt;br/&gt;SET&lt;br/&gt;&lt;span&gt;$4&lt;/span&gt;&lt;br/&gt;a  // 错误的分行&lt;br/&gt;b // 错误的分行&lt;br/&gt;&lt;span&gt;$11&lt;/span&gt;&lt;br/&gt;hellogithub&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;因此当读取到第四行 &lt;code&gt;$4&lt;/code&gt; 后，不应该继续使用 &lt;code&gt;ReadBytes(&#x27;\n&#x27;)&lt;/code&gt; 读取下一行，应使用 &lt;code&gt;io.ReadFull(reader, msg)&lt;/code&gt; 方法来读取指定长度的内容。&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;msg = &lt;span&gt;make&lt;/span&gt;([]&lt;span&gt;byte&lt;/span&gt;, &lt;span&gt;4&lt;/span&gt; + &lt;span&gt;2&lt;/span&gt;) &lt;span&gt;// 正文长度4 + 换行符长度2&lt;/span&gt;&lt;br/&gt;_, err = io.ReadFull(reader, msg)&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;2.5 编写 RESP 协议解析器&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;解决完上面内容包含 &quot;\r\n&quot; 的问题，我们就可以开始放手编写 Redis 协议解析器啦！&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;type&lt;/span&gt; Payload &lt;span&gt;struct&lt;/span&gt; {&lt;br/&gt; Data redis.Reply&lt;br/&gt; Err  error&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;// ParseStream 通过 io.Reader 读取数据并将结果通过 channel 将结果返回给调用者&lt;/span&gt;&lt;br/&gt;&lt;span&gt;// 流式处理的接口适合供客户端/服务端使用&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;ParseStream&lt;/span&gt;&lt;span&gt;(reader io.Reader)&lt;/span&gt; &amp;lt;-&lt;span&gt;chan&lt;/span&gt; *&lt;span&gt;Payload&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; ch := &lt;span&gt;make&lt;/span&gt;(&lt;span&gt;chan&lt;/span&gt; *Payload)&lt;br/&gt; &lt;span&gt;go&lt;/span&gt; parse0(reader, ch)&lt;br/&gt; &lt;span&gt;return&lt;/span&gt; ch&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;由于解析器的代码比较多，这里只简单地介绍一下核心流程。&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;parse0&lt;/span&gt;&lt;span&gt;(reader io.Reader, ch &lt;span&gt;chan&lt;/span&gt;&amp;lt;- *Payload)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    &lt;span&gt;// 初始化读取状态&lt;/span&gt;&lt;br/&gt;    readingMultiLine := &lt;span&gt;false&lt;/span&gt;&lt;br/&gt;    expectedArgsCount := &lt;span&gt;0&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;var&lt;/span&gt; args [][]&lt;span&gt;byte&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;var&lt;/span&gt; bulkLen &lt;span&gt;int64&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;for&lt;/span&gt; {&lt;br/&gt;        &lt;span&gt;// 上文中我们提到 RESP 是以行为单位的&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 因为行分为简单字符串和二进制安全的 BulkString，我们需要封装一个 readLine 函数来兼容&lt;/span&gt;&lt;br/&gt;        line, err = readLine(reader, bulkLen)&lt;br/&gt;        &lt;span&gt;if&lt;/span&gt; err != &lt;span&gt;nil&lt;/span&gt; { &lt;br/&gt;            &lt;span&gt;// 处理错误&lt;/span&gt;&lt;br/&gt;            &lt;span&gt;return&lt;/span&gt;&lt;br/&gt;        }&lt;br/&gt;        &lt;span&gt;// 接下来我们对刚刚读取的行进行解析&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 我们简单的将 Reply 分为两类:&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 单行: StatusReply, IntReply, ErrorReply&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 多行: BulkReply, MultiBulkReply&lt;/span&gt;&lt;br/&gt;&lt;br/&gt;        &lt;span&gt;if&lt;/span&gt; !readingMultiLine {&lt;br/&gt;            &lt;span&gt;if&lt;/span&gt; isMulitBulkHeader(line) {&lt;br/&gt;                &lt;span&gt;// 我们收到了 MulitBulkReply 的第一行&lt;/span&gt;&lt;br/&gt;                &lt;span&gt;// 获得 MulitBulkReply 中 BulkString 的个数&lt;/span&gt;&lt;br/&gt;                expectedArgsCount = parseMulitBulkHeader(line)&lt;br/&gt;                &lt;span&gt;// 等待 MulitBulkReply 后续行&lt;/span&gt;&lt;br/&gt;                readingMultiLine = &lt;span&gt;true&lt;/span&gt;&lt;br/&gt;            } &lt;span&gt;else&lt;/span&gt; &lt;span&gt;if&lt;/span&gt; isBulkHeader(line) {&lt;br/&gt;                &lt;span&gt;// 我们收到了 BulkReply 的第一行&lt;/span&gt;&lt;br/&gt;                &lt;span&gt;// 获得 BulkReply 第二行的长度, 通过 bulkLen 告诉 readLine 函数下一行 BulkString 的长度&lt;/span&gt;&lt;br/&gt;                bulkLen = parseBulkHeader()&lt;br/&gt;                &lt;span&gt;// 这个 Reply 中一共有 1 个 BulkString&lt;/span&gt;&lt;br/&gt;                expectedArgsCount = &lt;span&gt;1&lt;/span&gt; &lt;br/&gt;                &lt;span&gt;// 等待 BulkReply 后续行&lt;/span&gt;&lt;br/&gt;                readingMultiLine = &lt;span&gt;true&lt;/span&gt;&lt;br/&gt;            } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;                &lt;span&gt;// 处理 StatusReply, IntReply, ErrorReply 等单行 Reply&lt;/span&gt;&lt;br/&gt;                reply := parseSingleLineReply(line)&lt;br/&gt;                &lt;span&gt;// 通过 ch 返回结果&lt;/span&gt;&lt;br/&gt;                emitReply(ch)&lt;br/&gt;            }&lt;br/&gt;        } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;            &lt;span&gt;// 进入此分支说明我们正在等待 MulitBulkReply 或 BulkReply 的后续行&lt;/span&gt;&lt;br/&gt;            &lt;span&gt;// MulitBulkReply 的后续行有两种，BulkHeader 或者 BulkString&lt;/span&gt;&lt;br/&gt;            &lt;span&gt;if&lt;/span&gt; isBulkHeader(line) {&lt;br/&gt;                bulkLen = parseBulkHeader()&lt;br/&gt;            } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;                &lt;span&gt;// 我们正在读取一个 BulkString, 它可能是 MulitBulkReply 或 BulkReply &lt;/span&gt;&lt;br/&gt;                args = &lt;span&gt;append&lt;/span&gt;(args, line)&lt;br/&gt;            }&lt;br/&gt;            &lt;span&gt;if&lt;/span&gt; &lt;span&gt;len&lt;/span&gt;(args) == expectedArgsCount { &lt;span&gt;// 我们已经读取了所有后续行&lt;/span&gt;&lt;br/&gt;                &lt;span&gt;// 通过 ch 返回结果&lt;/span&gt;&lt;br/&gt;                emitReply(ch)&lt;br/&gt;                &lt;span&gt;// 重置状态, 准备解析下一条 Reply&lt;/span&gt;&lt;br/&gt;                readingMultiLine = &lt;span&gt;false&lt;/span&gt;&lt;br/&gt;                expectedArgsCount = &lt;span&gt;0&lt;/span&gt;&lt;br/&gt;                args = &lt;span&gt;nil&lt;/span&gt;&lt;br/&gt;                bulkLen = &lt;span&gt;0&lt;/span&gt;&lt;br/&gt;            }&lt;br/&gt;        }&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;三、实现内存数据库&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;至此我们已经搞定数据接收和解析的部分了，剩下就是我们应该把数据存在哪里了？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;抛开持久化部分，作为基于内存的 KV 数据库 Redis 的所有数据需要都存储在内存中的哈希表，而这个哈希表就是我们今天需要编写的最后一个组件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;与单线程的 Redis 不同我们实现的 Redis（godis）是并行工作的，所以我们必须考虑各种并发安全问题。常见的并发安全哈希表设计有几种：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;code&gt;sync.map&lt;/code&gt;：Golang 官方提供的并发哈希表，适合读多写少的场景。但是在 &lt;code&gt;m.dirty&lt;/code&gt; 刚被提升后会将 &lt;code&gt;m.read&lt;/code&gt; 复制到新的 &lt;code&gt;m.dirty&lt;/code&gt; 中，在数据量较大的情况下复制操作会阻塞所有协程，存在较大的隐患。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;code&gt;juc.ConcurrentHashMap&lt;/code&gt;：Java 的并发哈希表采用分段锁实现。在进行扩容时访问哈希表线程都将协助进行 rehash 操作，在 rehash 结束前所有的读写操作都会阻塞。因为缓存数据库中键值对数量巨大且对读写操作响应时间要求较高，使用 juc 的策略是不合适的。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;p&gt;&lt;code&gt;memcached hashtable&lt;/code&gt;：在后台线程进行 rehash 操作时，主线程会判断要访问的哈希槽是否已被 rehash 从而决定操作 old_hashtable 还是操作 new_hashtable。这种设计被称为&lt;strong&gt;渐进式 rehash&lt;/strong&gt; 它的优点是 rehash 操作基本不会阻塞主线程的读写，是最理想的的方案。&lt;/p&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但渐进式 rehash 的实现非常复杂，所以 godis 采用 Golang 社区广泛使用的分段锁策略（非上面的三种），就是将 key 分散到固定数量的 shard 中避免进行整体 rehash 操作。shard 是有锁保护的 map，当 shard 进行 rehash 时会阻塞 shard 内的读写，但不会对其他 shard 造成影响。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.5855728429985856&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/xBgIbW1vdNPWkD6xmM3YDpLKRk9icwRvyAgDiaNQU6ny3uGJqrZIhKlT4MLibuX6BWDeHwaAN5BdbYBs7XRhhawgg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;707&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;代码如下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;type&lt;/span&gt; ConcurrentDict &lt;span&gt;struct&lt;/span&gt; {&lt;br/&gt;    table []*Shard&lt;br/&gt;    count &lt;span&gt;int32&lt;/span&gt;&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;type&lt;/span&gt; Shard &lt;span&gt;struct&lt;/span&gt; {&lt;br/&gt;    m     &lt;span&gt;map&lt;/span&gt;[&lt;span&gt;string&lt;/span&gt;]&lt;span&gt;interface&lt;/span&gt;{}&lt;br/&gt;    mutex sync.RWMutex&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(dict *ConcurrentDict)&lt;/span&gt; &lt;span&gt;spread&lt;/span&gt;&lt;span&gt;(hashCode &lt;span&gt;uint32&lt;/span&gt;)&lt;/span&gt; &lt;span&gt;uint32&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; tableSize := &lt;span&gt;uint32&lt;/span&gt;(&lt;span&gt;len&lt;/span&gt;(dict.table))&lt;br/&gt; &lt;span&gt;return&lt;/span&gt; (tableSize - &lt;span&gt;1&lt;/span&gt;) &amp;amp; &lt;span&gt;uint32&lt;/span&gt;(hashCode)&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(dict *ConcurrentDict)&lt;/span&gt; &lt;span&gt;getShard&lt;/span&gt;&lt;span&gt;(index &lt;span&gt;uint32&lt;/span&gt;)&lt;/span&gt; *&lt;span&gt;Shard&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;return&lt;/span&gt; dict.table[index]&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(dict *ConcurrentDict)&lt;/span&gt; &lt;span&gt;Get&lt;/span&gt;&lt;span&gt;(key &lt;span&gt;string&lt;/span&gt;)&lt;/span&gt; &lt;span&gt;(val &lt;span&gt;interface&lt;/span&gt;{}, exists &lt;span&gt;bool&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; hashCode := fnv32(key)&lt;br/&gt; index := dict.spread(hashCode)&lt;br/&gt; shard := dict.getShard(index)&lt;br/&gt; shard.mutex.RLock()&lt;br/&gt; &lt;span&gt;defer&lt;/span&gt; shard.mutex.RUnlock()&lt;br/&gt; val, exists = shard.m[key]&lt;br/&gt; &lt;span&gt;return&lt;/span&gt;&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(dict *ConcurrentDict)&lt;/span&gt; &lt;span&gt;Put&lt;/span&gt;&lt;span&gt;(key &lt;span&gt;string&lt;/span&gt;, val &lt;span&gt;interface&lt;/span&gt;{})&lt;/span&gt; &lt;span&gt;(result &lt;span&gt;int&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;if&lt;/span&gt; dict == &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;  &lt;span&gt;panic&lt;/span&gt;(&lt;span&gt;&quot;dict is nil&quot;&lt;/span&gt;)&lt;br/&gt; }&lt;br/&gt; hashCode := fnv32(key)&lt;br/&gt; index := dict.spread(hashCode)&lt;br/&gt; shard := dict.getShard(index)&lt;br/&gt; shard.mutex.Lock()&lt;br/&gt; &lt;span&gt;defer&lt;/span&gt; shard.mutex.Unlock()&lt;br/&gt;&lt;br/&gt; &lt;span&gt;if&lt;/span&gt; _, ok := shard.m[key]; ok {&lt;br/&gt;  shard.m[key] = val&lt;br/&gt;  &lt;span&gt;return&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;&lt;br/&gt; } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;  shard.m[key] = val&lt;br/&gt;  dict.addCount()&lt;br/&gt;  &lt;span&gt;return&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;ConcurrentDict&lt;/code&gt; 可以保证对单个 key 操作的并发安全性，但是仍然无法满足并发安全的需求，举例来说：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;Incr 命令需要完成：&lt;code&gt;读取 -&amp;gt; 做加法 -&amp;gt; 写入&lt;/code&gt; 三步操作，读取和写入两步操作不是原子性的&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;MSETNX 命令当且仅当所有给定键都不存在时所有给定键设置值，我们需要保证「检查多个key是否存在」以及「写入多个key」这两个操作的原子性&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;因此我们需要实现 &lt;code&gt;db.Locker&lt;/code&gt; 用于锁定一个或一组 key 直到我们完成所有操作后再释放。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;实现 &lt;code&gt;db.Locker&lt;/code&gt; 最直接的想法是使用一个 &lt;code&gt;map[string]*sync.RWMutex&lt;/code&gt;&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;加锁过程分为两步：初始化 mutex -&amp;gt; 加锁&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;解锁过程也分为两步: 解锁 -&amp;gt; 释放mutex&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那么存在一个无法解决的并发问题：&lt;/p&gt;&lt;section data-tool=&quot;mdnice编辑器&quot;&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;时间&lt;/th&gt;&lt;th&gt;协程A&lt;/th&gt;&lt;th&gt;协程B&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;&lt;br/&gt;&lt;/td&gt;&lt;td&gt;locker[&quot;a&quot;].Unlock()&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;locker[&quot;a&quot;] = &amp;amp;sync.RWMutex{}&lt;/td&gt;&lt;td&gt;&lt;br/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;&lt;br/&gt;&lt;/td&gt;&lt;td&gt;delete(locker[&quot;a&quot;])&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;locker[&quot;a&quot;].Lock()&lt;/td&gt;&lt;td&gt;&lt;br/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;由于 t3 时协程 B 释放了锁，t4 时协程 A 试图加锁会失败。若协程B在解锁时不执行 &lt;code&gt;delete(locker[&quot;a&quot;])&lt;/code&gt; 就可以避免该异常的发生，但是这样会造成严重的内存泄露。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们注意到哈希槽的数量远少于 key 的数量，反过来说多个键可以共用一个哈希槽。所以我们不再直接对 key 进行加锁而是锁定 key 所在的哈希槽也可以保证安全，另一方面哈希槽数量较少即使不释放也不会消耗太多内存。&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;type&lt;/span&gt; Locks &lt;span&gt;struct&lt;/span&gt; {&lt;br/&gt;    table []*sync.RWMutex&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;Make&lt;/span&gt;&lt;span&gt;(tableSize &lt;span&gt;int&lt;/span&gt;)&lt;/span&gt; *&lt;span&gt;Locks&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    table := &lt;span&gt;make&lt;/span&gt;([]*sync.RWMutex, tableSize)&lt;br/&gt;    &lt;span&gt;for&lt;/span&gt; i := &lt;span&gt;0&lt;/span&gt;; i &amp;lt; tableSize; i++ {&lt;br/&gt;        table[i] = &amp;amp;sync.RWMutex{}&lt;br/&gt;    }&lt;br/&gt;    &lt;span&gt;return&lt;/span&gt; &amp;amp;Locks{&lt;br/&gt;        table: table,&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(locks *Locks)&lt;/span&gt;&lt;span&gt;Lock&lt;/span&gt;&lt;span&gt;(key &lt;span&gt;string&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    index := locks.spread(fnv32(key))&lt;br/&gt;    mu := locks.table[index]&lt;br/&gt;    mu.Lock()&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(locks *Locks)&lt;/span&gt;&lt;span&gt;UnLock&lt;/span&gt;&lt;span&gt;(key &lt;span&gt;string&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    index := locks.spread(fnv32(key))&lt;br/&gt;    mu := locks.table[index]&lt;br/&gt;    mu.Unlock()&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在锁定多个 key 时需要注意，若 协程A 持有 键a 的锁试图获得 键b 的锁，此时 协程B 持有 键b 的锁试图获得 键a 的锁则会形成死锁。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;解决方法是所有协程都按照相同顺序加锁，若两个协程都想获得 键a 和 键b 的锁，那么必须先获取 键a 的锁后获取 键b 的锁，这样就可以避免循环等待。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;到目前为止构建 Redis 服务器所需的基本组件已经备齐，只需要将 TCP 服务器、协议解析器与哈希表组装起来我们的 Redis 服务器就可以开始工作啦。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;最后，以上代码均简化自我写的 Godis：一个开源仅用 Go 语言实现的 Redis 服务器。期待您的关注和 Star：&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;项目地址：https://github.com/HDT3213/godis&lt;/p&gt;&lt;/blockquote&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;/h1&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;四、结束&lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;很多朋友的日常工作主要是编写业务代码，对于框架、数据库、中间件这些“架构”、“底层代码” 有一些恐惧感。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但本文我们只写了 3 个组件，共计几百行代码就实现了一个基本的 Redis 服务器。所以底层的技术并不难，只要你对技术感兴趣由浅入深、从简到繁，“底层代码”也并不神秘。&lt;/p&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;span&gt;- END -&lt;/span&gt;&lt;/section&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzA5MzYyNzQ0MQ==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/xBgIbW1vdNOqkqThUJBICyFBlvLvTyOCgBpibwWotSxGExfnOYFfPiaL9yn3GMUOCEVYN2RNslGCdQwgZy6ticdyA/0?wx_fmt=png&quot; data-nickname=&quot;HelloGitHub&quot; data-alias=&quot;GitHub520&quot; data-signature=&quot;分享 GitHub 上有趣、入门级的开源项目。&quot; data-from=&quot;0&quot;/&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;兴趣是最好的老师，&lt;strong&gt;HelloGitHub&lt;/strong&gt; 发现编程的乐趣&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>f47a160e354d1f67c2d3624da57ac34e</guid>
<title>[推荐] Redis 最佳实践：7 个维度 + 43 条使用规范，带你彻底玩转 Redis（附实践清单）</title>
<link>https://toutiao.io/k/kkvemjf</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p&gt;&lt;span&gt;这篇文&lt;/span&gt;&lt;span&gt;章我想和你聊一聊 Redis 的最佳实践。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你的项目或许已经使用 Redis 很长时间了，但在使用过程中，你可能还会或多或少地遇到以下问题：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;我的 Redis 内存为什么增长这么快？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;为什么我的 Redis 操作延迟变大了？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;如何降低 Redis 故障发生的频率？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;日常运维 Redis 需要注意什么？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;部署 Redis 时，如何做好资源规划？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Redis 监控重点要关注哪些指标？&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;尤其是当你的项目越来越依赖 Redis 时，这些问题就变得尤为重要。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;此时，你迫切需要一份&lt;strong&gt;「最佳实践指南」&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这篇文章，我将从以下七个维度，带你「全面」分析 Redis 的最佳实践优化：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在文章的最后，我还会给你一个完整的最佳实践清单，不管你是业务开发人员，还是 DBA 运维人员，这个清单将会帮助你更加「优雅」地用好 Redis。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;这篇文章干货很多，希望你可以耐心读完。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.5761589403973509&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyx7AWsPEBcwibhCgk78ibcYPOJrBMEjvclD1wYHibyjNg0OsKnKFapTBbyQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;453&quot;/&gt;&lt;/figure&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;如何使用 Redis 更节省内存？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;首先，我们来看一下 Redis 内存方面的优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;众所周知，Redis 的性能之所以如此之高，原因就在于它的数据都存储在「内存」中，所以访问 Redis 中的数据速度极快。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但从资源利用率层面来说，机器的内存资源相比于磁盘，还是比较昂贵的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当你的业务应用在 Redis 中存储数据很少时，你可能并不太关心内存资源的使用情况。但随着业务的发展，你的业务存储在 Redis 中的数据就会越来越多。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果没有提前制定好内存优化策略，那么等业务开始增长时，Redis 占用的内存也会开始膨胀。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，提前制定合理的内存优化策略，对于资源利用率的提升是很有必要的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那在使用 Redis 时，怎样做才能更节省内存呢？这里我给你总结了 6 点建议，我们依次来看：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;1) 控制 key 的长度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;最简单直接的内存优化，就是控制 key 的长度。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在开发业务时，你需要提前预估整个 Redis 中写入 key 的数量，如果 key 数量达到了百万级别，那么，过长的 key 名也会占用过多的内存空间。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你需要保证 key 在简单、清晰的前提下，尽可能把 key 定义得短一些。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;例如，原有的 key 为 user:book:123，则可以优化为 u:bk:123。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这样一来，你的 Redis 就可以节省大量的内存，这个方案对内存的优化非常直接和高效。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;2) 避免存储 bigkey&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除了控制 key 的长度之外，你同样需要关注 value 的大小，如果大量存储 bigkey，也会导致 Redis 内存增长过快。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除此之外，客户端在读写 bigkey 时，还有产生性能问题（下文会具体详述）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你要避免在 Redis 中存储 bigkey，我给你的建议是：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;String：大小控制在 10KB 以下&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;List/Hash/Set/ZSet：元素数量控制在 1 万以下&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;3) 选择合适的数据类型&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 提供了丰富的数据类型，这些数据类型在实现上，也对内存使用做了优化。具体来说就是，一种数据类型对应多种数据结构来实现：&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.7314974182444062&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxDPJAlJj31DWQxBtUc4qCyqPt5SEWNpY1JpDyibcJxo1rhVBmqMjIw9g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;例如，String、Set 在存储 int 数据时，会采用整数编码存储。Hash、ZSet 在元素数量比较少时（可配置），会采用压缩列表（ziplist）存储，在存储比较多的数据时，才会转换为哈希表和跳表。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;作者这么设计的原因，就是为了进一步节约内存资源。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那么你在存储数据时，就可以利用这些特性来优化 Redis 的内存。这里我给你的建议如下：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;String、Set：尽可能存储 int 类型数据&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Hash、ZSet：存储的元素数量控制在转换阈值之下，以压缩列表存储，节约内存&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;4) 把 Redis 当作缓存使用&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 数据存储在内存中，这也意味着其资源是有限的。你在使用 Redis 时，要把它当做缓存来使用，而不是数据库。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你的应用写入到  Redis 中的数据，尽可能地都设置「过期时间」。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;业务应用在 Redis 中查不到数据时，再从后端数据库中加载到 Redis 中。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.5591647331786543&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyx4t8UZ34WMpePuN9IFbicJlvRyuZFrXTAZSemoQV44KSmZLvt8AYqE7A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;431&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;采用这种方案，可以让 Redis 中只保留经常访问的「热数据」，内存利用率也会比较高。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;5) 实例设置 maxmemory + 淘汰策略&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;虽然你的 Redis key 都设置了过期时间，但如果你的业务应用写入量很大，并且过期时间设置得比较久，那么短期间内 Redis 的内存依旧会快速增长。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果不控制 Redis 的内存上限，也会导致使用过多的内存资源。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于这种场景，你需要提前预估业务数据量，然后给这个实例设置 maxmemory 控制实例的内存上限，这样可以避免 Redis 的内存持续膨胀。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;配置了 maxmemory，此时你还要设置数据淘汰策略，而淘汰策略如何选择，你需要结合你的业务特点来决定：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;volatile-lru / allkeys-lru：优先保留最近访问过的数据&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;volatile-lfu / allkeys-lfu：优先保留访问次数最频繁的数据（4.0+版本支持）&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;volatile-ttl ：优先淘汰即将过期的数据&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;volatile-random / allkeys-random：随机淘汰数据&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;6) 数据压缩后写入 Redis&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上方案基本涵盖了 Redis 内存优化的各个方面。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你还想进一步优化 Redis 内存，你还可以在业务应用中先将数据压缩，再写入到 Redis 中（例如采用 snappy、gzip 等压缩算法）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当然，压缩存储的数据，客户端在读取时还需要解压缩，在这期间会消耗更多 CPU 资源，你需要根据实际情况进行权衡。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上就是「节省内存资源」方面的实践优化，是不是都比较简单？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面我们来看「性能」方面的优化。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;如何持续发挥 Redis 的高性能？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当你的系统决定引入 Redis 时，想必看中它最关键的一点就是：&lt;strong&gt;性能&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们知道，一个单机版 Redis 就可以达到 10W QPS，这么高的性能，也意味着如果在使用过程中发生延迟情况，就会与我们的预期不符。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，在使用 Redis 时，如何持续发挥它的高性能，避免操作延迟的情况发生，也是我们的关注焦点。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在这方面，我给你总结了 13 条建议：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;1) 避免存储 bigkey&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;存储 bigkey 除了前面讲到的使用过多内存之外，对 Redis 性能也会有很大影响。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;由于 Redis 处理请求是单线程的，当你的应用在写入一个 bigkey 时，更多时间将消耗在「内存分配」上，这时操作延迟就会增加。同样地，删除一个 bigkey 在「释放内存」时，也会发生耗时。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而且，当你在读取这个 bigkey 时，也会在「网络数据传输」上花费更多时间，此时后面待执行的请求就会发生排队，Redis 性能下降。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.3168469860896445&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyx7J5WTib2UYFNWSPaP7UMOulkISC5KQ0syDoibcouL0iaiay0vzLPbNQSJg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;647&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你的业务应用尽量不要存储 bigkey，避免操作延迟发生。&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;如果你确实有存储 bigkey 的需求，你可以把 bigkey 拆分为多个小 key 存储。&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;2) 开启 lazy-free 机制&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你无法避免存储 bigkey，那么我建议你开启 Redis 的 lazy-free 机制。（4.0+版本支持）&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当开启这个机制后，Redis 在删除一个 bigkey 时，释放内存的耗时操作，将会放到后台线程中去执行，这样可以在最大程度上，避免对主线程的影响。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.4406196213425129&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxhrumCSKsfLUgVaAWxicFawP6ZbMrCyGSAL6Tas29Wue76gNLSHEIesA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;581&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;3) 不使用复杂度过高的命令&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 是单线程模型处理请求，除了操作 bigkey 会导致后面请求发生排队之外，在执行复杂度过高的命令时，也会发生这种情况。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;因为执行复杂度过高的命令，会消耗更多的 CPU 资源，主线程中的其它请求只能等待，这时也会发生排队延迟。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你需要避免执行例如 SORT、SINTER、SINTERSTORE、ZUNIONSTORE、ZINTERSTORE 等聚合类命令。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于这种聚合类操作，我建议你把它放到客户端来执行，不要让 Redis 承担太多的计算工作。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;4) 执行 O(N) 命令时，关注 N 的大小&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;规避使用复杂度过高的命令，就可以高枕无忧了么？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;答案是否定的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当你在执行 O(N) 命令时，同样需要注意 N 的大小。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果一次性查询过多的数据，也会在网络传输过程中耗时过长，操作延迟变大。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，对于容器类型（List/Hash/Set/ZSet），在元素数量未知的情况下，一定不要无脑执行 LRANGE key 0 -1 / HGETALL / SMEMBERS / ZRANGE key 0 -1。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在查询数据时，你要遵循以下原则：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;先查询数据元素的数量（LLEN/HLEN/SCARD/ZCARD）&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;元素数量较少，可一次性查询全量数据&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;元素数量非常多，分批查询数据（LRANGE/HASCAN/SSCAN/ZSCAN）&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;5) 关注 DEL 时间复杂度&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你没看错，在删除一个 key 时，如果姿势不对，也有可能影响到 Redis 性能。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;删除一个 key，我们通常使用的是 DEL 命令，回想一下，你觉得 DEL 的时间复杂度是多少？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;O(1) ？其实不一定。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当你删除的是一个 String 类型 key 时，时间复杂度确实是 O(1)。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但当你要删除的 key 是 List/Hash/Set/ZSet 类型，它的复杂度其实为 O(N)，N 代表元素个数。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;也就是说，删除一个 key，其元素数量越多，执行 DEL 也就越慢！&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;原因在于，删除大量元素时，需要依次回收每个元素的内存，元素越多，花费的时间也就越久！&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而且，这个过程默认是在主线程中执行的，这势必会阻塞主线程，产生性能问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那删除这种元素比较多的 key，如何处理呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我给你的建议是，分批删除：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;List类型：执行多次 LPOP/RPOP，直到所有元素都删除完成&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Hash/Set/ZSet类型：先执行 HSCAN/SSCAN/SCAN 查询元素，再执行 HDEL/SREM/ZREM 依次删除每个元素&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;没想到吧？一个小小的删除操作，稍微不小心，也有可能引发性能问题，你在操作时需要格外注意。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;6) 批量命令代替单个命令&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当你需要一次性操作多个 key 时，你应该使用批量命令来处理。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;批量操作相比于多次单个操作的优势在于，可以显著减少客户端、服务端的来回网络 IO 次数。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以我给你的建议是：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;String / Hash 使用 MGET/MSET 替代 GET/SET，HMGET/HMSET 替代 HGET/HSET&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;其它数据类型使用 Pipeline，打包一次性发送多个命令到服务端执行&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.8656429942418427&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxjWasaOIYSbt5M6jhuURC6qRiabvsCQa5UwNZL8uk06ulNicxrXVkCNNA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;521&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;7) 避免集中过期 key&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 清理过期 key 是采用定时 + 懒惰的方式来做的，而且这个过程都是在主线程中执行。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你的业务存在大量 key 集中过期的情况，那么 Redis 在清理过期 key 时，也会有阻塞主线程的风险。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.22901849217638692&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxZicHACvFNPU8gXS3VK2mDhfVJVnuTD5FSo5fTQdOABSrpYVrndge37A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;想要避免这种情况发生，你可以在设置过期时间时，增加一个随机时间，把这些 key 的过期时间打散，从而降低集中过期对主线程的影响。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;&lt;span&gt;8) 使用长连接操作 Redis，合理配置连接池&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你的业务应该使用长连接操作 Redis，避免短连接。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当使用短连接操作 Redis 时，每次都需要经过 TCP 三次握手、四次挥手，这个过程也会增加操作耗时。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;同时，你的客户端应该使用连接池的方式访问 Redis，并设置合理的参数，长时间不操作 Redis 时，需及时释放连接资源。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;9) 只使用 db0&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;尽管 Redis 提供了 16 个 db，但我只建议你使用 db0。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;为什么呢？我总结了以下 3 点原因：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;在一个连接上操作多个 db 数据时，每次都需要先执行 SELECT，这会给 Redis 带来额外的压力&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;使用多个 db 的目的是，按不同业务线存储数据，那为何不拆分多个实例存储呢？拆分多个实例部署，多个业务线不会互相影响，还能提高 Redis 的访问性能&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Redis Cluster 只支持 db0，如果后期你想要迁移到 Redis Cluster，迁移成本高&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;10) 使用读写分离 + 分片集群&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你的业务读请求量很大，那么可以采用部署多个从库的方式，实现读写分离，让 Redis 的从库分担读压力，进而提升性能。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.6519721577726219&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxjLxTKDdPHc6GLHiaKF4iaialXGwoicYRCjUSLibnqR4GTaGO3yTMMotawng/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;431&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你的业务写请求量很大，单个 Redis 实例已无法支撑这么大的写流量，那么此时你需要使用分片集群，分担写压力。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.501466275659824&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxw0H7LsTOpeAUpREmuyZQBAelbaMGY6oOHzvyf1jKMTyxiagkYYZqCkw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;682&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;11) 不开启 AOF 或 AOF 配置为每秒刷盘&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果对于丢失数据不敏感的业务，我建议你不开启 AOF，避免 AOF 写磁盘拖慢 Redis 的性能。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果确实需要开启 AOF，那么我建议你配置为 appendfsync everysec，把数据持久化的刷盘操作，放到后台线程中去执行，尽量降低 Redis 写磁盘对性能的影响。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;12) 使用物理机部署 Redis&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 在做数据持久化时，采用创建子进程的方式进行。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而创建子进程会调用操作系统的 fork 系统调用，这个系统调用的执行耗时，与系统环境有关。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;虚拟机环境执行 fork 的耗时，要比物理机慢得多，所以你的 Redis 应该尽可能部署在物理机上。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;13) 关闭操作系统内存大页机制&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Linux 操作系统提供了内存大页机制，其特点在于，每次应用程序向操作系统申请内存时，申请单位由之前的 4KB 变为了 2MB。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这会导致什么问题呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当 Redis 在做数据持久化时，会先 fork 一个子进程，此时主进程和子进程共享相同的内存地址空间。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当主进程需要修改现有数据时，会采用写时复制（Copy On Write）的方式进行操作，在这个过程中，需要重新申请内存。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果申请内存单位变为了 2MB，那么势必会增加内存申请的耗时，如果此时主进程有大量写操作，需要修改原有的数据，那么在此期间，操作延迟就会变大。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.9879663056558363&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxNz0n0fpaMdYViaibYjWb0Kjx1PZ7Cdu651ib4LSHm1TAJGX5UEQLEOMfA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;831&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，为了避免出现这种问题，你需要在操作系统上关闭内存大页机制。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;好了，以上这些就是 Redis 「高性能」方面的实践优化。如果你非常关心 Redis 的性能问题，可以结合这些方面针对性优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们再来看 Redis 「可靠性」如何保证。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;如何保证 Redis 的可靠性？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里我想提醒你的是，保证 Redis 可靠性其实并不难，但难的是如何做到「持续稳定」。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;下面我会从「资源隔离」、「多副本」、「故障恢复」这三大维度，带你分析保障 Redis 可靠性的最佳实践。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;1) 按业务线部署实例&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;提升可靠性的第一步，就是「资源隔离」。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你最好按不同的业务线来部署 Redis 实例，这样当其中一个实例发生故障时，不会影响到其它业务。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这种资源隔离的方案，实施成本是最低的，但成效却是非常大的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;2) 部署主从集群&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你只使用单机版 Redis，那么就会存在机器宕机服务不可用的风险。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，你需要部署「多副本」实例，即主从集群，这样当主库宕机后，依旧有从库可以使用，避免了数据丢失的风险，也降低了服务不可用的时间。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在部署主从集群时，你还需要注意，主从库需要分布在不同机器上，避免交叉部署。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这么做的原因在于，通常情况下，Redis 的主库会承担所有的读写流量，所以我们一定要优先保证主库的稳定性，即使从库机器异常，也不要对主库造成影响。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而且，有时我们需要对 Redis 做日常维护，例如数据定时备份等操作，这时你就可以只在从库上进行，这只会消耗从库机器的资源，也避免了对主库的影响。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;3) 合理配置主从复制参数&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在部署主从集群时，如果参数配置不合理，也有可能导致主从复制发生问题：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在这方面我给你的建议有以下 2 点：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;设置合理的 repl-backlog 参数：过小的 repl-backlog 在写流量比较大的场景下，主从复制中断会引发全量复制数据的风险&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;设置合理的 slave client-output-buffer-limit：当从库复制发生问题时，过小的 buffer 会导致从库缓冲区溢出，从而导致复制中断&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;4) 部署哨兵集群，实现故障自动切换&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;只部署了主从节点，但故障发生时是无法自动切换的，所以，你还需要部署哨兵集群，实现故障的「自动切换」。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而且，多个哨兵节点需要分布在不同机器上，实例为奇数个，防止哨兵选举失败，影响切换时间。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上这些就是保障 Redis「高可靠」实践优化，你应该也发现了，这些都是部署和运维层的优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除此之外，你可能还会对 Redis 做一些「日常运维」工作，这时你要注意哪些问题呢？&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;日常运维 Redis 需要注意什么？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;如果你是 DBA 运维人员，在平时运维 Redis 时，也需要注意以下 6 个方面。&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;1) 禁止使用 KEYS/FLUSHALL/FLUSHDB 命令&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;执行这些命令，会长时间阻塞 Redis 主线程，危害极大，所以你必须禁止使用它。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果确实想使用这些命令，我给你的建议是：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;SCAN 替换 KEYS&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;4.0+版本可使用 FLUSHALL/FLUSHDB ASYNC，清空数据的操作放在后台线程执行&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;2) 扫描线上实例时，设置休眠时间&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;不管你是使用 SCAN 扫描线上实例，还是对实例做 bigkey 统计分析，我建议你在扫描时一定记得设置休眠时间。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;防止在扫描过程中，实例 OPS 过高对 Redis 产生性能抖动。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;3) 慎用 MONITOR 命令&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;有时在排查 Redis 问题时，你会使用 MONITOR 查看 Redis 正在执行的命令。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但如果你的 Redis OPS 比较高，那么在执行 MONITOR 会导致 Redis 输出缓冲区的内存持续增长，这会严重消耗 Redis 的内存资源，甚至会导致实例内存超过 maxmemory，引发数据淘汰，这种情况你需要格外注意。&lt;/p&gt;&lt;figure data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-ratio=&quot;0.3315972222222222&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxmZvibuS8oeZibfl9TDvwmAx5mleOmpUtYcprib55awsF3Dh8vTBen7teg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;576&quot;/&gt;&lt;/figure&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以你在执行 MONITOR 命令时，一定要谨慎，尽量少用。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;4) 从库必须设置为 slave-read-only&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你的从库必须设置为 slave-read-only 状态，避免从库写入数据，导致主从数据不一致。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除此之外，从库如果是非 read-only 状态，如果你使用的是 4.0 以下的 Redis，它存在这样的 Bug：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;从库写入了有过期时间的数据，不会做定时清理和释放内存。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这会造成从库的内存泄露！这个问题直到 4.0 版本才修复，你在配置从库时需要格外注意。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;5) 合理配置 timeout 和 tcp-keepalive 参数&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果因为网络原因，导致你的大量客户端连接与 Redis 意外中断，恰好你的 Redis 配置的 maxclients 参数比较小，此时有可能导致客户端无法与服务端建立新的连接（服务端认为超过了 maxclients）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;造成这个问题原因在于，客户端与服务端每建立一个连接，Redis 都会给这个客户端分配了一个 client fd。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;当客户端与服务端网络发生问题时，服务端并不会立即释放这个 client fd。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;什么时候释放呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 内部有一个定时任务，会定时检测所有 client 的空闲时间是否超过配置的 timeout 值。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果 Redis 没有开启 tcp-keepalive 的话，服务端直到配置的 timeout 时间后，才会清理释放这个 client fd。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在没有清理之前，如果还有大量新连接进来，就有可能导致 Redis 服务端内部持有的 client fd 超过了 maxclients，这时新连接就会被拒绝。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;针对这种情况，我给你的优化建议是：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;不要配置过高的 timeout：让服务端尽快把无效的 client fd 清理掉&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Redis 开启 tcp-keepalive：这样服务端会定时给客户端发送 TCP 心跳包，检测连接连通性，当网络异常时，可以尽快清理僵尸 client fd&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;6) 调整 maxmemory 时，注意主从库的调整顺序&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Redis 5.0 以下版本存在这样一个问题：&lt;strong&gt;从库内存如果超过了 maxmemory，也会触发数据淘汰。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在某些场景下，从库是可能优先主库达到 maxmemory 的（例如在从库执行 MONITOR 命令，输出缓冲区占用大量内存），那么此时从库开始淘汰数据，主从库就会产生不一致。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;要想避免此问题，在调整 maxmemory 时，一定要注意主从库的修改顺序：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;调大 maxmemory：先修改从库，再修改主库&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;调小 maxmemory：先修改主库，再修改从库&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;直到 Redis 5.0，Redis 才增加了一个配置 replica-ignore-maxmemory，默认从库超过 maxmemory 不会淘汰数据，才解决了此问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;好了，以上这些就是「日常运维」Redis 需要注意的，你可以对各个配置项查漏补缺，看有哪些是需要优化的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;接下来，我们来看一下，保障 Redis「安全」都需要注意哪些问题。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;Redis 安全如何保证？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;无论如何，在互联网时代，安全问题一定是我们需要随时警戒的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你可能听说过 Redis 被注入可执行脚本，然后拿到机器 root 权限的安全问题，都是因为在部署 Redis 时，没有把安全风险注意起来。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;针对这方面，我给你的建议是：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;不要把 Redis 部署在公网可访问的服务器上&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;部署时不使用默认端口 6379&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;以普通用户启动 Redis 进程，禁止 root 用户启动&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;限制 Redis 配置文件的目录访问权限&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;推荐开启密码认证&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;禁用/重命名危险命令（KEYS/FLUSHALL/FLUSHDB/CONFIG/EVAL）&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;只要你把这些做到位，基本上就可以保证 Redis 的安全风险在可控范围内。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;至此，我们分析了 Redis 在内存、性能、可靠性、日常运维方面的最佳实践优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除了以上这些，你还需要做到提前「预防」。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;如何预防 Redis 问题？&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;要想提前预防 Redis 问题，你需要做好以下两个方面：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;合理的资源规划&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;完善的监控预警&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;先来说资源规划。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在部署 Redis 时，如果你可以提前做好资源规划，可以避免很多因为资源不足产生的问题。这方面我给你的建议有以下 3 点：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;保证机器有足够的 CPU、内存、带宽、磁盘资源&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;提前做好容量规划，主库机器预留一半内存资源，防止主从机器网络故障，引发大面积全量同步，导致主库机器内存不足的问题&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;单个实例内存建议控制在 10G 以下，大实例在主从全量同步、RDB 备份时有阻塞风险&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;再来看监控如何做。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;监控预警是提高稳定性的重要环节，完善的监控预警，可以把问题提前暴露出来，这样我们才可以快速反应，把问题最小化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这方面我给你的建议是：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;做好机器 CPU、内存、带宽、磁盘监控，资源不足时及时报警，任意资源不足都会影响 Redis 性能&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;设置合理的 slowlog 阈值，并对其进行监控，slowlog 过多及时报警&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;监控组件采集 Redis INFO 信息时，采用长连接，避免频繁的短连接&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;做好实例运行时监控，重点关注 expired_keys、evicted_keys、latest_fork_usec 指标，这些指标短时突增可能会有阻塞风险&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;好了，总结一下，这篇文章我带你全面分析了 Redis 最佳实践的优化路径，其中包括内存资源、高性能、高可靠、日常运维、资源规划、监控、安全 7 个维度。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里我画成了思维导图，方便你在实践时做参考。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-backh=&quot;491&quot; data-backw=&quot;568&quot; data-ratio=&quot;0.8651419558359621&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxoZ9Qog3MIZmVibbhozjUdiaAvXZzBQwecWskzfluZ0FWQoiaPDI69NGNw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2536&quot;/&gt;&lt;span&gt;我还&lt;/span&gt;&lt;span&gt;把这些实践优化，按照「业务开发」和「运维」两个维度，进一步做了划分。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;并且以「强制」、「推荐」、「参考」3 个级别做了标注，这样你在实践优化时，就会更明确哪些该做，哪些需要结合实际的业务场景进一步分析。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;这些级&lt;/span&gt;&lt;span&gt;别的实施规则如下：&lt;/span&gt;&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;强制：需严格遵守，否则危害极大&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;推荐：推荐遵守，可提升性能、降低内存、便于运维&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;参考：根据业务特点参考实施&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你是业务开发人员，你需要了解 Redis 的运行机制，例如各个命令的执行时间复杂度、数据过期策略、数据淘汰策略等，使用合理的命令，并结合业务场景进行优化。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;img data-backh=&quot;568&quot; data-backw=&quot;568&quot; data-ratio=&quot;1&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyx4KzYB53twzEqX99WuDzqTEZcTB2ZPAib4iayicx6woZDKGYF6wtd4DHPQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1490&quot;/&gt;&lt;span&gt;如果你是 DBA 运&lt;/span&gt;&lt;span&gt;维人员，你需要在资源规划、运维、监控、安全层面做到位，&lt;span&gt;做到未雨绸缪。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;&lt;img data-backh=&quot;656&quot; data-backw=&quot;568&quot; data-ratio=&quot;1.1548582995951417&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/gB9Yvac5K3OSCEIQBtroLfFiaMMWzJpyxvHiaHzsZjBhn2KS2Eq2xSrfYZiaCiaGhicruGN6ENHrEpxcmcmlZalGia3A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1976&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;后记&lt;/span&gt;&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果你能耐心地读到这里，应该对如何「用好」Redis 有了新的认识。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这篇文章我们主要讲的是 Redis 最佳实践，对于「最佳实践」这个话题，我想再和你多聊几句。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;如果你面对的不是 Redis，而是其它中间件，例如 MySQL、Kafka，你在使用这些组件时，会有什么优化思路吗？&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;你也可以沿用这篇文章的这几个维度来分析：&lt;/p&gt;&lt;section&gt;你可以思考一下，MySQL 和 Kafka 在这几个维度，需要注意哪些问题。&lt;br/&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;另外，从学习技能的角度来讲，我们在软件开发过程中，要尽可能地去思考和探索「最佳实践」的方式。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;&lt;strong&gt;因为只有这样，我们才会不断督促自己去思考，对自己提出更高的要求，做到持续进步。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;span/&gt;&lt;/section&gt;&lt;hr/&gt;&lt;p&gt;往期推荐&lt;/p&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzU3ODkzMDA2Mw==&amp;amp;mid=2247483874&amp;amp;idx=1&amp;amp;sn=2fc108a4495ebea67e72ba01f2ea4fea&amp;amp;chksm=fd6c9502ca1b1c14985958ca223ef7d4031a7041464c8131a76499a541da1c92f397790d3902&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;彻底搞懂事件驱动模型 - Reactor&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzU3ODkzMDA2Mw==&amp;amp;mid=2247483865&amp;amp;idx=1&amp;amp;sn=7f758aa084e30195d32d51f67dacc5d8&amp;amp;chksm=fd6c9539ca1b1c2f322fd00bd683e0ecfdb084b082a632a1011c98790e3a04081298a6417f81&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;11&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;如何科学破解慢SQL?&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最后，欢迎大家关注Kaito和铁柱，一起成长。&lt;/p&gt;&lt;figure&gt;&lt;img data-ratio=&quot;1&quot; data-src=&quot;https://mmbiz.qpic.cn/sz_mmbiz_jpg/bymGBmLC3zGuqeeB9CXfeY68KDuHKbP2e0tu0b3IdRH4xAT5FwcUNoVIIFibkQr1LSahlnw08SCLia9zN6QMVTyw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;258&quot; title=&quot;&quot;/&gt;&lt;/figure&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>b144105616152d8d36cb5964fa175d60</guid>
<title>[推荐] [译] 2021 年 Java 集合面试 Top 问题（一）</title>
<link>https://toutiao.io/k/zbogucw</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;blockquote&gt;&lt;p&gt;原文:https://dzone.com/articles/top-java-collection-interview-questions-for-2021&lt;/p&gt;&lt;p&gt;作者: Sonia Mathias&lt;/p&gt;&lt;p&gt;翻译: 祝坤荣&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在Java中，集合是一种提供了存储与操作批量对象的框架。在JDK1.2中“集合框架”就被定义了，而且它提供了所有的集合类和接口。Java集合类中最主要的两个主要接口是Collection接口（java.util.Collection）和Map接口（java.util.Map）。Java集合框架提供的接口包括Set,List,Queue,Deque，提供类包括ArrayList,Vector,LinkedList,HashSet,PriorityQueue,TreeSet和LinkedHashSet。&lt;/p&gt;&lt;h2&gt;需要一个分离的集合框架&lt;/h2&gt;&lt;p&gt;如果我们不使用集合框架，标准的用于给Java对象分组的方法是Arrays,Vectors或者HashTable。他们都没有通用的接口。他们的实现都是被单独定义的且互相之间没有任何联系。因此，要去记住所有不同的方法，语法，和创建函数都很困难。&lt;/p&gt;&lt;p&gt;比如，如果要给Vector加一个元素我们会使用addElement()方法，而给Hashtable加一个元素则使用put()方法。&lt;/p&gt;&lt;h2&gt;使用集合框架的好处&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;1.&lt;/span&gt;降低编程负担：一个开发者可以聚焦在集合的最佳使用方法上而不是聚焦在集合的设计上。这对实现抽象有好处。&lt;/span&gt;&lt;span&gt;&lt;span&gt;2.&lt;/span&gt;提升编程速度：集合提供了一种数据结构的高性能实现，这可以提升速度。&lt;/span&gt;&lt;span&gt;&lt;span&gt;3.&lt;/span&gt;由于Java已经是一种广泛使用的语言了，大大小小的组织都在使用它。为自己准备好基础的和高级的Java面试题可以对面试有好处。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;让我们看看java面试中被问得最多的一些问题。&lt;/p&gt;&lt;h2&gt;对初级开发者问的最多的问题&lt;/h2&gt;&lt;p&gt;问题1：什么是Java中的框架？答：框架是提供了脚手架功能的一组类和对象的集合。理想的面向对象设计都应该有一个框架提供了对于集合类的同类型任务提供同样的操作。&lt;/p&gt;&lt;p&gt;问题2：定义Java的集合框架。答：Java的集合框架是一组接口和类的集合，提供了高效保存和处理数据的方法。Java集合框架提供的接口有Set，List，Queue，Deque，提供的类包括ArrayList,Vector,LinkedList,HashSet,PriorityQueue,TreeSet和LinkedHashSet。&lt;/p&gt;&lt;p&gt;问题3：Java集合框架中ArrayList与Vector的不同之处。答：ArrayList&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;不是synchronized的&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它可以以其数组大小的50%来扩展其大小&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它不是线程安全的&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它不是遗留类 Vector：&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它是synchronized的&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它可以增加自己双倍的大小&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它是线程安全的&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它是遗留类&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;问题4：Iterator与Enumeration的不同点。答：Iterator&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它可以遍历遗留类和非遗留类&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它比Enumeration慢&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它在遍历集合时可以执行remove操作&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它是fail-fast的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Enumeration&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它只能遍历遗留元素&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它比Iterator快&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它只能在集合上执行traverse&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它不是fail-fast的&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题5：LinkedList与ArrayList的区别？答：ArrayList&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;这个类实现了list接口&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;这个类使用了动态数组来存储元素&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;插入与移除操作的最佳复杂度是O（1），而最差复杂度是O（n）。查询操作（如存储一个特定索引的元素）会需要O（1）的时间。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;ArrayList在存储和查询数据时能干的比较好。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;LinkedList&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;这个类实现了list接口和deque接口。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;这个类存储元素使用了双链列表。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;插入与移除操作提供了O（1）的性能。查询操作（比如查询一个特定索引的元素）需要O（n）的时间。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;LinkedList在操作存储好的数据时性能更好。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;问题6：解释一下Queue接口的poll（）与remove（）方法的不同。答：两个方法都返回并移除队列头的内容。它们只有在队列为空时行为不一样；remove（）会抛一个异常而poll（）会返回null。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题7：Comparable与Comparator的不同。答：Comparable&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它提供了被排序元素使用的compareTo（）方法&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它属于java.lang包&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;排序逻辑必须要与我们想要排序的对象在同一个类中&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它提供单独的排序序列。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;实际类被改过了&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Comparator&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它提供排序元素用的compare（）方法&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它在java.util包中&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;排序逻辑必须要在不同的类中，以便根据不同对象的不同属性来编写排序方法&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;它提供多种排序序列&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;实际类没有被改动&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题8：计算机内存中Stack的定义是？答：stack是计算机内存中一个特殊区域，用来存储函数创建的临时变量。在stack中，变量是在运行时被声明，存储和初始化的。&lt;/p&gt;&lt;p&gt;问题9：列出map接口的集合视图。答：集合视图（Collection view）方法可以让Map在以下&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;三种方式视为一个集合：&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;键集合视图：Map中保存的所有键的Set集合。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;值集合视图：Map中保存的所有值的集合。这个集合不是一个Set，因为不同的主键key可以被映射到同一个值value。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Entry集合视图：Map中键值对的集合Set。Map接口提供了一个小的内嵌接口Map.Entry，元素都存储在这个Set。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题10：定义EnumSet。答：它是可以使用enum枚举类型的Set实现。所有的元素都必须属于一个特定的enum类型。它不是synchronized。NULL key是不允许的。&lt;/p&gt;&lt;p&gt;问题11：什么方法能让集合变成线程安全的？答：这些方法是：&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Collections.synchronizedList(list);&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Collections.synchronizedMap(map);&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Collections.synchronizedSet(set);&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;问题12：Queue与Deque的不同点 答：Queue&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;众所周知是单向队列&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;队列中的元素在填加或删除都在同一端&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Less versatile Deque&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;是双端队列&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;队列中的元素可以从队尾填加或者从两端填加和删除。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;More versatile&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题13：hashmap与hashtable的不同 答：Hashmap&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;不是synchronized，不是线程安全&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;继承了AbstractMap类&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;允许一个null key和多个null value。&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;可以被iterator遍历 Hashtable&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;Synchronized的，线程安全&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;继承了Dictionary类&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;不允许空key或者值&lt;/span&gt;&lt;span&gt;&lt;span&gt;•&lt;/span&gt;可以被enumerator和iterator遍历&lt;/span&gt;&lt;/p&gt;&lt;p&gt;问题14：定义Iterator。答：Iterator（）是一个提供了遍历集合方法的接口。它提供了一种普世的方式来遍历集合中的元素，并实现了iterator设计模式。&lt;/p&gt;&lt;p&gt;问题15：什么是navigable map？答：NavigableMap接口，Java集合框架的成员，属于java.util包。它是SortedMap的子接口，提供了如lowerKey，floorKey，ceilingKey和higherKey这样方便的导航方法。它也提供了从现有map创建一个子map的方法。&lt;/p&gt;&lt;p&gt;问题16：什么是queue接口的peek（）？答：Peek（）返回了队列的头。它不移除任何元素。当队列为空时返回null。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未完待续&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;本文来自祝坤荣(时序)的微信公众号「麦芽面包」，公众号id「darkjune_think」 &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;开发者/科幻爱好者/硬核主机玩家/业余翻译 转载请注明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;微博:祝坤荣 B站: https://space.bilibili.com/23185593/&lt;/p&gt;&lt;p&gt;交流Email: &lt;span&gt;zhukunrong@yeah.net&lt;sup&gt;[1]&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;&lt;h3&gt;References&lt;/h3&gt;&lt;p&gt;&lt;code&gt;[1]&lt;/code&gt; zhukunrong@yeah.net: &lt;em&gt;mailto:zhukunrong@yeah.net&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>