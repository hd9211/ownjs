<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>446dfe7c2225898af8d60b893c3aec2d</guid>
<title>如何正确新增字段</title>
<link>https://toutiao.io/k/13htx1u</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section class=&quot;post&quot;&gt;
&lt;p&gt;最近运维执行 DB 变更，给业务表 T_pay 新增字段 F_id，引起 DAO 模块读数据接口 GetBillInfo 出现大量找不到记录的失败。为什么只是新增 MySQL 字段，会引起模块接口失败？&lt;/p&gt;

&lt;hr/&gt;

&lt;h2 id=&quot;1原因分析&quot;&gt;1.原因分析&lt;/h2&gt;

&lt;p&gt;业务 MySQL 采用了读写分离的方式，写 Master 主机，读 Slave0 从机。这样的好处是读写负载被分布到不同机器，同时可以防止大量读引起 Master 故障。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/20210402-1.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;看 DB 监控发现新增字段时，出现了大量的主备延时。什么是主备延时？&lt;/p&gt;

&lt;p&gt;下图是主备同步的流程，Master 生成 binlog 后，dump_tread 线程将 binlog 同步传输给 Slave。Slave 上的 io_thread 线程接收数据并存储为中转日志 relog，最后由 sql_thread 线程将数据重放到引擎存储中。主备延时是从主机执行事务成功到备机 sql_thread 线程重放数据到最新事务 id 的时间差。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/20210402-2.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;11为什么-db-新增字段会影响主备延时&quot;&gt;1.1.为什么 DB 新增字段会影响主备延时？&lt;/h4&gt;

&lt;p&gt;运维操作 DDL 变更时选择的策略是从主机变更，那么会先在 Master 进行 DDL 操作，再同步 binlog 到 Slave。&lt;/p&gt;

&lt;p&gt;T_pay 表有 300M 大小，主库进行 DDL 时，会产生 mixed 的 binlog。从库 sql_thread 为单线程重放 binlog 日志，这个过程需要重建数据，占用线程时间长，造成 relaylog 堆积，产生延时。&lt;/p&gt;

&lt;h4 id=&quot;12主备延时会影响-master-上的写操作吗&quot;&gt;1.2.主备延时会影响 master 上的写操作吗？&lt;/h4&gt;

&lt;p&gt;出现主备延迟只影响了 Slave0 上的 select，并没影响 Master 上的 insert、update。原因是主备延时不会影响半同步。&lt;/p&gt;

&lt;p&gt;业务写 Master 时，会先写 binlog，只要有一个 Slave 返回 ACK，半同步就完成了。Slave 只要更新到 binlog 就返回 ACK 给 Master 了，然后 Slave上 的 sql_thread 进程可以慢慢重放数据。&lt;/p&gt;

&lt;h4 id=&quot;13出现读数据接口失败原因&quot;&gt;1.3.出现读数据接口失败原因&lt;/h4&gt;

&lt;p&gt;用户先付款成功，会在 Master 上 InsertBillInfo 生成一条订单记录；然后在 Slave0 上 GetBillInfo 去读这条订单记录，但由于主备延时，这条订单记录还没有在从机上生成，所以出现找不到记录的失败。&lt;/p&gt;

&lt;hr/&gt;

&lt;h2 id=&quot;2如何变更&quot;&gt;2.如何变更&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;方案一&lt;/strong&gt;：DAO 切换为写 Master 读 Master&lt;/p&gt;

&lt;p&gt;优势：切换之后实时性更好，不会出现主备延时&lt;/p&gt;

&lt;p&gt;劣势：DAO 在读写分离时，可以容忍读故障。例如，索引问题出现慢查询引发 DB 故障，此时只是影响读，业务可以正常运行。读写分离可以对这些场景进行兜底。如果切换为写 Master 读 Master，一旦出现故障，业务会不可用。可以考虑在读 Master 后变更 DB，观察业务情况后再判断是否需要切回 Slave。&lt;/p&gt;

&lt;p&gt;风险：需要修改配置切换为读写 Master&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方案二&lt;/strong&gt;：低峰期从主机缓慢变更&lt;/p&gt;

&lt;p&gt;优势：DB 变更已开始执行，这种方案不需要代码层面的修改&lt;/p&gt;

&lt;p&gt;劣势：每个表之间的变更间隔必须加大，尽量让中间 sleep 的时间追齐前面表变更导致的延迟。不加间隔时大约需要执行 4 小时&lt;/p&gt;

&lt;p&gt;风险：出现主备延时，导致销帐延迟&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方案三&lt;/strong&gt;：运维写脚本分开主备变更&lt;/p&gt;

&lt;p&gt;优势：可以尽量减少备机延时&lt;/p&gt;

&lt;p&gt;劣势：需要运维手写脚本来分开主备变更，属于非标准操作，风险较高&lt;/p&gt;

&lt;p&gt;风险：运维手写脚本分开主备变更&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方案四&lt;/strong&gt;：从机读不到时再读 Master 兜底&lt;/p&gt;

&lt;p&gt;优势：可以解决一部分主备延时问题&lt;/p&gt;

&lt;p&gt;劣势：放大请求，有些订单本来就读不到&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方案五&lt;/strong&gt;：从机获取两个 Slave 的延时，选最优 Slave 读&lt;/p&gt;

&lt;p&gt;优势：可以减轻主备延时&lt;/p&gt;

&lt;p&gt;劣势：可能还是存在延时问题&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;方案六&lt;/strong&gt;：低峰期挂公告停服再变更&lt;/p&gt;

&lt;p&gt;优势：不需要开发和运维变更&lt;/p&gt;

&lt;p&gt;劣势：MySQL 执行无法并行，执行时间 4 小时+，停服时间过长&lt;/p&gt;

&lt;p&gt;风险：影响用户体验&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结论&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对比以上方案后，选择方案一进行变更。变更当晚又出现主备延时。但因为 GetBillInfo 读订单表已经切换到读 Master，所以不再出现读不到订单的问题。&lt;/p&gt;

&lt;hr/&gt;

&lt;h2 id=&quot;3主备延迟来源&quot;&gt;3、主备延迟来源&lt;/h2&gt;

&lt;p&gt;在备库上执行 show slave status 命令查看 Seconds_Behind_Master 字段可以知道当前备库的延迟时间。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/20210402-3.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;主备延时包含两部分时间损耗：主机传输 binlog 日志到备库；备机接收完 binlog 和执行完这个事务。在网络正常情况下，第一部分耗时很短，主备延迟主要由第二部分引起，也就是备库 sql_thread 线程消费 relay log 的速度比主库生产要慢。&lt;/p&gt;

&lt;p&gt;除了上面业务中出现的场景会导致这种问题，还有哪些场景也会导致主备延迟？&lt;/p&gt;

&lt;p&gt;1、备机机器性能比主机差，或者参数配置不同&lt;/p&gt;

&lt;p&gt;2、备机负载压力大，比如在上面执行了数据统计分析语句、运维日常导出脚本&lt;/p&gt;

&lt;p&gt;3、大事务，在主机上执行 1 min，在备机上可能重放数据也需要 1 min&lt;/p&gt;

&lt;hr/&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://cloud.tencent.com/developer/article/1418418&quot;&gt;MySQL DDL 为什么成本高&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MySQL 45 讲：MySQL 是这么保证高可用的？&lt;/p&gt;
                          
&lt;/section&gt;



&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>4901d721221d9a6442edd6edafb65003</guid>
<title>书单丨“1 本抵 10 本” 的好书，建议收藏</title>
<link>https://toutiao.io/k/08uadl6</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;reward_qrcode_area reward_area tc&quot; id=&quot;js_reward_qrcode&quot;&gt;
                                &lt;p class=&quot;tips_global&quot;&gt;Long-press QR code to transfer me a reward&lt;/p&gt;
                                                                &lt;p class=&quot;reward_tips&quot;&gt;不用赞赏，在看就行（不点也行）&lt;/p&gt;
                                &lt;span class=&quot;reward_qrcode_img_wrp&quot;&gt;&lt;img class=&quot;reward_qrcode_img&quot; id=&quot;js_reward_qrcode_img&quot;/&gt;&lt;/span&gt;
                                &lt;p class=&quot;tips_global&quot;&gt;As required by Apple&#x27;s new policy, the Reward feature has been disabled on Weixin for iOS. You can still reward an Official Account by transferring money via QR code.&lt;/p&gt;
                            &lt;/div&gt;
                                                                            
                              
            &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>d6265346d91cd911cebd47da4d629331</guid>
<title>实时数仓中的消息队列技术深度对比</title>
<link>https://toutiao.io/k/us38se6</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;section data-role=&quot;outer&quot; label=&quot;Powered by 135editor.com&quot; data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;95966&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;img data-ratio=&quot;0.17708333333333334&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_gif/z2DApiaibzMiciblIVkTcwBn828uRZk8d2HHbpph3YNBDZSoib1norn7UnFpA6BSKza4wyUWSj2wwx2VmdhH7P1xjiag/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;288&quot; data-width=&quot;80%&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;公众号推文规则变了，点击上方 &quot;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;数据社&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&quot;, &lt;/span&gt;&lt;span&gt;&lt;strong&gt;设为星&lt;/strong&gt;&lt;strong&gt;标&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;后台回复【&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;加群&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;】，申请加入数据学习交流群&lt;/span&gt;&lt;/p&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzI4MzE4MjQxOQ==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/z2DApiaibzMicibStrZbEKicUIkEia0iaUrqcr4dq64JSoTr0gAkQE7QESQcYrsNqGvdQWrNUZKUz9zjO2WxEAKD5J5GQ/0?wx_fmt=png&quot; data-nickname=&quot;数据社&quot; data-alias=&quot;DataClub&quot; data-signature=&quot;我是一哥，在这里和大家分享大数据实践的那些事，个人专注MPP数据库研究、流处理计算、数据仓库架构和数据分析~&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;大家好，&lt;/span&gt;&lt;span&gt;我是一哥&lt;/span&gt;&lt;span&gt;，昨天分享了实时数仓的架构《&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368814&amp;amp;idx=1&amp;amp;sn=7a615d9170254e5116fd8e5647a762d0&amp;amp;chksm=f3901cd2c4e795c4fc7082d70a3b18932eb5663500a1c8ecfeb8583ede266e2a81685eaa4660&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;11&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;网易实时数仓1.0 2.0 3.0 架构&lt;/a&gt;》，实时数仓一定离不开消息队列，现在市面上有非常多的消息中间件，rabbitMQ、kafka、rocketMQ、pulsar、 redis等等，多得令人眼花缭乱。它们到底有什么异同，你应该选哪个？本文尝试通过技术演进的方式，以redis、kafka和 pulsar为例，逐步深入，讲讲它们架构和原理，帮助你更好地理解和学习消息队列。文章作者：刘德恩，腾讯IEG研发工程师。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;一、最基础的队列&lt;/strong&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br mpa-from-tpl=&quot;t&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最基础的消息队列其实就是一个双端队列，我们可以用双向链表来实现，如下图所示：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.07360672975814932&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;951&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRqLkt0e2mYIicaclqjFNAe0KBGdx4HJurQezCoM6ziaZCP41TC5b5bClg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;push_front：添加元素到队首；&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;pop_tail：从队尾取出元素。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;有了这样的数据结构之后，我们就可以在内存中构建一个消息队列，一些任务不停地往队列里添加消息，同时另一些任务不断地从队尾有序地取出这些消息。添加消息的任务我们称为producer，而取出并使用消息的任务，我们称之为consumer。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;要实现这样的内存消息队列并不难，甚至可以说很容易。但是如果要让它能在应对海量的并发读写时保持高效，还是需要下很多功夫的。&lt;/span&gt;&lt;/p&gt;&lt;h3/&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;二、Redis的队列&lt;/strong&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br mpa-from-tpl=&quot;t&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;redis刚好提供了上述的数据结构——list。redis list支持：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;lpush：从队列左边插入数据；&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;rpop：从队列右边取出数据。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这正好对应了我们队列抽象的push_front和pop_tail，因此我们可以直接把redis的list当成一个消息队列来使用。而且redis本身对高并发做了很好的优化，内部数据结构经过了精心地设计和优化。所以从某种意义上讲，用redis的list大概率比你自己重新实现一个list强很多。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但另一方面，使用redis list作为消息队列也有一些不足，比如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;对于上述的不足，目前看来第一条（持久化）是可以解决的。很多公司都有团队基于rocksdb leveldb进行二次开发，实现了支持redis协议的kv存储。这些存储已经不是redis了，但是用起来和redis几乎一样。它们能够保证数据的持久化，但对于上述的其他缺陷也无能为力了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实redis 5.0开始新增了一个stream数据类型，它是专门设计成为消息队列的数据结构，借鉴了很多kafka的设计，但是依然还有很多问题…直接进入到kafka的世界它不香吗？&lt;/span&gt;&lt;/p&gt;&lt;h3/&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;三、Kafka&lt;/strong&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;br mpa-from-tpl=&quot;t&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从上面你可以看到，一个真正的消息中间件不仅仅是一个队列那么简单。尤其是当它承载了公司大量业务的时候，它的功能完备性、吞吐量、稳定性、扩展性都有非常苛刻的要求。kafka应运而生，它是专门设计用来做消息中间件的系统。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;前面说redis list的不足时，虽然有很多不足，但是如果你仔细思考，其实可以归纳为两点：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这两点也是kafka要解决的核心问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;热key的本质问题是数据都集中在一台实例上，所以想办法把它分散到多个机器上就好了。为此，kafka提出了&lt;strong&gt;partition&lt;/strong&gt;的概念。一个队列（redis中的list），对应到kafka里叫topic。kafka把一个topic拆成了多个partition，每个partition可以分散到不同的机器上，这样就可以把单机的压力分散到多台机器上。因此topic在kafka中更多是一个逻辑上的概念，实际存储单元都是partition。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;其实redis的list也能实现这种效果，不过这需要在业务代码中增加额外的逻辑。比如可以建立n个list，key1, key2, ..., keyn，客户端每次往不同的key里push，消费端也可以同时从key1到keyn这n个list中rpop消费数据，这就能达到kafka多partition的效果。所以你可以看到，partition就是一个非常朴素的概念，用来把请求分散到多台机器。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;redis list中另一个大问题是rpop会删除数据，所以kafka的解决办法也很简单，不删就行了嘛。kafka用游标（cursor）解决这个问题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.9426877470355731&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;506&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRibL9Jl1laetYCF7ibia3V4oqFdflSYT97viaOkvqgSD6gTJEa6arBEWuCA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;和redis list不同的是，首先kafka的topic（实际上是partion）是用的单向队列来存储数据的，新数据每次直接追加到队尾。同时它维护了一个游标cursor，从头开始，每次指向即将被消费的数据的下标。每消费一条，cursor+1 。通过这种方式，kafka也能和redis list一样实现先入先出的语义，但是kafka每次只需要更新游标，并不会去删数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这样设计的好处太多了，尤其是性能方面，顺序写一直是最大化利用磁盘带宽的不二法门。但我们主要讲讲游标这种设计带来功能上的优势。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;首先可以支持消息的ACK机制了。由于消息不会被删除，因此可以等消费者明确告知kafka这条消息消费成功以后，再去更新游标。这样的话，只要kafka持久化存储了游标的位置，即使消费失败进程崩溃，等它恢复时依然可以重新消费&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;第二是可以支持分组消费：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.27710843373493976&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRxqN7bSY0alSIXibeuogjytgKxnkLnWlaYPSgt50nOHG7pK6aSe4PN8A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;这里需要引入一个消费组的概念，这个概念非常简单，因为消费组本质上就是一组游标。对于同一个topic，不同的消费组有各自的游标。监控组的游标指向第二条，BI组的游标指向第4条，trace组指向到了第10000条……各消费者游标彼此隔离，互不影响。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;通过引入消费组的概念，就可以非常容易地支持多业务方同时消费一个topic，也就是说所谓的1-N的“广播”，一条消息广播给N个订阅方。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，通过游标也很容易实现重新消费。因为游标仅仅就是记录当前消费到哪一条数据了，要重新消费的话直接修改游标的值就可以了。你可以把游标重置为任何你想要指定的位置，比如重置到0重新开始消费，也可以直接重置到最后，相当于忽略现有所有数据。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此你可以看到，kafka这种数据结构相比于redis的双向链表有了一个质的飞跃，不仅是性能上，同时也是功能上，全面的领先。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们可以来看看kafka的一个简单的架构图：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.7226074895977809&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;721&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRWrCLEIv28iaAY6GYTyfVylwd219UhcLGwJ7THSnHp0ZDgSicL7Ro2xXg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从这个图里我们可以看出，topic是一个逻辑上的概念，不是一个实体。一个topic包含多个partition，partition分布在多台机器上。这个机器，kafka中称之为&lt;strong&gt;broker&lt;/strong&gt;。（kafka集群中的一个broker对应redis集群中的一个实例）。对于一个topic，可以有多个不同的消费组同时进行消费。一个消费组内部可以有多个消费者实例同时进行消费，这样可以提高消费速率。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;但是这里需要非常注意的是，一个partition只能被消费组中的一个消费者实例来消费。换句话说，消费组中如果有多个消费者，不能够存在两个消费者同时消费一个partition的场景。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为什么呢？其实kafka要在partition级别提供顺序消费的语义，如果多个consumer消费一个partition，即使kafka本身是按顺序分发数据的，但是由于网络延迟等各种情况，consumer并不能保证按kafka的分发顺序接收到数据，这样达到消费者的消息顺序就是无法保证的。因此一个partition只能被一个consumer消费。kafka各consumer group的游标可以表示成类似这样的数据结构：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;json&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;{&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;topic-foo&quot;&lt;/span&gt;: {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;groupA&quot;&lt;/span&gt;: {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-0&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;0&lt;/span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-1&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;123&lt;/span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-2&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;78&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        },&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;groupB&quot;&lt;/span&gt;: {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-0&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;85&lt;/span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-1&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;9991&lt;/span&gt;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &lt;span class=&quot;code-snippet__attr&quot;&gt;&quot;partition-2&quot;&lt;/span&gt;: &lt;span class=&quot;code-snippet__number&quot;&gt;772&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        },&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    }&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;了解了kafka的宏观架构，你可能会有个疑惑，kafka的消费如果只是移动游标并不删除数据，那么随着时间的推移数据肯定会把磁盘打满，这个问题该如何解决呢？这就涉及到kafka的retention机制，也就是消息过期，类似于redis中的expire。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;不同的是，redis是按key来过期的，如果你给redis list设置了1分钟有效期，1分钟之后redis直接把整个list删除了。而kafka的过期是针对消息的，不会删除整个topic(partition)，只会删除partition中过期的消息。不过好在kafka的partition是单向的队列，因此队列中消息的生产时间都是有序的。因此每次过期删除消息时，从头开始删就行了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看起来似乎很简单，但仔细想一下还是有不少问题。举例来说，假如topicA-partition-0的所有消息被写入到一个文件中，比如就叫topicA-partition-0.log。我们再把问题简化一下，假如生产者生产的消息在topicA-partition-0.log中一条消息占一行，很快这个文件就到200G了。现在告诉你，这个文件前x行失效了，你应该怎么删除呢？非常难办，这和让你删除一个数组中的前n个元素一样，需要把后续的元素向前移动，这涉及到大量的CPU copy操作。假如这个文件有10M，这个删除操作的代价都非常大，更别说200G了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因此，kafka在实际存储partition时又进行了一个拆分。topicA-partition-0的数据并不是写到一个文件里，而是写到多个segment文件里。假如设置的一个segment文件大小上限是100M，当写满100M时就会创建新的segment文件，后续的消息就写到新创建的segment文件，就像我们业务系统的日志文件切割一样。这样做的好处是，当segment中所有消息都过期时，可以很容易地直接删除整个文件。而由于segment中消息是有序的，看是否都过期就看最后一条是否过期就行了。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. Kafka中的数据查找&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;topic的一个partition是一个逻辑上的数组，由多个segment组成，如下图所示：&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.8897058823529411&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;680&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRMKFQicQftyxw8EOcOvAsh67PmDCFnB1Abu1ssTco3icezowHxf8CWRFw/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这时候就有一个问题，如果我把游标重置到一个任意位置，比如第2897条消息，我怎么读取数据呢？&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;根据上面的文件组织结构，你可以发现我们需要确定两件事才能读出对应的数据：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;为了解决上面两个问题，kafka有一个非常巧妙的设计。首先，segment文件的文件名是以该文件里第一条消息的offset来命名的。一开始的segment文件名是 0.log，然后一直写直到写了18234条消息后，发现达到了设置的文件大小上限100M，然后就创建一个新的segment文件，名字是18234.log……&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;go&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;- /kafka/topic/order_create/partition&lt;span class=&quot;code-snippet__number&quot;&gt;-0&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;0.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;18234.l&lt;/span&gt;og #segment file&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;39712.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;54101.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;当我们要找offset为x的消息在哪个segment时，只需要通过文件名做一次二分查找就行了。比如offset为2879的消息（第2880条消息），显然就在0.log这个segment文件里。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;定位到segment文件之后，另一个问题就是要找到该消息在文件中的位置，也就是偏移量。&lt;/span&gt;&lt;span&gt;如果从头开始一条条地找，这个耗时肯定是无法接受的！&lt;/span&gt;&lt;span&gt;kafka的解决办法就是索引文件。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span/&gt;&lt;span&gt;就如mysql的索引一样，kafka为每个segment文件创建了一个对应的索引文件。&lt;/span&gt;&lt;span&gt;索引文件很简单，每条记录就是一个kv组，key是消息的offset，value是该消息在segment文件中的偏移量：&lt;/span&gt;&lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;每个segment文件对应一个索引文件：&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;go&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;- /kafka/topic/order_create/partition&lt;span class=&quot;code-snippet__number&quot;&gt;-0&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;0.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;0.i&lt;/span&gt;ndex&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;18234.l&lt;/span&gt;og #segment file&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;18234.i&lt;/span&gt;ndex #index file&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;39712.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;39712.i&lt;/span&gt;ndex&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;54101.l&lt;/span&gt;og&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    - &lt;span class=&quot;code-snippet__number&quot;&gt;54101.i&lt;/span&gt;ndex&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;有了索引文件，我们就可以拿到某条消息具体的位置，从而直接进行读取。再捋一遍这个流程：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;通过这种文件组织形式，我们可以在kafka中非常快速地读取出任何一条消息。但这又引出了另一个问题，如果消息量特别大，每条消息都在index文件中加一条记录，这将浪费很多空间。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;可以简单地计算一下，假如index中一条记录16个字节（offset 8 + position 8），一亿条消息就是16*10^8字节=1.6G。对于一个稍微大一点的公司，kafka用来收集日志的话，一天的量远远不止1亿条，可能是数十倍上百倍。这样的话，index文件就会占用大量的存储。因此，权衡之下kafka选择了使用”&lt;strong&gt;稀疏索引&lt;/strong&gt;“。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;所谓稀疏索引就是并非所有消息都会在index文件中记录它的position，每间隔多少条消息记录一条，比如每间隔10条消息记录一条offset-position：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;p&gt;&lt;span&gt;offset&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;th&gt;&lt;p&gt;&lt;span&gt;position&lt;/span&gt;&lt;/p&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;0&lt;/span&gt;&lt;/td&gt;&lt;td&gt;&lt;span&gt;0&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;10&lt;/span&gt;&lt;/td&gt;&lt;td&gt;&lt;span&gt;1852&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;20&lt;/span&gt;&lt;/td&gt;&lt;td&gt;&lt;span&gt;4518&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;30&lt;/span&gt;&lt;/td&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;6006&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;40&lt;/span&gt;&lt;/td&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;8756&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;50&lt;/span&gt;&lt;/td&gt;&lt;td colspan=&quot;1&quot;&gt;&lt;span&gt;10844&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这样的话，如果当要查询offset为x的消息，我们可能没办法查到它的精确位置，但是可以利用二分查找，快速地确定离他最近的那条消息的位置，然后往后多读几条数据就可以读到我们想要的消息了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;比如，当我们要查到offset为33的消息，按照上表，我们可以利用二分查找定位到offset为30的消息所在的位置，然后去对应的log文件中从该位置开始向后读取3条消息，第四条就是我们要找的33。这种方式其实就是在性能和存储空间上的一个折中，很多系统设计时都会面临类似的选择，牺牲时间换空间还是牺牲空间换时间。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;到这里，我们对kafka的整体架构应该有了一个比较清晰的认识了。不过在上面的分析中，我故意隐去了kafka中另一个非常非常重要的点，就是高可用方面的设计。因为这部分内容比较晦涩，会引入很多分布式理论的复杂性，妨碍我们理解kafka的基本模型。在接下来的部分，将着重讨论这个主题。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h4&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. Kafka高可用&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;高可用（HA）对于企业的核心系统来说是至关重要的。因为随着业务的发展，集群规模会不断增大，而大规模集群中总会出现故障，硬件、网络都是不稳定的。当系统中某些节点各种原因无法正常使用时，整个系统可以容忍这个故障，继续正常对外提供服务，这就是所谓的高可用性。对于有状态服务来说，容忍局部故障本质上就是容忍丢数据（不一定是永久，但是至少一段时间内读不到数据）。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;系统要容忍丢数据，最朴素也是唯一的办法就是做备份，让同一份数据复制到多台机器，所谓的&lt;strong&gt;冗余&lt;/strong&gt;，或者说&lt;strong&gt;多副本&lt;/strong&gt;。为此，kafka引入 leader-follower的概念。topic的每个partition都有一个leader，所有对这个partition的读写都在该partition leader所在的broker上进行。partition的数据会被复制到其它broker上，这些broker上对应的partition就是follower:&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.649737302977233&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;571&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRgCCzL7j9gFRVctuRZ1HQKSKLYNLuzNcrPJ2dJOLHEQO0FzaAJz8eHw/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;producer在生产消息时，会直接把消息发送到partition leader上，partition leader把消息写入自己的log中，然后等待follower来拉取数据进行同步。具体交互如下：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.43342036553524804&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1532&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRicZaohVy3YYxiaUKAdFSsoKazkyudhiaCr1xNmficskN2IsWmiaHdOCAV7w/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt; &lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;上图中对producer进行ack的时机非常关键，这直接关系到kafka集群的可用性和可靠性。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;如果producer的数据到达leader并成功写入leader的log就进行ack&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;优点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：不用等数据同步完成，速度快，吞吐率高，可用性高；&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：如果follower数据同步未完成时leader挂了，就会造成数据丢失，可靠性低。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;如果等follower都同步完数据时进行ack&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span/&gt;&lt;span&gt;优点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：当leader挂了之后follower中也有完备的数据，可靠性高；&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;缺点&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：等所有follower同步完成很慢，性能差，容易造成生产方超时，可用性低。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;而具体什么时候进行ack，对于kafka来说是可以根据实际应用场景配置的。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其实kafka真正的数据同步过程还是非常复杂的，本文主要是想讲一讲kafka的一些核心原理，数据同步里面涉及到的很多技术细节，HW epoch等，就不在此一一展开了。最后展示一下kafka的一个全景图：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6414950419527079&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1311&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRI9ib6eu8HO9icvxK1gp70ZAFtnMdTG8QQHQd2jzwFovdqhwUhjDh9k3Q/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;最后对kafka进行一个简要地总结：kafka通过引入partition的概念，让topic能够分散到多台broker上，提高吞吐率。但是引入多partition的代价就是无法保证topic维度的全局顺序性，需要这种特性的场景只能使用单个partition。在内部，每个partition以多个segment文件的方式进行存储，新来的消息append到最新的segment log文件中，并使用稀疏索引记录消息在log文件中的位置，方便快速读取消息。当数据过期时，直接删除过期的segment文件即可。为了实现高可用，每个partition都有多个副本，其中一个是leader，其它是follower，分布在不同的broker上。对partition的读写都在leader所在的broker上完成，follower只会定时地拉取leader的数据进行同步。当leader挂了，系统会选出和leader保持同步的follower作为新的leader，继续对外提供服务，大大提高可用性。在消费端，kafka引入了消费组的概念，每个消费组都可以互相独立地消费topic，但一个partition只能被消费组中的唯一一个消费者消费。消费组通过记录游标，可以实现ACK机制、重复消费等多种特性。除了真正的消息记录在segment中，其它几乎所有meta信息都保存在全局的zookeeper中。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h4&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3. 优缺点&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;（1）优点：kafka的优点非常多&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;高性能：单机测试能达到 100w tps；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;低延时：生产和消费的延时都很低，e2e的延时在正常的cluster中也很低；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;可用性高：replicate + isr + 选举 机制保证；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;工具链成熟：监控 运维 管理 方案齐全；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;生态成熟：大数据场景必不可少 kafka stream.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;（2）不足&lt;/span&gt;&lt;/strong&gt;&lt;span/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;无法弹性扩容：对partition的读写都在partition leader所在的broker，如果该broker压力过大，也无法通过新增broker来解决问题；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;扩容成本高：集群中新增的broker只会处理新topic，如果要分担老topic-partition的压力，需要手动迁移partition，这时会占用大量集群带宽；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;消费者新加入和退出会造成整个消费组rebalance：导致数据重复消费，影响消费速度，增加e2e延迟；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;partition过多会使得性能显著下降：ZK压力大，broker上partition过多让磁盘顺序写几乎退化成随机写。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;在了解了kafka的架构之后，你可以仔细想一想，为什么kafka扩容这么费劲呢？其实这本质上和redis集群扩容是一样的！当redis集群出现热key时，某个实例扛不住了，你通过加机器并不能解决什么问题，因为那个热key还是在之前的某个实例中，新扩容的实例起不到分流的作用。kafka类似，它扩容有两种：新加机器（加broker）以及给topic增加partition。给topic新加partition这个操作，你可以联想一下mysql的分表。比如用户订单表，由于量太大把它按用户id拆分成1024个子表user_order_{0..1023}，如果到后期发现还不够用，要增加这个分表数，就会比较麻烦。因为分表总数增多，会让user_id的hash值发生变化，从而导致老的数据无法查询。所以只能停服做数据迁移，然后再重新上线。kafka给topic新增partition一样的道理，比如在某些场景下msg包含key，那producer就要保证相同的key放到相同的partition。但是如果partition总量增加了，根据key去进行hash，比如 hash(key) % parition_num，得到的结果就不同，就无法保证相同的key存到同一个partition。当然也可以在producer上实现一个自定义的partitioner，保证不论怎么扩partition相同的key都落到相同的partition上，但是这又会使得新增加的partition没有任何数据。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其实你可以发现一个问题，kafka的核心复杂度几乎都在存储这一块。数据如何分片，如何高效的存储，如何高效地读取，如何保证一致性，如何从错误中恢复，如何扩容再平衡……&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;上面这些不足总结起来就是一个词：scalebility。通过直接加机器就能解决问题的系统才是大家的终极追求。Pulsar号称云原生时代的分布式消息和流平台，所以接下来我们看看pulsar是怎么样的。&lt;/span&gt;&lt;/section&gt;&lt;h3/&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;四、Pulsar&lt;/strong&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;br mpa-from-tpl=&quot;t&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;kafka的核心复杂度是它的存储，高性能、高可用、低延迟、支持快速扩容的分布式存储不仅仅是kafka的需求，应该是现代所有系统共同的追求。而apache项目底下刚好有一个专门就是为日志存储打造的这样的系统，它叫bookeeper！&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;有了专门的存储组件，那么实现一个消息系统剩下的就是如何来使用这个存储系统来实现feature了。pulsar就是这样一个”计算-存储 分离“的消息系统：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6466083150984683&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1828&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRLsbVo0uicfxr1H7CB5ia9iatSumVk4iaWX4jJTVkDibDyicwk2bVibdRs2YPA/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt; &lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;pulsar利用bookeeper作为存储服务，剩下的是计算层。这其实是目前非常流行的架构也是一种趋势，很多新型的存储都是这种”存算分离“的架构。比如tidb，底层存储其实是tikv这种kv存储。tidb是更上层的计算层，自己实现sql相关的功能。还有的例子就是很多&quot;持久化&quot;redis产品，大部分底层依赖于rocksdb做kv存储，然后基于kv存储关系实现redis的各种数据结构。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;在pulsar中，broker的含义和kafka中的broker是一致的，就是一个运行的pulsar实例。但是和kafka不同的是，pulsar的broker是无状态服务，它只是一个”API接口层“，负责处理海量的用户请求，当用户消息到来时负责调用bookeeper的接口写数据，当用户要查询消息时从bookeeper中查数据，当然这个过程中broker本身也会做很多缓存之类的。同时broker也依赖于zookeeper来保存很多元数据的关系。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;由于broker本身是无状态的，因此这一层可以非常非常容易地进行扩容，尤其是在k8s环境下，点下鼠标的事儿。至于消息的持久化，高可用，容错，存储的扩容，这些都通通交给bookeeper来解决。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;但就像能量守恒定律一样，系统的复杂性也是守恒的。实现既高性能又可靠的存储需要的技术复杂性，不会凭空消失，只会从一个地方转移到另一个地方。就像你写业务逻辑，产品经理提出了20个不同的业务场景，就至少对应20个if else，不论你用什么设计模式和架构，这些if else不会被消除，只会从从一个文件放到另一个文件，从一个对象放到另一个对象而已。所以那些复杂性一定会出现在bookeeper中，并且会比kafka的存储实现更为复杂。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;但是pulsar存算分离架构的一个好处就是，当我们在学习pulsar时可以有一个比较明确的界限，所谓的concern segregation。只要理解bookeeper对上层的broker提供的API语义，即使不了解bookeeper内部的实现，也能很好的理解pulsar的原理。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;接下来你可以思考一个问题：既然pulsar的broker层是无状态的服务，那么我们是否可以随意在某个broker进行对某个topic的数据生产呢？&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;看起来似乎没什么问题，但答案还是否定的——不可以。为什么呢？想一想，假如生产者可以在任意一台broker上对topic进行生产，比如生产3条消息a b c，三条生产消息的请求分别发送到broker A B C，那最终怎么保证消息按照a b c的顺序写入bookeeper呢？这是没办法保证，只有让a b c三条消息都发送到同一台broker，才能保证消息写入的顺序。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;既然如此，那似乎又回到和kafka一样的问题，如果某个topic写入量特别特别大，一个broker扛不住怎么办？所以pulsar和kafka一样，也有partition的概念。一个topic可以分成多个partition，为了每个partition内部消息的顺序一致，对每个partition的生产必须对应同一台broker。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.48881239242685026&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRx8DxBUEFibBnZ6KVn8kWLUFpDrFYJPic1F1dFYRJeicJE6xcav5KTgDbA/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这里看起来似乎和kafka没区别，也是每个partition对应一个broker，但是其实差别很大。为了保证对partition的顺序写入，不论kafka还是pulsar都要求写入请求发送到partition对应的broker上，由该broker来保证写入的顺序性。然而区别在于，kafka同时会把消息存储到该broker上，而pulsar是存储到bookeeper上。这样的好处是，当pulsar的某台broker挂了，可以立刻把partition对应的broker切换到另一个broker，只要保证全局只有一个broker对topic-partition-x有写权限就行了，本质上只是做一个所有权转移而已，不会有任何数据的搬迁。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;当对partition的写请求到达对应broker时，broker就需要调用bookeeper提供的接口进行消息存储。和kafka一样，pulsar在这里也有segment的概念，而且和kafka一样的是，pulsar也是以segment为单位进行存储的（respect respect respect）。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;为了说清楚这里，就不得不引入一个bookeeper的概念，叫ledger，也就是账本。可以把ledger类比为文件系统上的一个文件，比如在kafka中就是写入到xxx.log这个文件里。pulsar以segment为单位，存入bookeeper中的ledger。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;在bookeeper集群中每个节点叫bookie（为什么集群的实例在kafka叫broker在bookeeper又叫bookie……无所谓，名字而已，作者写了那么多代码，还不能让人开心地命个名啊）。在实例化一个bookeeper的writer时，就需要提供3个参数：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;节点数n：bookeeper集群的bookie数；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;副本数m：某一个ledger会写入到n个bookie中的m个里，也就是说所谓的m副本；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;确认写入数t：每次向ledger写入数据时（并发写入到m个bookie），需要确保收到t个acks，才返回成功。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;bookeeper会根据这三个参数来为我们做复杂的数据同步，所以我们不用担心那些副本啊一致性啊的东西，直接调bookeeper的提供的append接口就行了，剩下的交给它来完成。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzIxMTE0ODU5NQ==&amp;amp;mid=2650245141&amp;amp;idx=2&amp;amp;sn=0f60792baff42d2070b10f045d8557d8&amp;amp;chksm=8f5ae249b82d6b5f486d4f0d861417abc45b46f04b88ad8b95f46f1bdde8fc3c038700d5240b&amp;amp;scene=21#wechat_redirect&quot; textvalue=&quot;你已选中了添加链接的内容&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;1&quot;&gt;&lt;span&gt;&lt;span class=&quot;js_jump_icon h5_image_link&quot; data-positionback=&quot;static&quot;&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;1.0890136327185245&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1247&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRYSEYahLjmuwpSzumw9kXzk6KsWNOEibYZE9H6ll1Rnia3AhzBvEBH3pg/640?wx_fmt=png&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;如上图所示，parition被分为了多个segment，每个segment会写入到4个bookie其中的3个中。比如segment1就写入到了bookie1,2,4中，segment2写入到bookie1,3,4中…&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这其实就相当于把kafka某个partition的segment均匀分布到了多台存储节点上。这样的好处是什么呢？在kafka中某个partition是一直往同一个broker的文件系统中进行写入，当磁盘不够用了，就需要做非常麻烦的扩容+迁移数据的操作。而对于pulsar，由于partition中不同segment可以保存在bookeeper不同的bookies上，当大量写入导致现有集群bookie磁盘不够用时，我们可以快速地添加机器解决问题，让新的segment寻找最合适的bookie（磁盘空间剩余最多或者负载最低等）进行写入，只要记住segment和bookies的关系就好了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.6005326231691078&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;751&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRia0Szp5HCS1tbGnnrE7kic33RZ7Gp1hR3QBVwxANh7oCt6W18icPXzdCg/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;由于partition以segment为粒度均匀的分散到bookeeper上的节点上，这使得存储的扩容变得非常非常容易。这也是Pulsar一直宣称的存算分离架构的先进性的体现：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其实在理解kafka的架构之后再来看pulsar，你会发现pulsar的核心就在于bookeeper的使用以及一些metadata的存储。但是换个角度，正是这个恰当的存储和计算分离的架构，帮助我们分离了关注点，从而能够快速地去学习上手。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h4&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;消费模型&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Pulsar相比于kafka另一个比较先进的设计就是对消费模型的抽象，叫做subscription。通过这层抽象，可以支持用户各种各样的消费场景。还是和kafka进行对比，kafka中只有一种消费模式，即一个或多个partition对一个consumer。如果想要让一个partition对多个consumer，就无法实现了。pulsar通过subscription，目前支持4种消费方式：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.7336561743341404&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1652&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/VY8SELNGe97st7ojaAZGJKvM7VGtg9tRxZbeXDyFtXWiaaB51rACO7wwLxVqicg56ZNagWZCn4rTwb7h21ywm1HA/640?wx_fmt=png&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt; &lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;可以把pulsar的subscription看成kafka的consumer group，但subscription更进一步，可以设置这个”consumer group“的消费类型：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;exclusive：消费组里有且仅有一个consumer能够进行消费，其它的根本连不上pulsar；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;failover：消费组里的每个消费者都能连上每个partition所在的broker，但有且仅有一个consumer能消费到数据。当这个消费者崩溃了，其它的消费者会被选出一个来接班；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;shared：消费组里所有消费者都能消费topic中的所有partition，消息以round-robin的方式来分发；&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;key-shared：消费组里所有消费者都能消费到topic中所有partition，但是带有相同key的消息会保证发送给同一个消费者。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这些消费模型可以满足多种业务场景，用户可以根据实际情况进行选择。通过这层抽象，pulsar既支持了queue消费模型，也支持了stream消费模型，还可以支持其它无数的消费模型（只要有人提pr），这就是pulsar所说的统一了消费模型。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其实在消费模型抽象的底下，就是不同的cursor管理逻辑。怎么ack，游标怎么移动，怎么快速查找下一条需要重试的msg……这都是一些技术细节，但是通过这层抽象，可以把这些细节进行隐藏，让大家更关注于应用。&lt;/span&gt;&lt;/section&gt;&lt;h3/&gt;&lt;section data-mpa-template=&quot;t&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot; mpa-from-tpl=&quot;t&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;五、存算分离架构&lt;/strong&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;br mpa-from-tpl=&quot;t&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;其实技术的发展都是螺旋式的，很多时候你会发现最新的发展方向又回到了20年前的技术路线了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;在20年前，由于普通计算机硬件设备的局限性，对大量数据的存储是通过NAS(Network Attached Storage)这样的“云端”集中式存储来完成。但这种方式的局限性也很多，不仅需要专用硬件设备，而且最大的问题就是难以扩容来适应海量数据的存储。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;数据库方面也主要是以Oracle小型机为主的方案。然而随着互联网的发展，数据量越来越大，Google后来又推出了以普通计算机为主的分布式存储方案，任意一台计算机都能作为一个存储节点，然后通过让这些节点协同工作组成一个更大的存储系统，这就是HDFS。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;然而移动互联网使得数据量进一步增大，并且4G 5G的普及让用户对延迟也非常敏感，既要可靠，又要快，又要可扩容的存储逐渐变成了一种企业的刚需。而且随着时间的推移，互联网应用的流量集中度会越来越高，大企业的这种刚需诉求也越来越强烈。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;因此，可靠的分布式存储作为一种基础设施也在不断地完善。它们都有一个共同的目标，就是让你像使用filesystem一样使用它们，并且具有高性能高可靠自动错误恢复等多种功能。然而我们需要面对的一个问题就是CAP理论的限制，线性一致性（C），可用性（A），分区容错性（P），三者只能同时满足两者。因此不可能存在完美的存储系统，总有那么一些“不足”。我们需要做的其实就是根据不同的业务场景，选用合适的存储设施，来构建上层的应用。这就是pulsar的逻辑，也是tidb等newsql的逻辑，也是未来大型分布式系统的基本逻辑，所谓的“云原生”。&lt;/span&gt;&lt;/section&gt;&lt;p&gt;最后给大家一个小福利：公众号“&lt;span&gt;&lt;strong&gt;数据社&lt;/strong&gt;&lt;/span&gt;”后台回复“&lt;span&gt;资料&lt;/span&gt;”，即可直接下载！&lt;/p&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzI4MzE4MjQxOQ==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/z2DApiaibzMicibStrZbEKicUIkEia0iaUrqcr4dq64JSoTr0gAkQE7QESQcYrsNqGvdQWrNUZKUz9zjO2WxEAKD5J5GQ/0?wx_fmt=png&quot; data-nickname=&quot;数据社&quot; data-alias=&quot;DataClub&quot; data-signature=&quot;我是一哥，在这里和大家分享大数据实践的那些事，个人专注MPP数据库研究、流处理计算、数据仓库架构和数据分析~&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.4748743718592965&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/z2DApiaibzMicicdgom68IKhbABvo81oQJ70jZSTqdvm3Sg93kRVrHcMBOn2aLJMxrrRKq1MZ3JiaFgeK63OvZR4LOQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;398&quot;/&gt;&lt;/p&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;欢迎大家加我微信好友，交个朋友，有需要的可以拉你进大数据交流群!&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.98125&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/z2DApiaibzMicicdgom68IKhbABvo81oQJ70pzEewwUJTFFd2hS7PqDeiaZrticGr38p3ia7GmtgGH24iaaJU7EX7avHcg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;320&quot;/&gt;&lt;/p&gt;&lt;section data-tools=&quot;135编辑器&quot; data-id=&quot;97836&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;历史好文推荐&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;ol class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368040&amp;amp;idx=1&amp;amp;sn=efd0bc69072a95ca8634906e900a7bfd&amp;amp;chksm=f39011d4c4e798c225bd6cb3fb2b0a125bc324c65eb58c94ca14bf8eba54f90bde7a3a84f694&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;数据人上班划水都聊什么？&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368814&amp;amp;idx=1&amp;amp;sn=7a615d9170254e5116fd8e5647a762d0&amp;amp;chksm=f3901cd2c4e795c4fc7082d70a3b18932eb5663500a1c8ecfeb8583ede266e2a81685eaa4660&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;11&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;网易实时数仓1.0 2.0 3.0 架构&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368773&amp;amp;idx=1&amp;amp;sn=47e1c5769b785665730a0d30f610d4cd&amp;amp;chksm=f3901cf9c4e795efde84a2a00d4d411ed7055a644aa90e174875cd823b9bf5eafe381d21c3ff&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;11&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;美团OneData探索之路&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368737&amp;amp;idx=1&amp;amp;sn=90c508bd195621ecd97eb9d9f16a1e0b&amp;amp;chksm=f3901c9dc4e7958b4dd0d6e05843c93958997a1ec21784d95a5f2b74d1c52b42912a9e802f62&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;收藏，Spark调优笔记&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4MzE4MjQxOQ==&amp;amp;mid=2649368718&amp;amp;idx=1&amp;amp;sn=688683fe3456b967c23d88b087ffbbe5&amp;amp;chksm=f3901cb2c4e795a426029db7a32b02d5cb7835740ec1e781cdb251ae32593df81d51b69af094&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;11&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;美团配送数据治理实践&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;最后，一哥最近也开通了知识星球，现在免费就可以加入，欢迎大家进入星球和一哥深度探讨一些问题！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5493333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/z2DApiaibzMic9SCSCNTDPvJTPRlFh1eO7MZS531nbKy7V6qay8Jwv8k8RR4fR3wvv5UcciaMNhfiaJ6etTGcaYxiaUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1125&quot;/&gt;&lt;/p&gt;&lt;section mpa-from-tpl=&quot;t&quot;&gt;&lt;section data-role=&quot;paragraph&quot;&gt;&lt;pre&gt;&lt;pre&gt;&lt;pre&gt;&lt;pre&gt;&lt;figure&gt;&lt;span/&gt;&lt;/figure&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/pre&gt;&lt;/section&gt;&lt;/section&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>84d9c2dd1b7dbde633a9b02db1c4b141</guid>
<title>CPU 和 GPU：异构计算的演进与发展</title>
<link>https://toutiao.io/k/yk8may5</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;article-content&quot;&gt;&lt;p&gt;世界上大多数事物的发展规律是相似的，在最开始往往都会出现相对通用的方案解决绝大多数的问题，随后会出现为某一场景专门设计的解决方案，这些解决方案不能解决通用的问题，但是在某些具体的领域会有极其出色的表现。而在计算领域中，CPU（Central Processing Unit）和 GPU（Graphics Processing Unit）分别是通用的和特定的方案，前者可以提供最基本的计算能力解决几乎所有问题，而后者在图形计算和机器学习等领域内表现优异。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/cpu-gpu-difference-2021-04-10-16180243330380.jpg&quot; alt=&quot;cpu-gpu-difference&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 1 - CPU 和 GPU&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;异构计算是指系统同时使用多种处理器或者核心，这些系统通过增加不同的协处理器（Coprocessors）提高整体的性能或者资源的利用率&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;，这些协处理器可以负责处理系统中特定的任务，例如用来渲染图形的 GPU 以及用来挖矿的 ASIC 集成电路。&lt;/p&gt;&lt;p&gt;中心处理单元（Central Processing Unit、CPU）&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;一词诞生于 1955 年，已经诞生 70 多年的 CPU 在今天已经是很成熟的技术了，不过它虽然能够很好地处理通用的计算任务，但是因为核心数量的限制在图形领域却远远不如图形处理单元（Graphics Processing Unit、GPU）&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;，复杂的图形渲染、全局光照等问题仍然需要 GPU 来解决，而大数据、机器学习和人工智能等技术的发展也推动着 GPU 的演进。&lt;/p&gt;&lt;p&gt;今天的软件工程师，尤其是数据中心和云计算的工程师因为异构计算的发展面对着更加复杂的场景，我们在这篇文章中主要谈一谈 CPU 和 GPU 的演进过程，重新回顾一下在过去几十年的时间里，工程师为它们增加了哪些有趣的功能。&lt;/p&gt;&lt;h2 id=&quot;cpu&quot;&gt;CPU&lt;/h2&gt;&lt;p&gt;更高、更快和更强是人类永恒的追求，在科技上的进步也不例外，CPU 的主要演进方向其实只有一个：消耗最少的能源实现最快的计算速度，无数工程师的工作都是为了实现这个看起来简单的目的。然而在 CPU 已经逐渐成熟的今天，想要提高它的性能需要花费极大的努力，我们在这一节简单展示历史上引入了哪些技术来提高 CPU 的性能。&lt;/p&gt;&lt;h3 id=&quot;制程&quot;&gt;制程&lt;/h3&gt;&lt;p&gt;当我们讨论 CPU 的发展时，制程（Fabrication Process）&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;是绕不开的关键字，相信不了解计算机的人也都听说过 Intel 处理器 10nm、7nm 的制程，而目前各个 CPU 制造厂商也都有各自的路线图来实现更小的制程，例如台积电准备在 2022 和 2023 年分别实现 3nm 和 2nm 的制造工艺。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/intel-cpu-architecture-2021-04-10-16180243330412.jpg&quot; alt=&quot;intel-cpu-architecture&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 2 - Intel CPU 制程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在大多数人眼中，仿佛 CPU 的制程越少就越先进，性能也会越好，但是制程并不是衡量 CPU 性能的标准，最起码制程的演进不会直接提高 CPU 的性能。工艺制程的每次提升，都可以让我们在单位面积内容纳更多的晶体管（Transistor），只有越多的晶体管才意味着越强的性能。&lt;/p&gt;&lt;p&gt;越小的晶体管在开关时消耗的能量越少，既然晶体管需要一些时间充电和放电，那么消耗的能量也就越少，速度也越快，而这也解释了为什么增加 CPU 的电压可以提高它的运行速度。除此之外，更小的晶体管间隔使得信号的传输变得更快，这也能够加快 CPU 的处理速度&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;&lt;h3 id=&quot;缓存&quot;&gt;缓存&lt;/h3&gt;&lt;p&gt;缓存也是 CPU 的重要组成部分，它能够减少 CPU 访问内存所需要的时间，相信很多开发者都看过如下所示的表格，我们可以看到从 CPU 的一级缓存中读取数据大约是主存的 200 倍，哪怕是二级缓存也有将近 15 倍的提升：&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Work&lt;/th&gt;&lt;th&gt;Latency&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;L1 cache reference&lt;/td&gt;&lt;td&gt;0.5 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Branch mispredict&lt;/td&gt;&lt;td&gt;5 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;L2 cache reference&lt;/td&gt;&lt;td&gt;7 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Mutex lock/unlock&lt;/td&gt;&lt;td&gt;25 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Main memory reference&lt;/td&gt;&lt;td&gt;100 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Compress 1K bytes with Zippy&lt;/td&gt;&lt;td&gt;3,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Send 1K bytes over 1 Gbps network&lt;/td&gt;&lt;td&gt;10,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read 4K randomly from SSD*&lt;/td&gt;&lt;td&gt;150,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read 1 MB sequentially from memory&lt;/td&gt;&lt;td&gt;250,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Round trip within same datacenter&lt;/td&gt;&lt;td&gt;500,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read 1 MB sequentially from SSD*&lt;/td&gt;&lt;td&gt;1,000,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Disk seek&lt;/td&gt;&lt;td&gt;10,000,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read 1 MB sequentially from disk&lt;/td&gt;&lt;td&gt;20,000,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Send packet CA-&amp;gt;Netherlands-&amp;gt;CA&lt;/td&gt;&lt;td&gt;150,000,000 ns&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;表 1 - 2012 年延迟数字对比&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;今天的 CPU 一般都包含 L1、L2 和 L3 三级缓存，CPU 访问这些缓存的速度仅次于访问寄存器，虽然缓存的速度很快，但是因为高性能需要保证尽可能靠近 CPU，所以它的成本异常昂贵。Intel 等 CPU 厂商也会通过增加 CPU 缓存的方式提高性能，更大的 CPU 缓存意味着更高的缓存命中率，也意味着更快的速度。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/cpu-cache-2021-04-10-16180243330419.png&quot; alt=&quot;cpu-cache&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 3 - CPU 缓存&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Intel 的处理器就在过去几十年的时间中不断增加 L1、L2 和 L3 的缓存大小、将 L1 和 L2 缓存集成在 CPU 中以提高访问速度并在 L1 缓存中区分数据缓存和指令缓存以提高缓存的命中率。今天的 Core i9 处理器每个核心都有 64 KB 的 L1 缓存和 256 KB 的 L2 缓存，所有的 CPU 还会共享 16 MB 的 L3 缓存&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;&lt;h3 id=&quot;并行计算&quot;&gt;并行计算&lt;/h3&gt;&lt;p&gt;多线程编程在今天几乎已经是工程师的必修课了，主机上越来越多的 CPU 核心让工程师不得不去思考如何才能通过多线程尽可能利用硬件的潜力，很多人可能都认为 CPU 会按照编写的程序串行执行命令，但是真正的现实往往比这复杂得多，早在很多年前嵌入式工程师就开始尝试在单个 CPU 上并行执行指令。&lt;/p&gt;&lt;p&gt;从软件工程师的角度，我们确实可以认为每一条汇编指令都是原子操作，而原子操作意味着该操作要么处于未执行的状态，要么处于已执行的状态，而数据库事务、日志以及并发控制都建立在原子操作上。不过如果再次放大指令的执行过程，我们会发现指令执行的过程并不是原子的：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/basic-5-stage-2021-04-10-16180243330424.png&quot; alt=&quot;basic-5-stage&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 4 - 指令执行的步骤&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;不同机器架构执行指令的过程会有所差别，上面是经典的精简指令集架构（RISC）中命令执行需要经过的 5 个步骤，其中包括获取指令、解码指令、执行、访问内存以及写回寄存器。&lt;/p&gt;&lt;p&gt;超标量处理器是可以实现指令级别并行的 CPU，它通过向处理器上的其他执行单元派发指令在一个时钟周期内同时执行多条指令&lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;，这里的执行单元是 CPU 内的资源，例如算术逻辑单元、浮点数单元等&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;&lt;p&gt;超标量设计意味着处理器会在一个时钟周期发出多条指令，该技术往往都与指令流水线一起使用&lt;sup id=&quot;fnref:10&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;，流水线会将执行拆分成多个步骤，而处理器的不同部分会分别负责这些步骤的处理，例如：因为指令的获取和解码由不同的执行单元处理，所以它们可以并行执行。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/superscalar-pipeline-2021-04-10-16180243330430.png&quot; alt=&quot;superscalar-pipeline&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 5 - 超标量和流水线&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了超标量和流水线技术之外，嵌入式工程师们还引入了乱序执行以及分支预测等更加复杂的技术，其中乱序执行也被称作动态执行，因为 CPU 执行指令时需要先将数据加载到寄存器中，所以我们分析 CPU 的寄存器操作确定哪些指令可以乱序执行。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/out-of-order-execution-2021-04-10-16180243330435.png&quot; alt=&quot;out-of-order-execution&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 6 - 乱序执行&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如上图所示，其中包含 &lt;code&gt;R1 = R0 + R1&lt;/code&gt;、&lt;code&gt;R2 = R1 - R0&lt;/code&gt; 和 &lt;code&gt;R3 = R3 + R5&lt;/code&gt; 三条指令，其中第三条指令使用的两个寄存器与前两条无关，所以该指令可以与前两条指令并行执行，也就能减少这段代码执行所需要的时间。&lt;/p&gt;&lt;p&gt;因为分支条件是程序中的常见逻辑，当我们在 CPU 的执行中引入流水线和乱序执行之后，如果遇到条件分支仍然需要等待分支确定才继续执行后面的代码，那么处理器可能会浪费很多时钟周期等待条件的确定。在计算机架构中，分支预测器是用来在分支确定前预判的数字电路，在遇到条件跳转指令时，它会预测条件的执行结果并选择分支执行&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如果预判正确，可以节约等待所需要的时钟周期，提高 CPU 的利用率；&lt;/li&gt;&lt;li&gt;如果预判失败，需要丢弃预判执行的全部或者部分结果，重新执行正确的分支；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为预判失败需要付出较大的代价，一般在 10 ~ 20 个时钟周期之间，所以如何提高分支预测器的准确率成为了比较重要的课题，常见的实现包括静态分支预测、动态分支预测和随机分支预测等。&lt;/p&gt;&lt;p&gt;上面的这些指令级并行仅仅存在于实现细节中，&lt;strong&gt;CPU 的使用者在外界观察时仍然会得到串行执行的观察结果&lt;/strong&gt;，所以工程师可以认为 CPU 是能够串行执行指令的黑箱。想要充分利用多个 CPU 的资源，仍然需要工程师理解多线程模型并掌握操作系统中一些并发控制机制。&lt;/p&gt;&lt;p&gt;单核的超标量处理器一般被分类为单指令单数据流（Single Instruction stream, Single Data stream、SISD）处理器，而如果处理器支持向量操作，就被分为单指令多数据流（Single Instruction stream, Multiple Data streams、SIMD）处理器，而 CPU 厂商会引入 SIMD 指令来提高 CPU 的处理能力。&lt;/p&gt;&lt;h3 id=&quot;片内布局&quot;&gt;片内布局&lt;/h3&gt;&lt;p&gt;前端总线是 Intel 在 1990 年在芯片中使用的通信接口，AMD 在 CPU 中也引入了类似的接口，它们的作用都是在 CPU 和内存控制器中心（也被称作北桥）之间传递数据。前端总线在刚设计时不仅灵活，而且成本很低，但是这种设计很难支持芯片中越来越多的 CPU：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/chipset-layout-2021-04-10-16180243330441.png&quot; alt=&quot;chipset-layout&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 6 - 常见芯片布局&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果 CPU 不能从主存中快速获取指令和数据，那么它会花费大量的时间等待读写主存中的数据，所以越高端的处理器越需要高带宽和低延迟，而速度较慢的前端总线无法满足这样的需求。Intel 和 AMD 分别引入了点对点连接的 HyperTransport 和 QuickPath Interconnect（QPI）机制解决这个问题，上图中的南桥被新的传输机制取代了，CPU 通过集成在内部的内存控制访问内存，通过 QPI 连接其他 CPU 以及 I/O 控制器。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/intel-quick-path-interface-2021-04-10-16180243330447.png&quot; alt=&quot;intel-quick-path-interface&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 7 - Intel QPI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;使用 QPI 让 CPU 直接连接其他组件确实可以提高效率，但是随着 CPU 核心数量的增加，这种连接的方式限制了核心的数量，所以 Intel 在 Sandy Bridge 微架构中引入了如下所示的环形总线（Ring Bus）&lt;sup id=&quot;fnref:12&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/ringbus-2021-04-10-16180243330455.jpg&quot; alt=&quot;ringbus&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 8 - 环形总线&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Sandy Bridge 在架构中引入了片内的 GPU 和视频解码器，这些组件也需要与 CPU 共享 L3 缓存，如果所有的组件都与 L3 缓存直接连接，那么片内会出现大量的连接，而这是芯片工程师不能接受的。片内环形总线连接了 CPU、GPU、L3 缓存、PCIe 控制器、DMI 和内存等部分，其中包含四个功能各异的环：数据、请求、确认和监听&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;，这种设计减少了不同组件内部的连接同时也具有较好的可扩展性。&lt;/p&gt;&lt;p&gt;然而随着 CPU 核心数量的继续增加，环形的连接会不断变大，这会增加环的大小进而影响整个环上组件之间的访问延迟，导致该设计遇到瓶颈。Intel 由此引入了一种新的网格微架构（Mesh Interconnect Architecture）&lt;sup id=&quot;fnref:14&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/intel-mesh-overview-2021-04-10-16180243330463.png&quot; alt=&quot;intel-mesh-overview&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 9 - 网格架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如上所示，Intel 的 Mesh 架构是一个二维的 CPU 阵列，网络中有两种不同的组件，一种是上图中蓝色的 CPU 核心，另一种是上图中黄色的集成内存控制器，这些组件不会直接相连，相邻的模块会通过聚合网格站（Converged Mesh Stop、CMS）连接，这与我们今天看到的服务网格非常相似。&lt;/p&gt;&lt;p&gt;当不同组件需要传输数据时，数据包会由 CMS 负责传输，先纵向路由后水平路由，数据到达目标组件后，CMS 会将数据传给 CPU 或者集成的内存控制器。&lt;/p&gt;&lt;h2 id=&quot;gpu&quot;&gt;GPU&lt;/h2&gt;&lt;p&gt;图形处理单元（Graphics Processing Unit、GPU）是在缓冲区中快速操作和修改内存的专用电路，因为可以加速图片的创建和渲染，所以在嵌入式系统、移动设备、个人电脑以及工作站等设备上应用都很广泛&lt;sup id=&quot;fnref:15&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;。然而随着机器学习和大数据的发展，很多公司都会使用 GPU 加速训练任务的执行，这也是今天数据中心中比较常见的用例。&lt;/p&gt;&lt;p&gt;大多数的 CPU 不仅期望在尽可能短的时间内更快地完成任务以降低系统的延迟，还需要在不同任务之间快速切换保证实时性，正是因为这样的需求，CPU 往往都会串行地执行任务。GPU 的设计与 CPU 完全不同，它期望提高系统的吞吐量，在同一时间竭尽全力处理更多的任务，而设计理念上的差异最终反映到了 CPU 和 GPU 的核心数量上&lt;sup id=&quot;fnref:16&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/cpu-vs-gpu-cores-2021-04-10-16180243330471.png&quot; alt=&quot;cpu-vs-gpu-cores&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 10 - CPU 和 GPU 的核心&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;虽然 GPU 在过去几十年的时间有着很大的发展，但是不同 GPU 的架构大同小异，我们在这里简单介绍下面的流式多处理器中不同组件的作用：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/fermi-architecture-2021-04-10-16180243330480.png&quot; alt=&quot;fermi-architecture&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 11 - 流式多处理器&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;流式多处理器（Streaming Multiprocessor、SM）是 GPU 的基本单元，每个 GPU 都由一组 SM 构成，SM 中最重要的结构就是计算核心 Core，上图中的 SM 包含以下组成部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;线程调度器（Warp Scheduler）：线程束（Warp）是最基本的单元，每个线程束中包含 32 个并行的线程，它们使用不同的数据执行相同的命令，调度器会负责这些线程的调度；&lt;/li&gt;&lt;li&gt;访问存储单元（Load/Store Queues）：在核心和内存之间快速传输数据；&lt;/li&gt;&lt;li&gt;核心（Core）：GPU 最基本的处理单元，也被称作流处理器（Streaming Processor），每个核心都可以负责整数和单精度浮点数的计算；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除了上述这些组件之外，SM 中还包含特殊函数的计算单元（Special Functions Unit、SPU）以及用于存储和缓存数据的寄存器文件（Register File）、共享内存（Shared Memory）、一级缓存和通用缓存。&lt;/p&gt;&lt;h3 id=&quot;水平扩容&quot;&gt;水平扩容&lt;/h3&gt;&lt;p&gt;与 CPU 一样，增加架构中的核心数目是提高 GPU 性能和吞吐量最简单粗暴的手段。Fermi&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; 是 Nvidia 早期图形处理器的微架构，在如下所示的架构中，共包含 16 个流式多处理器，512 个 CUDA 核心以及 3,000,000,000 个晶体管：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/nvdia-fermi-architecture-2021-04-10-16180243330490.png&quot; alt=&quot;nvdia-fermi-architecture&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 12 - Nvidia Fermi 架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;除了 512 个 CUDA 核心之外，上述架构中还包含 256 个用于传输数据的访问存储单元和 64 个特殊函数单元。如果我们把 2010 年发布的 Fermi 架构和 2020 年发布的 Ampere 做一个简单的对比，就可以发现两者核心数量的巨大差别：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/nvidia-ampere-architecture-2021-04-10-16180243330511.jpg&quot; alt=&quot;nvidia-ampere-architecture&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 13 - Nvidia Ampere 架构&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Ampere 架构中的流式多处理器增加到了 128 个，而每个处理器中的核心数也增加到了 64 个，整张显卡上一共包含 8,192 个 CUDA 核心，是 Fermi 架构中核心数量的 16 倍。为了提高系统的吞吐量，新的 GPU 架构不只拥有了更多的核心数量，它还需要更大的寄存器、内存、缓存以及带宽满足计算和传输的需求。&lt;/p&gt;&lt;h3 id=&quot;专用核心&quot;&gt;专用核心&lt;/h3&gt;&lt;p&gt;最初的 GPU 仅仅是为了更快地创建和渲染图片，它们广泛存在于个人主机上承担着图像渲染的任务，但是随着机器学习等技术的发展，GPU 中出现了更多种类的专用核心来支撑特定的场景，我们在这里介绍两种 GPU 中存在的专用核心：张量核心（Tensor Core）和光线追踪核心（Ray-Tracing Core）：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/specialized-cores-2021-04-10-16180243330519.png&quot; alt=&quot;specialized-cores&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 14 - 专用核心&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;与个人电脑上的 GPU 不同，数据中心中的 GPU 往往都会用来执行高性能计算和 AI 模型的训练任务。正是因为社区有了类似的需求，Nvidia 才会在 GPU 中加入张量核心（Tensor Core）&lt;sup id=&quot;fnref:18&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;专门处理相关的任务。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/tensor-cores-2021-04-10-16180243330562.gif&quot; alt=&quot;tensor-cores&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 15 - 张量核心&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;张量核心与普通的 CUDA 核心其实有很大的区别，CUDA 核心在每个时钟周期都可以准确的执行一次整数或者浮点数的运算，时钟的速度和核心的数量都会影响整体性能。张量核心通过牺牲一定的精度可以在每个时钟计算执行一次 4 x 4 的矩阵运算，它的引入使得游戏中的实时深度学习任务成为了可能，能够加速度图像的生成和渲染&lt;sup id=&quot;fnref:19&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;&lt;p&gt;计算机图形领域的圣杯是实时的全局光照，实现更好的光线追踪可以帮助我们在屏幕上渲染更加真实的图像，然而全局光照需要 GPU 进行大量的计算，而实时的全局光照更是对性能有着非常高的要求。传统的 GPU 架构并不擅长光线追踪等任务，所以 Nvidia 在 Turing 架构中首次引入了光线追踪核心（Ray-Tracing Core、RT Core）。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/ray-tracing-cores-2021-04-10-16180243330574.jpg&quot; alt=&quot;ray-tracing-cores&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 16 - 光线追踪核心&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Nvidia 的光线追踪核心实际上是为追踪光线设计的特殊电路，光线追踪中比较常见的算法就是 Bounding Volume Hierarchy（BVH）遍历和光线三角形相交测试，使用流式多处理器计算该算法每条光线都会花费上千条指令&lt;sup id=&quot;fnref:20&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;，而光线追踪核心可以加速这一过程。&lt;/p&gt;&lt;h3 id=&quot;多租户&quot;&gt;多租户&lt;/h3&gt;&lt;p&gt;今天 GPU 的性能已经非常强大，但是无论使用数据中心提供的 GPU 实例，还是自己搭建服务器运行计算任务都很昂贵，然而 GPU 算力的拆分在目前仍然是一个比较复杂的问题，运行简单的训练任务可能占用整块 GPU，在这种情况下每提升一点 GPU 的利用率都可以降低一些成本。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img.draveness.me/nvidia-multi-instance-gpu-2021-04-10-16180243330584.jpg&quot; alt=&quot;nvidia-multi-instance-gpu&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;图 17 - 多实例 GPU&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Nvidia 最新的 Ampere 架构支持多实例 GPU（Multi-Instance GPU、MIG）技术，它能够水平切分 GPU 资源&lt;sup id=&quot;fnref:21&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;。每个 A100 GPU 都可以被拆分成 7 个 GPU 实例，每个实例都有隔离的内存、缓存和计算核心，这不仅可以满足数据中心分割 GPU 资源的需要，还能在同一张显卡上并行运行不同的训练任务。&lt;/p&gt;&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;&lt;p&gt;从 CPU 和 GPU 的演进过程我们可以看到，所有的计算单元都受益于更精细的制作工艺，我们尝试在相同的面积内放入更多的晶体管并增加更多的计算单元、使用更大的缓存，当这种『简单粗暴』的方式因为物理上的瓶颈逐渐变得困难时，我们开始为特定领域设计专门的计算单元。&lt;/p&gt;&lt;p&gt;文中没有提到的 ASIC 和 FPGA 是更加特殊的电路，在图像渲染领域之外，我们可以通过设计适用于特定领域的 ASIC 和 FPGA 电路提高某一项任务的性能，OSDI ’20 的最佳论文 hXDP: Efficient Software Packet Processing on FPGA NICs&lt;sup id=&quot;fnref:22&quot;&gt;&lt;a href=&quot;#fn:22&quot; class=&quot;footnote-ref&quot; role=&quot;doc-noteref&quot;&gt;22&lt;/a&gt;&lt;/sup&gt; 就研究了如何使用可编程的 FPGA 更高效地处理数据包的转发，而在未来越来越多的任务会使用专门的硬件。&lt;/p&gt;&lt;h2 id=&quot;推荐阅读&quot;&gt;推荐阅读&lt;/h2&gt;&lt;section class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;&lt;hr/&gt;&lt;ol&gt;&lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Heterogeneous computing &lt;a href=&quot;https://en.wikipedia.org/wiki/Heterogeneous_computing&quot;&gt;https://en.wikipedia.org/wiki/Heterogeneous_computing&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Central processing unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_processing_unit&quot;&gt;https://en.wikipedia.org/wiki/Central_processing_unit&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Graphics processing unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;https://en.wikipedia.org/wiki/Graphics_processing_unit&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Semiconductor device fabrication &lt;a href=&quot;https://en.wikipedia.org/wiki/Semiconductor_device_fabrication&quot;&gt;https://en.wikipedia.org/wiki/Semiconductor_device_fabrication&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Moore’s Law: Why does adding more transistors make computation ‘faster’? &lt;a href=&quot;https://qr.ae/pG8DNr&quot;&gt;https://qr.ae/pG8DNr&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Latency Numbers Every Programmer Should Know &lt;a href=&quot;https://gist.github.com/jboner/2841832&quot;&gt;https://gist.github.com/jboner/2841832&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Intel Core i9-9900K &lt;a href=&quot;https://www.techpowerup.com/cpu-specs/core-i9-9900k.c2098&quot;&gt;https://www.techpowerup.com/cpu-specs/core-i9-9900k.c2098&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Superscalar processor &lt;a href=&quot;https://en.wikipedia.org/wiki/Superscalar_processor&quot;&gt;https://en.wikipedia.org/wiki/Superscalar_processor&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Execution unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Execution_unit&quot;&gt;https://en.wikipedia.org/wiki/Execution_unit&lt;/a&gt; &lt;a href=&quot;#fnref:9&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Instruction pipelining &lt;a href=&quot;https://en.wikipedia.org/wiki/Instruction_pipelining&quot;&gt;https://en.wikipedia.org/wiki/Instruction_pipelining&lt;/a&gt; &lt;a href=&quot;#fnref:10&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Branch predictor &lt;a href=&quot;https://en.wikipedia.org/wiki/Branch_predictor&quot;&gt;https://en.wikipedia.org/wiki/Branch_predictor&lt;/a&gt; &lt;a href=&quot;#fnref:11&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikiwand: Sandy Bridge &lt;a href=&quot;https://www.wikiwand.com/en/Sandy_Bridge&quot;&gt;https://www.wikiwand.com/en/Sandy_Bridge&lt;/a&gt; &lt;a href=&quot;#fnref:12&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Intel’s Sandy Bridge Architecture Exposed &lt;a href=&quot;https://www.anandtech.com/show/3922/intels-sandy-bridge-architecture-exposed/4&quot;&gt;https://www.anandtech.com/show/3922/intels-sandy-bridge-architecture-exposed/4&lt;/a&gt; &lt;a href=&quot;#fnref:13&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Mesh Interconnect Architecture - Intel &lt;a href=&quot;https://en.wikichip.org/wiki/intel/mesh_interconnect_architecture&quot;&gt;https://en.wikichip.org/wiki/intel/mesh_interconnect_architecture&lt;/a&gt; &lt;a href=&quot;#fnref:14&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Graphics processing unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphics_processing_unit&quot;&gt;https://en.wikipedia.org/wiki/Graphics_processing_unit&lt;/a&gt; &lt;a href=&quot;#fnref:15&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Exploring the GPU Architecture &lt;a href=&quot;https://nielshagoort.com/2019/03/12/exploring-the-gpu-architecture/&quot;&gt;https://nielshagoort.com/2019/03/12/exploring-the-gpu-architecture/&lt;/a&gt; &lt;a href=&quot;#fnref:16&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:17&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Wikipedia: Fermi (microarchitecture) &lt;a href=&quot;https://en.wikipedia.org/wiki/Fermi_(microarchitecture)&quot;&gt;https://en.wikipedia.org/wiki/Fermi_(microarchitecture)&lt;/a&gt; &lt;a href=&quot;#fnref:17&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:18&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Nvidia Tensor Cores &lt;a href=&quot;https://www.nvidia.com/en-sg/data-center/tensor-cores/&quot;&gt;https://www.nvidia.com/en-sg/data-center/tensor-cores/&lt;/a&gt; &lt;a href=&quot;#fnref:18&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:19&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;What On Earth Is A Tensorcore? &lt;a href=&quot;https://towardsdatascience.com/what-on-earth-is-a-tensorcore-bad6208a3c62&quot;&gt;https://towardsdatascience.com/what-on-earth-is-a-tensorcore-bad6208a3c62&lt;/a&gt; &lt;a href=&quot;#fnref:19&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:20&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;What you need to know about ray tracing and NVIDIA’s Turing architecture &lt;a href=&quot;https://www.hardwarezone.com.sg/feature-what-you-need-know-about-ray-tracing-and-nvidias-turing-architecture/rt-cores-and-tensor-cores&quot;&gt;https://www.hardwarezone.com.sg/feature-what-you-need-know-about-ray-tracing-and-nvidias-turing-architecture/rt-cores-and-tensor-cores&lt;/a&gt; &lt;a href=&quot;#fnref:20&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:21&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;Nvidia Multi-Instance GPU &lt;a href=&quot;https://www.nvidia.com/en-sg/technologies/multi-instance-gpu/&quot;&gt;https://www.nvidia.com/en-sg/technologies/multi-instance-gpu/&lt;/a&gt; &lt;a href=&quot;#fnref:21&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&quot;fn:22&quot; role=&quot;doc-endnote&quot;&gt;&lt;p&gt;hXDP: Efficient Software Packet Processing on FPGA NICs &lt;a href=&quot;https://www.usenix.org/system/files/osdi20-brunella.pdf&quot;&gt;https://www.usenix.org/system/files/osdi20-brunella.pdf&lt;/a&gt; &lt;a href=&quot;#fnref:22&quot; class=&quot;footnote-backref&quot; role=&quot;doc-backlink&quot;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;&lt;img src=&quot;https://img.draveness.me/2020-03-11-15839264230785-wechat-qr-code.png&quot; alt=&quot;wechat-account-qrcode&quot;/&gt;&lt;h3&gt;转载申请&lt;/h3&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;&lt;img alt=&quot;知识共享许可协议&quot; src=&quot;https://img.draveness.me/creative-commons.png&quot;/&gt;&lt;/a&gt;&lt;p&gt;本作品采用&lt;/p&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;知识共享署名 4.0 国际许可协议&lt;/a&gt;&lt;p&gt;进行许可，转载时请注明原文链接，图片在使用时请保留全部内容，可适当缩放并在引用处附上图片所在的文章链接。&lt;/p&gt;&lt;h3&gt;文章图片&lt;/h3&gt;&lt;p&gt;你可以在 &lt;/p&gt;&lt;a href=&quot;/sketch-and-sketch&quot;&gt;技术文章配图指南&lt;/a&gt;&lt;p&gt; 中找到画图的方法和素材。&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>5f0a64e942f9bfab0722e6ef0ef20dd3</guid>
<title>一文带你更方便的控制 goroutine</title>
<link>https://toutiao.io/k/ye0n3l3</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot;&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;上一篇我们讲了 &lt;code&gt;go-zero&lt;/code&gt; 中的并发工具包 &lt;code&gt;core/syncx&lt;/code&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;从整体分析来看，并发组件主要通过 &lt;code&gt;channel + mutex&lt;/code&gt; 控制程序中协程之间沟通。&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;Do not communicate by sharing memory; instead, share memory by communicating.&lt;/p&gt;&lt;p&gt;不要通过共享内存来通信，而应通过通信来共享内存。&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;本篇来聊 &lt;code&gt;go-zero&lt;/code&gt; 对 Go 中 &lt;code&gt;goroutine&lt;/code&gt; 支持的并发组件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们回顾一下，go原生支持的 &lt;code&gt;goroutine&lt;/code&gt; 控制的工具有哪些？&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;go func()&lt;/code&gt; 开启一个协程&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;sync.WaitGroup&lt;/code&gt; 控制多个协程任务编排&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;sync.Cond&lt;/code&gt; 协程唤醒或者是协程等待&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那可能会问 &lt;code&gt;go-zero&lt;/code&gt; 为什么还要拿出来讲这些？回到 &lt;code&gt;go-zero&lt;/code&gt; 的设计理念：&lt;strong&gt;工具大于约定和文档&lt;/strong&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那么就来看看，&lt;code&gt;go-zero&lt;/code&gt; 提供哪些工具？&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;threading&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;虽然 &lt;code&gt;go func()&lt;/code&gt; 已经很方便，但是有几个问题：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;如果协程异常退出，无法追踪异常栈&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;某个异常请求触发panic，应该做故障隔离，而不是整个进程退出，容易被攻击&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们看看 &lt;code&gt;core/threading&lt;/code&gt; 包提供了哪些额外选择：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;GoSafe&lt;/span&gt;&lt;span&gt;(fn &lt;span&gt;func&lt;/span&gt;()&lt;/span&gt;)&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;go&lt;/span&gt; RunSafe(fn)&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;RunSafe&lt;/span&gt;&lt;span&gt;(fn &lt;span&gt;func&lt;/span&gt;()&lt;/span&gt;)&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;defer&lt;/span&gt; rescue.Recover()&lt;br/&gt; fn()&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;Recover&lt;/span&gt;&lt;span&gt;(cleanups ...&lt;span&gt;func&lt;/span&gt;()&lt;/span&gt;)&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;for&lt;/span&gt; _, cleanup := &lt;span&gt;range&lt;/span&gt; cleanups {&lt;br/&gt;  cleanup()&lt;br/&gt; }&lt;br/&gt;&lt;br/&gt; &lt;span&gt;if&lt;/span&gt; p := &lt;span&gt;recover&lt;/span&gt;(); p != &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;  logx.ErrorStack(p)&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;GoSafe&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;threading.GoSafe()&lt;/code&gt; 就帮你解决了这个问题。开发者可以将自己在协程中需要完成逻辑，以闭包的方式传入，由 &lt;code&gt;GoSafe()&lt;/code&gt; 内部 &lt;code&gt;go func()&lt;/code&gt;；&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当开发者的函数出现异常退出时，会在 &lt;code&gt;Recover()&lt;/code&gt; 中打印异常栈，以便让开发者更快确定异常发生点和调用栈。&lt;/p&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;NewWorkerGroup&lt;span/&gt;&lt;/h3&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们再看第二个：&lt;code&gt;WaitGroup&lt;/code&gt;。日常开发，其实 &lt;code&gt;WaitGroup&lt;/code&gt; 没什么好说的，你需要 &lt;code&gt;N&lt;/code&gt; 个协程协作 ：&lt;code&gt;wg.Add(N)&lt;/code&gt; ，等待全部协程完成任务：&lt;code&gt;wg.Wait()&lt;/code&gt;，同时完成一个任务需要手动 &lt;code&gt;wg.Done()&lt;/code&gt;。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;可以看的出来，在任务开始 -&amp;gt; 结束 -&amp;gt; 等待，整个过程需要开发者关注任务的状态然后手动修改状态。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;NewWorkerGroup&lt;/code&gt;  就帮开发者减轻了负担，开发者只需要关注：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;任务逻辑【函数】&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;任务数【&lt;code&gt;workers&lt;/code&gt;】&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;然后启动 &lt;code&gt;WorkerGroup.Start()&lt;/code&gt;，对应任务数就会启动：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(wg WorkerGroup)&lt;/span&gt; &lt;span&gt;Start&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;  &lt;span&gt;// 包装了sync.WaitGroup&lt;/span&gt;&lt;br/&gt; group := NewRoutineGroup()&lt;br/&gt; &lt;span&gt;for&lt;/span&gt; i := &lt;span&gt;0&lt;/span&gt;; i &amp;lt; wg.workers; i++ {&lt;br/&gt;    &lt;span&gt;// 内部维护了 wg.Add(1) wg.Done()&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;// 同时也是 goroutine 安全模式下进行的&lt;/span&gt;&lt;br/&gt;  group.RunSafe(wg.job)&lt;br/&gt; }&lt;br/&gt; group.Wait()&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;worker&lt;/code&gt; 的状态会自动管理，可以用来固定数量的 &lt;code&gt;worker&lt;/code&gt; 来处理消息队列的任务，用法如下：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;main&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;  group := NewWorkerGroup(&lt;span&gt;&lt;span&gt;func&lt;/span&gt;&lt;span&gt;()&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt;    &lt;span&gt;// process tasks&lt;/span&gt;&lt;br/&gt; }, runtime.NumCPU())&lt;br/&gt; group.Start()&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;Pool&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里的 &lt;code&gt;Pool&lt;/code&gt; 不是 &lt;code&gt;sync.Pool&lt;/code&gt;。&lt;code&gt;sync.Pool&lt;/code&gt; 有个不方便的地方是&lt;strong&gt;它池化的对象可能会被垃圾回收掉&lt;/strong&gt;，这个就让开发者疑惑了，不知道自己创建并存入的对象什么时候就没了。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;go-zero&lt;/code&gt; 中的 &lt;code&gt;pool&lt;/code&gt;：&lt;/p&gt;&lt;ol data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;code&gt;pool&lt;/code&gt; 中的对象会根据使用时间做懒销毁；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;使用 &lt;code&gt;cond&lt;/code&gt; 做对象消费和生产的通知以及阻塞；&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;开发者可以自定义自己的生产函数，销毁函数；&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;那我来看看生产对象，和消费对象在 &lt;code&gt;pool&lt;/code&gt; 中时怎么实现的：&lt;/p&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(p *Pool)&lt;/span&gt; &lt;span&gt;Get&lt;/span&gt;&lt;span&gt;()&lt;/span&gt; &lt;span&gt;interface&lt;/span&gt;&lt;/span&gt;{} {&lt;br/&gt;  &lt;span&gt;// 调用 cond.Wait 时必须要持有c.L的锁&lt;/span&gt;&lt;br/&gt; p.lock.Lock()&lt;br/&gt; &lt;span&gt;defer&lt;/span&gt; p.lock.Unlock()&lt;br/&gt;&lt;br/&gt; &lt;span&gt;for&lt;/span&gt; {&lt;br/&gt;    &lt;span&gt;// 1. pool中对象池是一个用链表连接的nodelist&lt;/span&gt;&lt;br/&gt;  &lt;span&gt;if&lt;/span&gt; p.head != &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;   head := p.head&lt;br/&gt;   p.head = head.next&lt;br/&gt;      &lt;span&gt;// 1.1 如果当前节点：当前时间 &amp;gt;= 上次使用时间+对象最大存活时间&lt;/span&gt;&lt;br/&gt;   &lt;span&gt;if&lt;/span&gt; p.maxAge &amp;gt; &lt;span&gt;0&lt;/span&gt; &amp;amp;&amp;amp; head.lastUsed+p.maxAge &amp;lt; timex.Now() {&lt;br/&gt;    p.created--&lt;br/&gt;        &lt;span&gt;// 说明当前节点已经过期了 -&amp;gt; 销毁节点对应的对象，然后继续寻找下一个节点&lt;/span&gt;&lt;br/&gt;        &lt;span&gt;// 【⚠️：不是销毁节点，而是销毁节点对应的对象】&lt;/span&gt;&lt;br/&gt;    p.destroy(head.item)&lt;br/&gt;    &lt;span&gt;continue&lt;/span&gt;&lt;br/&gt;   } &lt;span&gt;else&lt;/span&gt; {&lt;br/&gt;    &lt;span&gt;return&lt;/span&gt; head.item&lt;br/&gt;   }&lt;br/&gt;  }&lt;br/&gt;  &lt;span&gt;// 2. 对象池是懒加载的，get的时候才去创建对象链表&lt;/span&gt;&lt;br/&gt;  &lt;span&gt;if&lt;/span&gt; p.created &amp;lt; p.limit {&lt;br/&gt;   p.created++&lt;br/&gt;      &lt;span&gt;// 由开发者自己传入：生产函数&lt;/span&gt;&lt;br/&gt;   &lt;span&gt;return&lt;/span&gt; p.create()&lt;br/&gt;  }&lt;br/&gt;  &lt;br/&gt;  p.cond.Wait()&lt;br/&gt; }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;pre data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;&lt;span&gt;&lt;span&gt;func&lt;/span&gt; &lt;span&gt;(p *Pool)&lt;/span&gt; &lt;span&gt;Put&lt;/span&gt;&lt;span&gt;(x &lt;span&gt;interface&lt;/span&gt;{})&lt;/span&gt;&lt;/span&gt; {&lt;br/&gt; &lt;span&gt;if&lt;/span&gt; x == &lt;span&gt;nil&lt;/span&gt; {&lt;br/&gt;  &lt;span&gt;return&lt;/span&gt;&lt;br/&gt; }&lt;br/&gt; &lt;span&gt;// 互斥访问 pool 中nodelist&lt;/span&gt;&lt;br/&gt; p.lock.Lock()&lt;br/&gt; &lt;span&gt;defer&lt;/span&gt; p.lock.Unlock()&lt;br/&gt;&lt;br/&gt; p.head = &amp;amp;node{&lt;br/&gt;  item:     x,&lt;br/&gt;  next:     p.head,&lt;br/&gt;  lastUsed: timex.Now(),&lt;br/&gt; }&lt;br/&gt;  &lt;span&gt;// 放入head，通知其他正在get的协程【极为关键】&lt;/span&gt;&lt;br/&gt; p.cond.Signal()&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;上述就是 &lt;code&gt;go-zero&lt;/code&gt; 对 &lt;code&gt;Cond&lt;/code&gt; 的使用。可以类比 &lt;em&gt;生产者-消费者模型&lt;/em&gt;，只是在这里没有使用 &lt;code&gt;channel&lt;/code&gt; 做通信，而是用 &lt;code&gt;Cond&lt;/code&gt; 。这里有几个特性：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;Cond和一个Locker关联，可以利用这个Locker对相关的依赖条件更改提供保护。&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Cond可以同时支持 &lt;code&gt;Signal&lt;/code&gt; 和 &lt;code&gt;Broadcast&lt;/code&gt; 方法，而 &lt;code&gt;Channel&lt;/code&gt; 只能同时支持其中一种。&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;总结&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;工具大于约定和文档&lt;/strong&gt;，一直是 &lt;code&gt;go-zero&lt;/code&gt; 设计主旨之一；也同时将平时业务沉淀到组件中，这才是框架和组件的意义。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;关于 &lt;code&gt;go-zero&lt;/code&gt; 更多的设计和实现文章，可以持续关注我们。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;项目地址&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://github.com/tal-tech/go-zero&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;欢迎使用 go-zero 并 &lt;strong&gt;star&lt;/strong&gt; 支持我们！&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span/&gt;&lt;span&gt;微信交流群&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/h2&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>