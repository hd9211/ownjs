<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>96fbe25d32bca208be385a3fa9f5274c</guid>
<title>一文带你了解推荐系统之标签体系</title>
<link>https://toutiao.io/k/nyb3xq7</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;weui-dialog&quot;&gt;
      &lt;p class=&quot;weui-dialog__hd&quot;&gt;&lt;strong class=&quot;weui-dialog__title&quot;&gt;&quot;Top Stories&quot; is disabled&lt;/strong&gt;&lt;/p&gt;
      &lt;p class=&quot;weui-dialog__bd&quot;&gt;
        Enable &quot;Top Stories&quot; in &quot;Settings&quot; &amp;gt; &quot;General&quot; &amp;gt; &quot;Manage Discover&quot;      &lt;/p&gt;
      
    &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>8e928ab8df37c1f72a3a48c9d08688c1</guid>
<title>如何构建业务数据分析体系</title>
<link>https://toutiao.io/k/2zcuj5x</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;关注微信公众号：一个数据人的自留地&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;@小宇&lt;/p&gt;&lt;p&gt;专注流量数据分析，就职过360和58。&lt;/p&gt;&lt;p&gt;主要负责流量分析和商业变现等相关数据分析工作。&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;span/&gt;&lt;/span&gt;&lt;/p&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;00&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;BEGIN&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;提及 “体系” 二字，我的脑海里浮现出老板说的 “对于工作的规划要从全局出发，内容要全面、要成体系！” 那么对于一个数据分析师而言我们的体系是什么？&lt;span&gt;是&lt;/span&gt; “目标监控体系？”，是 “运营分析体系？” ，还是 “APP 指标体系？” 到底该如何构建数据分析体系赋能业务呢？今天就来跟大家聊聊体系构建的话题。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;构建业务数据分析体系，对于分析师同学有两个方面的要求：&lt;strong&gt;&lt;span&gt;第一，&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;要了解业务模式&lt;/strong&gt;&lt;/span&gt;，能够解释数据背后的业务含义，找到业务的问题点、提升点，驱动业务向前发展；&lt;strong&gt;&lt;span&gt;第二，不&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;strong&gt;能只做数据、图表的堆砌&lt;/strong&gt;&lt;/span&gt;，需要根据业务的流程链路有目标、有逻辑、有顺序的分模块分专题展现数据。满足这两个方面的要求才是真正意义上的数据分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面结合我的工作场景，给大家讲讲业务数据分析体系搭建的基本思路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;01&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;明确业务逻辑&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分析师同学要明确自己服务的业务是什么？业务逻辑是什么？业务核心是什么？在业务的基础上构建分析体系，才能让分析结果更接地气，更好的应用到业务中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以流量外采业务为例，梳理业务逻辑（如下图）：各业务线发起流量需求 → 多渠道进行流量采买 → 流量引入落地页 → 落地页产生流量转化 → 流量变现、效果转化，这一系列步骤决定采买目标是否达成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.5143229166666666&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQK8dVicwN4CX6Y3PGVWdgqzKyYufW1vhu54NxOJcpAmxlxyicoDabpc2sg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;768&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;02&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;拆解分析模块&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;明确了业务逻辑后，根据目标和事件顺序进行分析模块拆解，明确各个目标分析的专题及关注核心。流量外采业务拆解分析模块如下：&lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.4734982332155477&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQK86cqUgRnvJ5CkqHVRhAC3mG4VoG9icq6XicEW2JFEqicaougnZIuGBmIw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;849&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;采买效率&lt;/strong&gt;&lt;/span&gt;关注核心：有多少预算？现在以什么样的价格买了多少流量？当前出价能否实现目标最大化？预算、价格、采买流量无论调节哪一项，只有三者维持平衡，才能实现流量供给相对稳定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;广告填充&lt;/strong&gt;&lt;/span&gt;直接影响流量变现。因此，在保证广告主预算合理消耗、效果满足预期的前提下，不断提升页面广告填充率，从而提升流量变现效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;用户行为&lt;/strong&gt;&lt;/span&gt;既能够决定收入转化又能够决定效果转化。细致研究收入、效果转化用户在单页面中有哪些行为、访问了几层页面、点—面—点—转化/跳出的访问路径是什么。根据转化用户特征优化产品策略从而帮助业务提升流量转化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以上各个模块的优化目的是为了达成共同的业务目标，&lt;strong&gt;&lt;span&gt;目标达成&lt;/span&gt;&lt;/strong&gt;的数据监控基础且重要。收入、效果、投入产出的数据表现直观的描述了业务现状和目标达成情况，及时的&lt;span&gt;监控&lt;/span&gt;目标达成有助于业务稳定健康发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;03&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;确定分析指标&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;确定了分析模块后，开始选定各个模块的分析指标，指标基本分为：结果量、转化率两类。结果量描述规模和目标达成，转化率描述效果。根据业务路径选取关键节点的转化和重要结果达成作为分析指标。按照路径的先后顺序列出指标，形成了核心数据看板，完成了数据体系的搭建。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于流量外采业务分析模块，可拆解出如下数据看板：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.5452609158679447&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKV0Vnk2GmIAjlSVLgTNO6r54IoOBIjxdWdvcNIbevOZYBrTv26QnVcQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;939&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;04&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;洞察走势与业务同步发展&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了清晰业务结构、模块拆解，数据看板就可以跟踪业务走势。在跟踪的时候，首先关注的是：目标达成情况。目标达成决定了后续一系列行动判断，根据业务走势的波动情况定位异常问题、发现业务提升点。产品、运营同学根据数据结论制定每个阶段的行动计划，同时分析师也要不断变换分析视角与业务联动实现同步发展。&lt;/p&gt;&lt;p&gt;如下示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.5183673469387755&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKX1gNZCN9h0N6ZUgTCnCmoUicS9wlCtTRZicicNzc16MWwWTveoXZcQB5w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;735&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据业务阶段性动作，明确阶段核心，定制专题分析方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;05&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;驱动业务增长&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;驱动业务增长是高阶数据分析要实现的目标之一。想要改善业务，就必须了解业务细节，发现问题，找出关键点，给出科学合理的优化方案，推动方案落地，才能实现业务增长。其中发现问题&lt;span&gt;、找关键点&lt;/span&gt;、优化方案、推动落地都属于数据驱动的范畴。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如流量采买业务中需求与供给匹配的问题：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.47050359712230216&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKicmNXnr0zDKicicCN0OPCia8Tqx0lBm7HCr0uAF2jvScoAWM80xRqKE3kA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;695&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.4943502824858757&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKhylJwcTTBeibfmv4qJSl7bQLJticau9ocwAbxQ2IZaiay9rQicsIz9wb3Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;708&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体应该如何分配流量？这就找到了数据分析在项目中可以为项目实践提供价值的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据规划求解的原理解决业务中流量分配的问题，具体方法参见《&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzIxMzAxNzEwNQ==&amp;amp;mid=2648095269&amp;amp;idx=1&amp;amp;sn=f934858a0e4091ee92e52527d9ebad31&amp;amp;chksm=8f9f2d5bb8e8a44d79af3c47c8db438e2bcb32f6bf72148a8bb08188289e289c33508cb0d0ae&amp;amp;scene=21#wechat_redirect&quot; textvalue=&quot;规划求解应用—预算分配&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;规划求解应用—预算分配&lt;/a&gt;》。&lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.37729357798165136&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKWcNMcp95HzH9nuvibc9hewicZf4iapTZfOLJbMDeEicSgaBxkdMeP34ClQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;872&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;项目实践测试的过程中，分析师需要不断跟进评估实验效果、推全后复盘项目的目标达成和可迭代升级的内容。实现全流程的参与、评估、决策才能称之为数据分析驱动业务增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;06&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;strong&gt;形成数据体系&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建数据分析体系的本质是：满足业务需求，解决业务问题，驱动业务增长。在满足需求、发现问题、解决问题、&lt;span&gt;跟进&lt;/span&gt;项目、落地复盘的过程中分析师同学要不断的提炼总结，进而形成自己的数据分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可以是个思维导图，可以是个表格，也可以是个文档。无论哪种形式只要实现了数据分析体系本质，发挥了它应有的作用&lt;span&gt;，落&lt;/span&gt;&lt;span&gt;在了&lt;/span&gt;&lt;span&gt;具体业务&lt;/span&gt;&lt;span&gt;中&lt;/span&gt;，就是一个优秀的业务数据分析体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到流量采买业务的示例，总结提炼形成的数据体系如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.6572199730094467&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/jWg3EibnYW99HMiaRAAMb2HRjYkyV0woQKian7iciaILAw2twccPZZNkiaaGOtwRibILTtZ8Euz23bEcqicz32dtnFhSNQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;741&quot;/&gt;&lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;现实中，很多分析师同学掌握了专业的统计分析方法、分析工具、算法模型，但在与业务配合的过程中，总是很关注自己的理论深度、难度、专业度，却忽略了与业务的贴合度，因此&lt;span&gt;分析结构&lt;/span&gt;就没有办法形成体系化的分析结构，分析技能也只能停留在初级水平。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用专业的方法服务个性化需求，将分析结果推广至业务中，只要这样才能真正的实现分析师价值，同时你也从初级成长为高级。&lt;/p&gt;&lt;p&gt;&lt;span&gt;希望对你有所启发！&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;img data-ratio=&quot;1&quot; data-w=&quot;258&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/jWg3EibnYW9ibAUWI7stk9Kiaq14L7AJ6C1xJZWuVcLxtgibUBppKquBclraqDj9W3gTKeibZE18BpQXCJE2TianfycA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;数据人交流和学习的社区，关注我们，掌握专业数据知识、结识更多的数据小伙伴。&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;带你探索数据的神奇奥秘&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;svg viewbox=&quot;0 0 1 1&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>b8098f7a4e62d5eaf5f99730867ae331</guid>
<title>用 Pandas 计算日志中事件的时间间隔</title>
<link>https://toutiao.io/k/o73tkeh</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;preview&quot;&gt;&lt;p&gt;&lt;img src=&quot;https://img.toutiao.io/attachment/18c599fbfd924453b429eaabacf6dc21/w600&quot; alt=&quot;file&quot;/&gt;&lt;/p&gt;

&lt;p&gt;通常我们需要对日志数据清洗出一些事件间隔的特征，给数据分析或者机器学习建模提供数据支持。&lt;/p&gt;

&lt;p&gt;假设我们有一些事件日志，这些日志记录了用户在App中事件类型、时间戳、用户ID等其他一些数据，像下面这样:&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;data = &quot;&quot;&quot;user_id,event_time,event_type
100003,2021-01-18 01:06:00,onLaunch
100001,2021-01-19 08:30:00,onLaunch
100003,2021-01-20 02:39:00,onLaunch
100003,2021-01-20 08:15:00,onLaunch
100002,2021-01-20 10:05:00,onLaunch
100001,2021-01-20 14:40:00,onLaunch
100001,2021-01-20 18:05:00,onLaunch
100002,2021-01-21 21:11:00,onLaunch
100003,2021-01-22 10:05:00,onLaunch
100001,2021-01-23 09:18:00,onLaunch
100003,2021-01-23 17:35:00,onLaunch
100001,2021-01-25 16:49:00,onLaunch
100003,2021-01-26 12:13:00,onLaunch
100001,2021-01-27 19:56:00,onLaunch&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们把日志读取到 Pandas 的内存表 DataFrame 中．真实环境可以通过 &lt;code&gt;read_sql&lt;/code&gt; 或者 &lt;code&gt;read_csv&lt;/code&gt; 等方法读取数据源&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;import pandas as pd
ldata = [x.split(&#x27;,&#x27;) for x in data.split(&#x27;\n&#x27;)]
df = pd.DataFrame(ldata[1:], columns=ldata[0])
df[&#x27;event_time&#x27;] =  pd.to_datetime(df[&#x27;event_time&#x27;])
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th&gt;event_type&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;100003&lt;/td&gt;
      &lt;td&gt;2021-01-18 01:06:00&lt;/td&gt;
      &lt;td&gt;onLaunch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;100001&lt;/td&gt;
      &lt;td&gt;2021-01-19 08:30:00&lt;/td&gt;
      &lt;td&gt;onLaunch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;100003&lt;/td&gt;
      &lt;td&gt;2021-01-20 02:39:00&lt;/td&gt;
      &lt;td&gt;onLaunch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;100003&lt;/td&gt;
      &lt;td&gt;2021-01-20 08:15:00&lt;/td&gt;
      &lt;td&gt;onLaunch&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;100002&lt;/td&gt;
      &lt;td&gt;2021-01-20 10:05:00&lt;/td&gt;
      &lt;td&gt;onLaunch&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;我们希望提取出每个用户的启动次数（&lt;code&gt;lifetime_launchs&lt;/code&gt;）、昨日启动次数(&lt;code&gt;yesterday_launchs&lt;/code&gt;) 以及距离上次登录的天数 (&lt;code&gt;days_since_last_launch&lt;/code&gt;)。&lt;/p&gt;

&lt;p&gt;使用 pandas，我们按照用户和日期进行汇总，得到每个用户每天的启动次数。&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs = (df.set_index(&#x27;event_time&#x27;)
               .groupby([&#x27;user_id&#x27;, pd.Grouper(freq=&#x27;D&#x27;)])
               .size()
               .rename(&#x27;launchs&#x27;))
user_launchs
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;user_id  event_time
100001   2021-01-19    1
         2021-01-20    2
         2021-01-23    1
         2021-01-25    1
         2021-01-27    1
100002   2021-01-20    1
         2021-01-21    1
100003   2021-01-18    1
         2021-01-20    2
         2021-01-22    1
         2021-01-23    1
         2021-01-26    1
Name: launchs, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以看出，这里只有有启动的日志，日期不连续，为了提取出上面的特征，我们需要填充没有启动的日期.
把用户放到一个连续的时间轴上.&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;# 创建一个日期范围作为 DataFrame 的索引
dates = pd.date_range(df.event_time.min().date(),
                      df.event_time.max().date(),
                      freq=&#x27;1D&#x27;)

# 取出去重的用户ID
users = df[&#x27;user_id&#x27;].unique()


# 通过用户和日期交叉创建出一个 MultiIndex
idx = pd.MultiIndex.from_product([users, dates], names=[&#x27;user_id&#x27;, &#x27;event_time&#x27;])

# 索引建索引
user_launchs = user_launchs.reindex(idx)

user_launchs.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;user_id  event_time
100003   2021-01-18    1.0
         2021-01-19    NaN
         2021-01-20    2.0
         2021-01-21    NaN
         2021-01-22    1.0
Name: launchs, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样我们构建了一个连续的时间序列．现在可以提取我们需要的特征了.&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;# 为空的数据行把空填充0,并转成 DataFrame
user_launchs = user_launchs.fillna(0).to_frame()
user_launchs.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th&gt;launchs&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th/&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;5&quot; valign=&quot;top&quot;&gt;100003&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过&lt;code&gt;shift&lt;/code&gt;方法向下移动一行作为下一行的前一天，给 user_launchs 增加 &lt;code&gt;launchs_yesterday&lt;/code&gt; 字段&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs[&#x27;launchs_yesterday&#x27;] = user_launchs.groupby(level=&#x27;user_id&#x27;)[&#x27;launchs&#x27;].shift(1)
user_launchs.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th&gt;launchs&lt;/th&gt;
      &lt;th&gt;launchs_yesterday&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th/&gt;
      &lt;th/&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;5&quot; valign=&quot;top&quot;&gt;100003&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;滚动的累计出当前日期下每个用户的总启动次数，给 user_launchs 增加 &lt;code&gt;lifetime_launchs&lt;/code&gt; 字段．&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs[&#x27;lifetime_launchs&#x27;] = (user_launchs
                                  .groupby(level=&#x27;user_id&#x27;)
                                  .launchs.cumsum()
                                  .groupby(level=&#x27;user_id&#x27;).shift(1))
user_launchs.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th&gt;launchs&lt;/th&gt;
      &lt;th&gt;launchs_yesterday&lt;/th&gt;
      &lt;th&gt;lifetime_launchs&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;5&quot; valign=&quot;top&quot;&gt;100003&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs.groupby(level=&#x27;user_id&#x27;).cumsum().groupby([&#x27;user_id&#x27;, &#x27;launchs&#x27;]).cumcount()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;user_id  event_time
100003   2021-01-18    0
         2021-01-19    1
         2021-01-20    0
         2021-01-21    1
         2021-01-22    0
         2021-01-23    0
         2021-01-24    1
         2021-01-25    2
         2021-01-26    0
         2021-01-27    1
100001   2021-01-18    0
         2021-01-19    0
         2021-01-20    0
         2021-01-21    1
         2021-01-22    2
         2021-01-23    0
         2021-01-24    1
         2021-01-25    0
         2021-01-26    1
         2021-01-27    0
100002   2021-01-18    0
         2021-01-19    1
         2021-01-20    0
         2021-01-21    0
         2021-01-22    1
         2021-01-23    2
         2021-01-24    3
         2021-01-25    4
         2021-01-26    5
         2021-01-27    6
dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;距离上次登录的天数 (&lt;code&gt;days_since_last_launch&lt;/code&gt;)&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs[&#x27;days_since_last_launch&#x27;] = (user_launchs
                                        .groupby(level=&#x27;user_id&#x27;)
                                        .cumsum()
                                        .groupby([&#x27;user_id&#x27;, &#x27;launchs&#x27;])
                                        .cumcount()
                                        .groupby(level=&#x27;user_id&#x27;).shift(1))
user_launchs.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th&gt;launchs&lt;/th&gt;
      &lt;th&gt;launchs_yesterday&lt;/th&gt;
      &lt;th&gt;lifetime_launchs&lt;/th&gt;
      &lt;th&gt;days_since_last_launch&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;5&quot; valign=&quot;top&quot;&gt;100003&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;类似的，我们还可以滚动的创建 &lt;code&gt;launchs_last_n_days&lt;/code&gt;&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;for n in [7, 14]: 
    col = &#x27;launchs_last_{}_days&#x27;.format(n)
    user_launchs[col] = (user_launchs
                        .groupby(level=&#x27;user_id&#x27;)
                        .launchs
                        .apply(lambda d: d.rolling(n).sum().shift(1)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;给空值填充 &lt;code&gt;0&lt;/code&gt;, 并格式化为 integer 类型&lt;/p&gt;

&lt;pre lang=&quot;python&quot;&gt;&lt;code&gt;user_launchs.fillna(0).applymap(int)
&lt;/code&gt;&lt;/pre&gt;

&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th&gt;launchs&lt;/th&gt;
      &lt;th&gt;launchs_yesterday&lt;/th&gt;
      &lt;th&gt;lifetime_launchs&lt;/th&gt;
      &lt;th&gt;days_since_last_launch&lt;/th&gt;
      &lt;th&gt;launchs_last_7_days&lt;/th&gt;
      &lt;th&gt;launchs_last_14_days&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;user_id&lt;/th&gt;
      &lt;th&gt;event_time&lt;/th&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
      &lt;th/&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;10&quot; valign=&quot;top&quot;&gt;100003&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-23&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-24&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-25&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-26&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-27&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;10&quot; valign=&quot;top&quot;&gt;100001&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-23&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-24&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-25&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-26&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-27&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&quot;10&quot; valign=&quot;top&quot;&gt;100002&lt;/th&gt;
      &lt;th&gt;2021-01-18&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-19&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-20&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-21&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-22&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-23&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-24&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-25&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-26&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2021-01-27&lt;/th&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;是不是很神奇&lt;/p&gt;
&lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>15fa99d986622da92597511ab7e6abbf</guid>
<title>[译] 编程高手是如何练成的？</title>
<link>https://toutiao.io/k/7c85i29</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;markdown-body detail-content&quot;&gt;&amp;#13;
                &amp;#13;
                &lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;http://download.broadview.com.cn/Original/2102c71e14027d3e8c3a&quot;/&gt;&amp;#13;
&lt;/p&gt;&lt;p&gt;每个人都有成长的渴望，也都会遇到成长的瓶颈。下面这个问题是一个读者问我的：&lt;br/&gt;如何才能训练成为一个编程高手？&amp;#13;
&lt;/p&gt;&lt;p&gt;先简单说下这个读者的背景：工作 3 年多，目前在大厂做后台开发，身边有不少编程高手，是他想要追赶的目标。&lt;br/&gt;最近半年时间，他花了一些精力在研究源码和底层原理上，但总觉得这些知识和实际工作衔接不好，提升很慢，学习动力也越来越差，希望我能给点建议。&lt;br/&gt;我和他语音聊了半个小时，讲了一些我个人以及身边同事的案例。交流下来后，双方都有所启发。所以再次整理下，分享给大家。&amp;#13;
&lt;/p&gt;&lt;p&gt;— 1 —&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;何谓 “编程高手” ？&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;我觉得能分成这样 3 类：&lt;/p&gt;&amp;#13;
&lt;p&gt;第 1 类：天赋和成就都很高的人中龙凤，比如能一个人写出 WPS 的求伯君、一个人开发出电子邮件 Foxmail 的张小龙，对开源领域影响很大的章亦春等人。&lt;/p&gt;&amp;#13;
&lt;p&gt;第 2 类：有不错的口碑同时广为人知的技术大牛，他们一般在知名公司的重要岗位任职过，对技术纯真热爱，同时热衷分享，比如耗子哥、阮一峰、尤雨溪等人。&lt;/p&gt;&amp;#13;
&lt;p&gt;第 3 类：工作中被我们视为崇拜对象、未来有希望企及的高 P 或者架构师。&lt;/p&gt;&amp;#13;
&lt;p&gt;前面两类不在我的圈层射程内（除了跟耗子哥在亚马逊有过一面之缘以外），因此本文主要聊下第 3 类人。这个也是绝大部分读者最关心的、同时可以设定为超越对象的群体。&lt;br/&gt;这样就有了一个新问题：既然想成为第 3 类人，那如何给第 3 类人下一个相对准确且具象的定义呢？（如果目标都是模糊的，就难言超越了）&lt;br/&gt;这其实是一个很有意思的问题，每个人的答案可能都不一样，因为「编程水平的高与低」本身就是一个相对的概念。&lt;br/&gt;我先说一下我的看法。&lt;br/&gt;刚毕业那会，我眼中的编程高手是我的同事 - 超哥，他是亚马逊的架构师，团队里面技术级别最高的人（后来他做到了亚马逊中国首架的位置，现在也算圈内比较知名的大牛了）。&lt;br/&gt;超哥是那种能搞定一个复杂项目（跨多个系统），从架构设计、到编码、到自动化测试、再到运维工具、甚至写文档都非常全面的人。显然是我这个刚毕业的菜鸟极其崇拜的大佬。&lt;br/&gt;再后来，我工作了四五年，走向了技术管理路线，因为带团队的原因，我又多了一个新视角来审视编程厉害的人。&lt;br/&gt;对比所谓的 PPT 架构师、以及讲到技术原理就满嘴跑火车而编码能力却稀松平常的人，我倾向给「编程高手」下一个更务实的定义：那就是实际工作中，能做到高效率、高质量、且稳定输出的人。看似容易，实则很难。我对这个定义的详细解读如下：&amp;#13;
&lt;/p&gt;&lt;p&gt;高效率：编程效率能做到团队中的 Top，对于复杂需求或者复杂问题能够快速理解，具备将复杂工作拆解成一系列简单子问题并搞定这些问题的人。他们能从实际场景出发，有造轮子的能力，也有不造轮子的觉悟。&lt;/p&gt;&amp;#13;
&lt;p&gt;高质量：编程质量能做到团队中的 Top，设计方案的合理性、编码的严谨性、测试方法、监控运维方案等，都能思考全面的人。&lt;/p&gt;&amp;#13;
&lt;p&gt;稳定输出：给高效率和高质量增加的限定。我觉得只有量变引起质变，真正形成了自己的方法论，能持续搞定一类问题，而不是单个问题的人才算是高手。&lt;/p&gt;&amp;#13;
&lt;p&gt;也许你会说要同时做到这 3 点太难了，但是实际工作中，一定有人能做到某个点或者某几点，那么以他们作为这一点的标杆即可。&lt;br/&gt;因为所谓的「编程高手」不过是我们设定的一个目标而已，你清楚这个目标是什么就足够了，不一定非得安在同一个人身上。&amp;#13;
&lt;/p&gt;&lt;p&gt;— 2 —&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;如何理解 “底层知识” 的价值？&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;要成为一个编程高手，底层知识是必不可少的。这是一句正确的废话，就不展开解释了。&lt;br/&gt;读者真正的问题在于：如何将底层知识和实际工作衔接起来，做到相辅相成？&lt;br/&gt;先看一个具体的例子。对于简历中写自己做过性能优化的候选人，我面试时比较喜欢问：你是如何定位性能瓶颈并完成调优的？如果再次碰到此类问题，你的详细思路是什么？&lt;br/&gt;很显然，这是一个系统性的工程问题，能同时对技术深度和宽度进行考察。&lt;br/&gt;不仅仅是应用层的代码，还包括所使用的框架、中间件、虚拟机、网络甚至操作系统等等。有性能指标等基础性的知识，有监控和压测工具的运用，还有成体系的排查思路和优化方法等等。&lt;br/&gt;如果是编程低手，他们的答案通常有这几类：&amp;#13;
&lt;/p&gt;&lt;p&gt;1、性能指标都不清楚，遇到问题也不知道该用什么工具，这一类人应用层都不达标。&lt;/p&gt;&amp;#13;
&lt;p&gt;2、初步定位到一个疑似瓶颈点后，就着手优化，最终解决的只是一个浅层次的性能瓶颈问题，根本原因并未触达到。&lt;/p&gt;&amp;#13;
&lt;p&gt;3、能定位到根本原因，但是对于解决方案的合理性缺少深入思考，不追求极致，最终只是用了曲线的方案再次隐藏了性能问题。&lt;/p&gt;&amp;#13;
&lt;p&gt;通过这个例子，我其实想说明两点：&lt;/p&gt;&amp;#13;
&lt;p&gt;1、不具备底层知识，你的视野根本就触达不到底层的东西，思维以及能力永远只能停留在应用层面，能解决的问题有限。&lt;/p&gt;&amp;#13;
&lt;p&gt;2、读源码、学习各种原理，所有这些都只是学习和巩固知识的过程，真正体现编程水平的是实际解决问题的能力，因此如何将知识变成有效的经验？这个才是关键，做不到要么是实践不够，要么是压根没研究明白。&lt;/p&gt;&amp;#13;
&lt;p&gt;然后，我们再回到问题身上：究竟如何才能做到底层知识和实际工作的衔接呢？如果衔接不好，那问题出在哪个环节？&lt;br/&gt;大家可以先思考下：过往工作中那些对你编程能力帮助很大的经历，你认为你能获得提升最关键的因素是什么？是底层知识的储备吗？&lt;br/&gt;细想一下一定不是，而是发现问题的能力。（注意：我说的是最关键的因素，并不是否认底层知识的储备不重要）&lt;br/&gt;大家可以去观察一个工作中很常见的现象：同一个问题你能看到哪一层？而编程高手又能看到哪一层？谁能更快地贴近问题的本质？谁又能衍生出一系列的问题？这其实就是「发现问题」的能力体现。&lt;br/&gt;只有当你意识到它是一个值得深挖的问题时，才会有一层一层的思考，一层一层的分析。&lt;br/&gt;那应该如何提高这方面的能力呢？其实就是思考力的提升过程，除了观察和刻意练习，我想不出更好的建议。多看高手是怎么做的？遇到问题时自己多问几个为什么？多反思自己的思路是否正确？日积月累水平自然会提高。&lt;br/&gt;当你具备了发现问题的能力，就相当于给知识和经验之间架起了一座桥梁，真正做到相辅相成以及互相驱动。&amp;#13;
&lt;/p&gt;&lt;p&gt;— 3 —&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;成为 “编程高手” 的几点建议&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;编程作为一门实践性的学科，多动手解决问题是最基本的要求了，解决问题越多、越难，能力提升越快。&lt;br/&gt;而在这个过程中，如果有知识输入（底层知识的储备），还有思考的加成（发现问题的能力），编程水平的提升会更加迅速。&lt;br/&gt;我带过的团队里面，还有一类人总说自己技术进步很慢，工作没有挑战。但是当团队遇到一个技术难题时，他根本没有意愿去做深入了解，嘴上说热爱技术、渴望成长，但是却看不到行动，这种人是永远成为不了编程高手的。&lt;br/&gt;真正的热爱来源于行动上的投入，不是找借口，不是追求舒服，这是很重要的一个区别。&amp;#13;
&lt;/p&gt;&lt;p&gt;编程高手是如何练成的？这样看来，它其实是一个体力、脑力、心力的修仙之路。&lt;/p&gt;&amp;#13;
&lt;h1 id=&quot;h1-u56FEu4E66u63A8u8350&quot;&gt;&lt;a name=&quot;图书推荐&quot; class=&quot;reference-link&quot;/&gt;&lt;span class=&quot;header-link octicon octicon-link&quot;/&gt;图书推荐&lt;/h1&gt;&lt;p&gt;▊&lt;a title=&quot;《程序员修炼之道：通向务实的最高境界（第2版）》&quot; href=&quot;http://www.broadview.com.cn/book/6274&quot;&gt;《程序员修炼之道：通向务实的最高境界（第2版）》&lt;/a&gt;&lt;br/&gt;[美] David，Thomas，Andrew，Hunt 著&amp;#13;
&lt;/p&gt;&lt;p&gt;云风 译&lt;/p&gt;&amp;#13;
&lt;p&gt;本书之所以在全球范围内广泛传播，被一代代开发者奉为圭臬，盖因它可以创造出真正的价值：或编写出更好的软件，或探究出编程的本质，而所有收获均不依赖于特定语言、框架和方法。&lt;/p&gt;&amp;#13;
&lt;p&gt;时隔20年的新版，经过全面的重新选材、组织和编写，覆盖哲学、方法、工具、设计、解耦、并发、重构、需求、团队等务实话题的最佳实践及重大陷阱，以及易于改造、复用的架构技术。本书极具洞察力与趣味性，适合从初学者到架构师的各阶层读者潜心研读或增广见闻。&lt;/p&gt;&amp;#13;
&amp;#13;
            &lt;/div&gt;&amp;#13;
&amp;#13;
            &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>dac5e8704656d1ade2c160679cb2d8bb</guid>
<title>Redis 专题：持久化方式之 RDB</title>
<link>https://toutiao.io/k/4tfdc2g</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;blockquote&gt;&lt;p&gt;公众号搜索“码路印记”，点关注不迷路！&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;通过本文你将了解到以下内容：&lt;img data-ratio=&quot;0.46153846153846156&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mZx0iasykfltUJKaqQlD7oMJoZCwokhwjyRAtB416ct8ZpCT1HQn9tjzULGbVHAz7ibtRWM2Spzv0j3VOFCjJcbw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;910&quot;/&gt;&lt;/p&gt;&lt;h3&gt;写在前面&lt;/h3&gt;&lt;p&gt;Redis是一个内存数据库，也就是说所有的数据将保存在内存中，这与传统的MySQL、Oracle、SqlServer等关系型数据库直接把数据保存到硬盘相比，Redis的读写效率非常高。但是保存在内存中也有一个很大的缺陷，一旦断电或者宕机，内存数据库中的内容将会全部丢失。为了弥补这一缺陷，Redis提供了把内存数据持久化到硬盘文件，以及通过备份文件来恢复数据的功能。&lt;/p&gt;&lt;p&gt;Redis支持两种方式的持久化：RDB快照文件和AOF。本文先介绍RDB快照方式的工作原理、优缺点等方面进行阐述，写完AOF方式后再对比两者的优缺点，结合前辈总结给出生产实践，希望能够对你理解Redis的持久化机制带来帮助。&lt;/p&gt;&lt;p&gt;RDB快照用官方的话来说：RDB持久化方案是按照指定时间间隔对你的数据集生成的时间点快照。它是Redis数据库中数据的内存快照，它是一个二进制文件（默认名称为：dump.rdb，可修改），存储了文件生成时Redis数据库中所有的数据内容。可用于Redis的数据备份、转移与恢复。&lt;/p&gt;&lt;h3&gt;配置参数&lt;/h3&gt;&lt;p&gt;RDB快照的触发方式及运行行为受配置参数的影响，打开配置文件&lt;code&gt;redis.conf&lt;/code&gt;查看“SNAPSHOTTING”章节，了解RDB快照的参数及作用。对于各个参数的含义进行了翻译，英语好的同学可以直接看英文。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;################################ SNAPSHOTTING  ################################&lt;br/&gt;#&lt;br/&gt;# Save the DB on disk:&lt;br/&gt;#&lt;br/&gt;#   save &amp;lt;seconds&amp;gt; &amp;lt;changes&amp;gt;&lt;br/&gt;#&lt;br/&gt;#   Will save the DB if both the given number of seconds and the given&lt;br/&gt;#   number of write operations against the DB occurred.&lt;br/&gt;#&lt;br/&gt;#   In the example below the behavior will be to save:&lt;br/&gt;#   after 900 sec (15 min) if at least 1 key changed&lt;br/&gt;#   after 300 sec (5 min) if at least 10 keys changed&lt;br/&gt;#   after 60 sec if at least 10000 keys changed&lt;br/&gt;#&lt;br/&gt;#   Note: you can disable saving completely by commenting out all &quot;save&quot; lines.&lt;br/&gt;#&lt;br/&gt;#   It is also possible to remove all the previously configured save&lt;br/&gt;#   points by adding a save directive with a single empty string argument&lt;br/&gt;#   like in the following example:&lt;br/&gt;#&lt;br/&gt;#   save &quot;&quot;&lt;br/&gt;&lt;br/&gt;save 900 1&lt;br/&gt;save 300 10&lt;br/&gt;save 60 10000&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;save参数是Redis触发自动备份的触发策略，&lt;code&gt;seconds&lt;/code&gt;为统计时间（单位：秒）， &lt;code&gt;changes&lt;/code&gt;为在统计时间内发生写入的次数。&lt;code&gt;save m n&lt;/code&gt;的意思是：m秒内有n条写入就触发一次快照，即备份一次。save参数可以配置多组，满足在不同条件的备份要求。如果需要关闭RDB的自动备份策略，可以使用&lt;code&gt;save &quot;&quot;&lt;/code&gt;。以下为几种配置的说明：&lt;/p&gt;&lt;pre&gt;&lt;code&gt;save 900 1：表示900秒（15分钟）内至少有1个key的值发生变化，则执行&lt;br/&gt;save 300 10：表示300秒（5分钟）内至少有1个key的值发生变化，则执行&lt;br/&gt;save 60 10000：表示60秒（1分钟）内至少有10000个key的值发生变化，则执行&lt;br/&gt;save &quot;&quot;：该配置将会关闭RDB方式的持久化&lt;/code&gt;&lt;/pre&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;dbfilename：快照文件的名称，默认为dump.rdb。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;dir：快照文件保存目录，默认与当前配置文件在同一目录。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;stop-writes-on-bgsave-error：默认值为yes。当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果Redis重启了，那么又可以重新开始接收数据了。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;pre&gt;&lt;code&gt;# By default Redis will stop accepting writes if RDB snapshots are enabled&lt;br/&gt;# (at least one save point) and the latest background save failed.&lt;br/&gt;# This will make the user aware (in a hard way) that data is not persisting&lt;br/&gt;# on disk properly, otherwise chances are that no one will notice and some&lt;br/&gt;# disaster will happen.&lt;br/&gt;#&lt;br/&gt;# If the background saving process will start working again Redis will&lt;br/&gt;# automatically allow writes again.&lt;br/&gt;#&lt;br/&gt;# However if you have setup your proper monitoring of the Redis server&lt;br/&gt;# and persistence, you may want to disable this feature so that Redis will&lt;br/&gt;# continue to work as usual even if there are problems with disk,&lt;br/&gt;# permissions, and so forth.&lt;br/&gt;stop-writes-on-bgsave-error yes&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# Compress string objects using LZF when dump .rdb databases?&lt;br/&gt;# By default compression is enabled as it&#x27;s almost always a win.&lt;br/&gt;# If you want to save some CPU in the saving child set it to &#x27;no&#x27; but&lt;br/&gt;# the dataset will likely be bigger if you have compressible values or keys.&lt;br/&gt;rdbcompression yes&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.&lt;br/&gt;# This makes the format more resistant to corruption but there is a performance&lt;br/&gt;# hit to pay (around 10%) when saving and loading RDB files, so you can disable it&lt;br/&gt;# for maximum performances.&lt;br/&gt;#&lt;br/&gt;# RDB files created with checksum disabled have a checksum of zero that will&lt;br/&gt;# tell the loading code to skip the check.&lt;br/&gt;rdbchecksum yes&lt;/code&gt;&lt;/pre&gt;&lt;h3&gt;持久化流程&lt;/h3&gt;&lt;p&gt;在Redis内完成RDB持久化的方法有rdbSave和rdbSaveBackground两个函数方法（源码文件rdb.c中），先简单说下两者差别：&lt;/p&gt;&lt;p&gt;RDB持久化的触发必然离不开以上两个方法，触发的方式分为手动和自动。手动触发容易理解，是指我们通过Redis客户端人为的对Redis服务端发起持久化备份指令，然后Redis服务端开始执行持久化流程，这里的指令有save和bgsave。自动触发是Redis根据自身运行要求，在满足预设条件时自动触发的持久化流程，自动触发的场景有如下几个（摘自这篇文章）：&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;serverCron中&lt;code&gt;save m n&lt;/code&gt;配置规则自动触发；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;从节点全量复制时，主节点发送rdb文件给从节点完成复制操作，主节点会触发bgsave；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;执行&lt;code&gt;debug reload&lt;/code&gt;命令重新加载redis时；&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;默认情况下（未开启AOF）执行shutdown命令时，自动执行bgsave；&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;结合源码及参考文章，我整理了RDB持久化流程来帮助大家有个整体的了解，然后再从一些细节进行说明。从下图可以知道，自动触发流程是一个完整的链路，涵盖了rdbSaveBackground、rdbSave等，接下来我以serverCron为例分析一下整个流程。&lt;img data-ratio=&quot;0.5476190476190477&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mZx0iasykfltUJKaqQlD7oMJoZCwokhwjEJfMVJToptsMhYnHjLd7PBACAVibxaEwqvTUXEejcskK6VBwNmneVTQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1428&quot;/&gt;&lt;/p&gt;&lt;h4&gt;serverCron&lt;/h4&gt;&lt;p&gt;serverCron是Redis内的一个周期性函数，每隔100毫秒执行一次，它的其中一项工作就是：根据配置文件中save规则来判断当前需要进行自动持久化流程，如果满足条件则尝试开始持久化。简单了解一下这部分的运行原理。&lt;/p&gt;&lt;p&gt;第一次遇到这个函数，通过代码看下这个函数的代码注释。我们可以发现它会完成过期key处理、软件监控、更新一些统计数据、触发RDB持久化或AOF重写、客户端超时处理等，本节我们只关注RDB持久化部分。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;/* This is our timer interrupt, called server.hz times per second.&lt;br/&gt; * Here is where we do a number of things that need to be done asynchronously.&lt;br/&gt; * For instance:&lt;br/&gt; *&lt;br/&gt; * - Active expired keys collection (it is also performed in a lazy way on&lt;br/&gt; *   lookup).&lt;br/&gt; * - Software watchdog.&lt;br/&gt; * - Update some statistic.&lt;br/&gt; * - Incremental rehashing of the DBs hash tables.&lt;br/&gt; * - Triggering BGSAVE / AOF rewrite, and handling of terminated children.&lt;br/&gt; * - Clients timeout of different kinds.&lt;br/&gt; * - Replication reconnection.&lt;br/&gt; * - Many more...&lt;br/&gt; *&lt;br/&gt; * Everything directly called here will be called server.hz times per second,&lt;br/&gt; * so in order to throttle execution of things we want to do less frequently&lt;br/&gt; * a macro is used: run_with_period(milliseconds) { .... }&lt;br/&gt; */&lt;br/&gt;int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在redisServer中有几个与RDB持久化有关的字段（如下代码）。&lt;code&gt;saveparams&lt;/code&gt;为配置文件中的save配置，lastsave为上次持久化的时间戳，dirty为上次持久化后发生修改的次数。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;struct redisServer {&lt;br/&gt;    /* 省略其他字段 */ &lt;br/&gt;    /* RDB persistence */&lt;br/&gt;    long long dirty;                /* Changes to DB from the last save，上次持久化后修改key的次数 */&lt;br/&gt;    struct saveparam *saveparams;   /* Save points array for RDB，对应配置文件多个save参数 */&lt;br/&gt;    int saveparamslen;              /* Number of saving points，save参数的数量 */&lt;br/&gt;    time_t lastsave;                /* Unix time of last successful save 上次持久化时间*/&lt;br/&gt;    /* 省略其他字段 */&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;/* 对应redis.conf中的save参数 */&lt;br/&gt;struct saveparam {&lt;br/&gt;    time_t seconds;                    /* 统计时间范围 */   &lt;br/&gt;    int changes;                    /* 数据修改次数 */&lt;br/&gt;};&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这部分的源码比较简单，感兴趣的同学可以下载查看，为了节省篇幅我就不贴代码了。如果没有后台的RDB持久化或AOF重写进程，serverCron会根据以上配置及状态判断是否需要执行持久化操作，判断依据就是看lastsave、dirty是否满足saveparams数组中的其中一个条件。如果有一个条件匹配，则调用rdbSaveBackground方法，执行异步持久化流程。&lt;/p&gt;&lt;h4&gt;rdbSaveBackground&lt;/h4&gt;&lt;p&gt;rdbSaveBackground是RDB持久化的辅助性方法，根据调用方（父进程或者子进程）不同，有两种不同的执行逻辑。如果调用方是父进程，则fork出子进程，保存子进程信息后直接返回。如果调用方是子进程则调用rdbSave执行RDB持久化逻辑，持久化完成后退出子进程。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;int rdbSaveBackground(char *filename, rdbSaveInfo *rsi) {&lt;br/&gt;    pid_t childpid;&lt;br/&gt;&lt;br/&gt;    if (hasActiveChildProcess()) return C_ERR;&lt;br/&gt;&lt;br/&gt;    server.dirty_before_bgsave = server.dirty;&lt;br/&gt;    server.lastbgsave_try = time(NULL);&lt;br/&gt;&lt;br/&gt;    // fork子进程&lt;br/&gt;    if ((childpid = redisFork(CHILD_TYPE_RDB)) == 0) {&lt;br/&gt;        int retval;&lt;br/&gt;&lt;br/&gt;        /* Child */&lt;br/&gt;        redisSetProcTitle(&quot;redis-rdb-bgsave&quot;);&lt;br/&gt;        redisSetCpuAffinity(server.bgsave_cpulist);&lt;br/&gt;        // 执行rdb持久化&lt;br/&gt;        retval = rdbSave(filename,rsi);&lt;br/&gt;        if (retval == C_OK) {&lt;br/&gt;            sendChildCOWInfo(CHILD_TYPE_RDB, 1, &quot;RDB&quot;);&lt;br/&gt;        }&lt;br/&gt;        // 持久化完成后，退出子进程&lt;br/&gt;        exitFromChild((retval == C_OK) ? 0 : 1);&lt;br/&gt;    } else {&lt;br/&gt;        /* Parent 父进程：记录fork子进程的时间等信息*/&lt;br/&gt;        if (childpid == -1) {&lt;br/&gt;            server.lastbgsave_status = C_ERR;&lt;br/&gt;            serverLog(LL_WARNING,&quot;Can&#x27;t save in background: fork: %s&quot;,&lt;br/&gt;                strerror(errno));&lt;br/&gt;            return C_ERR;&lt;br/&gt;        }&lt;br/&gt;        serverLog(LL_NOTICE,&quot;Background saving started by pid %ld&quot;,(long) childpid);&lt;br/&gt;        server.rdb_save_time_start = time(NULL);&lt;br/&gt;        server.rdb_child_type = RDB_CHILD_TYPE_DISK;&lt;br/&gt;        return C_OK;&lt;br/&gt;    }&lt;br/&gt;    return C_OK; /* unreached */&lt;br/&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;rdbSave是真正执行持久化的方法，它在执行时存在大量的I/O、计算操作，耗时、CPU占用较大，在Redis的单线程模型中持久化过程会持续占用线程资源，进而导致Redis无法提供其他服务。为了解决这一问题Redis在rdbSaveBackground中fork出子进程，由子进程完成持久化工作，避免了占用父进程过多的资源。&lt;/p&gt;&lt;p&gt;需要注意的是，如果父进程内存占用过大，fork过程会比较耗时，在这个过程中父进程无法对外提供服务；另外，需要综合考虑计算机内存使用量，fork子进程后会占用双倍的内存资源，需要确保内存够用。通过info stats命令查看latest_fork_usec选项，可以获取最近一个fork以操作的耗时。&lt;/p&gt;&lt;h4&gt;rdbSave&lt;/h4&gt;&lt;p&gt;Redis的rdbSave函数是真正进行RDB持久化的函数，流程、细节贼多，整体流程可以总结为：创建并打开临时文件、Redis内存数据写入临时文件、临时文件写入磁盘、临时文件重命名为正式RDB文件、更新持久化状态信息（dirty、lastsave）。其中“Redis内存数据写入临时文件”最为核心和复杂，写入过程直接体现了RDB文件的文件格式，本着一图胜千言的理念，我按照源码流程绘制了下图。&lt;img data-ratio=&quot;0.7005420054200542&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mZx0iasykfltUJKaqQlD7oMJoZCwokhwjHpbFqSuJA6S4ZtRia9wxm9VzJBj4bkoXHAp4vYQ9JRDHTmrHcctyTbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1476&quot;/&gt;补充说明一下，上图右下角“遍历当前数据库的键值对并写入”这个环节会根据不同类型的Redis数据类型及底层数据结构采用不同的格式写入到RDB文件中，不再展开了。我觉得大家对整个过程有个直观的理解就好，这对于我们理解Redis内部的运作机制大有裨益。&lt;/p&gt;&lt;h3&gt;数据恢复&lt;/h3&gt;&lt;p&gt;数据恢复是自动执行的，我们将备份文件 (例如：dump.rdb) 移动到Redis备份文件目录并启动服务即可，Redis就会自动加载文件数据至内存。Redis 服务器在载入RDB文件期间，会一直处于阻塞状态，直到载入工作完成为止。&lt;/p&gt;&lt;p&gt;这里备份文件名称及目录需要与redis.conf中的配置信息保持一致。&lt;/p&gt;&lt;h3&gt;RDB优缺点&lt;/h3&gt;&lt;p&gt;了解了RDB的工作原理后，对于RDB的优缺点就比较容易总结了。先来看下RDB的优点：&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;RDB是一个紧凑压缩的二进制文件，代表Redis在某一个时间点上的数据快照，非常适合用于备份、全量复制等场景。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RDB对灾难恢复、数据迁移非常友好，RDB文件可以转移至任何需要的地方并重新加载。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RDB是Redis数据的内存快照，数据恢复速度较快，相比于AOF的命令重放有着更高的性能。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;事物总是有两面性的，RDB优点明显，同样也存在缺点：&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;RDB方式无法做到实时或秒级持久化。因为持久化过程是通过fork子进程后由子进程完成的，子进程的内存只是在fork操作那一时刻父进程的数据快照。而fork操作是一个耗时操作，无法做到实时性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RDB持久化过程中的fork操作，会导致内存占用加倍，而且父进程数据越多，fork过程越长。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;RDB文件有文件格式要求，不同版本的Redis会对文件格式进行调整，存在老版本无法兼容新版本的问题。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;参考文献&lt;/h3&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;Redis官方文档：《Redis Persistence》：https://redis.io/topics/persistence&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;《Redis RDB Persistence Details》：https://programming.vip/docs/redis-rdb-persistence-details.html&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;一文看懂Redis的持久化原理：https://programming.vip/docs/redis-rdb-persistence-details.html&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Redis详解（六）------ RDB 持久化：https://www.cnblogs.com/ysocean/p/9114268.html#_label2&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;img data-ratio=&quot;0.4605263157894737&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/mZx0iasykfltUJKaqQlD7oMJoZCwokhwjaoXYI83lvbNppCmiaATVullkVKiaPCyWl5p6yAawtdzia8rkQZAh4Adkg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;760&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>