<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>e1d52e0179ef961f976b6ef0bc740b5a</guid>
<title>一站搞定各种开发文档</title>
<link>https://toutiao.io/k/zg31kv0</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;p class=&quot;original_area_primary&quot;&gt;
                                                                                                &lt;/p&gt;

                    
                                            &lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;开发者的苦恼：经常要在多个API文档中切换，浏览器书签栏收藏各种语言相关的接口说明文档。&lt;/p&gt;&lt;p&gt;无意中在Github上发现&lt;span&gt;DevDocs&lt;sup&gt;[1]&lt;/sup&gt;&lt;/span&gt;这个开源项目，它是一个把所有开发相关的文档以web的形式做了一个综合的网站，并提供搜索，离线访问，移动版本支持，暗黑主题，快捷键操作等功能。&lt;/p&gt;&lt;p&gt;访问：https://devdocs.io/&lt;/p&gt;&lt;p&gt;在访问前你需要知道的：&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;1.&lt;/span&gt;打开&lt;span&gt;preference&lt;sup&gt;[1]&lt;/sup&gt;&lt;/span&gt;设置，勾选上你需要访问的文档列表，并且可以设置主题&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.5721200387221684&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/wokNJRf0ytqYNndvZIwgJ8ibicC5HOS8DZQR8ic04iaumzANqUWtSoWKd4noy4DeJC0PK2pUz0sVGgnJgnjibNqECgg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1033&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;span&gt;&lt;span&gt;2.&lt;/span&gt;你甚至可以抛弃鼠标，使用键盘按键操作，查看&lt;span&gt;快捷键说明&lt;sup&gt;[2]&lt;/sup&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;3.&lt;/span&gt;支持模糊搜索，比如“bgcp”就能搜索到“background-clip”的内容&lt;/span&gt;&lt;span&gt;&lt;span&gt;4.&lt;/span&gt;搜索特定的文档，输入名字即可或者缩写，回车进入搜索结果&lt;/span&gt;&lt;span&gt;&lt;span&gt;5.&lt;/span&gt;可以使用浏览器的地址栏进行搜索，因为它被大部分浏览器内嵌安装了&lt;/span&gt;&lt;span&gt;&lt;span&gt;6.&lt;/span&gt;DevDocs在手机端支持离线访问，也可以在谷歌浏览器上安装&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;因为它是开源的，还可以在本地进行安装，该开源程序由两部分组成，Ruby负责元数据和文档的生成，Javascript负责app网站的搭建。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;DevDocs需要Ruby 2.6.x的环境，libcurl库和由ExecJS支持的Javascript运行时，装好之后，运行以下命令：&lt;/p&gt;&lt;section&gt;&lt;pre&gt;&lt;code&gt;git &lt;span&gt;clone&lt;/span&gt; https://github.com/freeCodeCamp/devdocs.git &amp;amp;&amp;amp; &lt;span&gt;cd&lt;/span&gt; devdocs&lt;br/&gt;gem install bundler&lt;br/&gt;bundle install&lt;br/&gt;bundle &lt;span&gt;exec&lt;/span&gt; thor docs:download --default&lt;br/&gt;bundle &lt;span&gt;exec&lt;/span&gt; rackup&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;最后在浏览器访问 &lt;span&gt;localhost:9292&lt;sup&gt;[4]&lt;/sup&gt;&lt;/span&gt; 第一次访问会比较慢，要编译静态代码。&lt;/p&gt;&lt;p&gt;或者免去环境安装的麻烦，直接使用Docker进行安装，命令如下：&lt;/p&gt;&lt;section&gt;&lt;pre&gt;&lt;code&gt;&lt;span&gt;# First, build the image&lt;/span&gt;&lt;br/&gt;git &lt;span&gt;clone&lt;/span&gt; https://github.com/freeCodeCamp/devdocs.git &amp;amp;&amp;amp; &lt;span&gt;cd&lt;/span&gt; devdocs&lt;br/&gt;docker build -t thibaut/devdocs .&lt;br/&gt;&lt;br/&gt;&lt;span&gt;# Finally, start a DevDocs container (access http://localhost:9292)&lt;/span&gt;&lt;br/&gt;docker run --name devdocs -d -p 9292:9292 thibaut/devdocs&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p&gt;如果你有很好的想法，又懂编程，可以向开源提交代码。&lt;/p&gt;&lt;p&gt;另外下面是开发者根据开源代码移植开发的一些项目，做成了很多IDE的插件，比如前端常用的VSCode插件&lt;span&gt;devdocs for VS Code&lt;sup&gt;[5]&lt;/sup&gt;&lt;/span&gt;，&lt;span&gt;Atom plugin&lt;sup&gt;[6]&lt;/sup&gt;&lt;/span&gt;，&lt;span&gt;Sublime Text plugin&lt;sup&gt;[7]&lt;/sup&gt;&lt;/span&gt; 等等。&lt;/p&gt;&lt;p&gt;&lt;span&gt;References&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;[1]&lt;/code&gt; DevDocs: &lt;em&gt;https://github.com/freeCodeCamp/devdocs&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[2]&lt;/code&gt; preference: &lt;em&gt;https://devdocs.io/settings&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[3]&lt;/code&gt; 快捷键说明: &lt;em&gt;https://devdocs.io/help#shortcuts&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[4]&lt;/code&gt; localhost:9292: &lt;em&gt;http://localhost:9292/&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[5]&lt;/code&gt; devdocs for VS Code: &lt;em&gt;https://marketplace.visualstudio.com/items?itemName=deibit.devdocs&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[6]&lt;/code&gt; Atom plugin: &lt;em&gt;https://atom.io/packages/devdocs&lt;/em&gt;&lt;br/&gt;&lt;code&gt;[7]&lt;/code&gt; Sublime Text plugin: &lt;em&gt;https://sublime.wbond.net/packages/DevDocs&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>bc77c43ef43ad246d0c548b1291cc30f</guid>
<title>发票总库 DDD 实践</title>
<link>https://toutiao.io/k/0d4bxmx</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;p&gt;&lt;img class=&quot;rich_pages __bg_gif&quot; data-backh=&quot;57&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.0984375&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_gif/UPBmkBoiaVwuh8DEJlYXXvEjNrISejeXSt7knYYiaGantXAsgOvXvxAOog7a4L9yEicM1biao6MGia2Ob4I3B77Z6Vg/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;640&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;总篇117篇 2021年第8篇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DDD&lt;/span&gt;&lt;span&gt;是一套完善的系统设计方法，可以帮助我们在系统设计的过程中缕清思路，规范流程，降低系统建设的复杂度，同时DDD的领域建模过程也是团队成员之间形成系统通用语言、建立良好沟通机制的过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在电商中台研发团队内部，很早之前就对DDD有过分享和讨论，近期我们在做购车发票总库的迁移工作，经过分析，借此项目做一次DDD的落地实践，在时间和人员投入上都比较适宜。本篇文章就对我们的整个实践过程做一个总结，与大家一起学习和提升。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;我们先来看一张DDD的经典知识体系结构图：&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;442&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.7651452282157676&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pMQMDEwxCqPbVnMRrAiaq4zGgqZHia6o3micO8XZgqmgicKAR0NRDCiaZvZw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1205&quot;/&gt;&lt;/section&gt;&lt;section&gt;&lt;span/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;DDD的核心就是将问题范围限定在特定的边界内，在这个特定的边界内建立领域模型，进而解决相应的业务问题。领域是用来限定业务边界和范围，自然它的边界就或大或小，当一个领域过大的时候可以将其进一步拆分为子领域。而子领域按照重要性以及关注度，可以分类为：核心子域、支撑子域、通用子域，这三类子域随着时间的推移或者业务的变化，其角色也可能发生转换，我们的重点应该放在核心子域建设上。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;明确了领域（子域）其实也就定义好了边界，即：限界上下文，为了确保在限界上下文内，所有人对概念的理解一致，不产生歧义，就需要对限界上下文中的每一个事件、动作、实体等对象都要形成通用语言，统一命名。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;实体和值对象是领域模型中非常重要的两个基本概念，一起构成了领域模型中最核心的领域逻辑。实体具有唯一标识以及本身的生命周期，当实体的属性（值）发生变化时，实体还是原来的那个实体，而值对象更多的是属性（值）的描述，当属性（值）发生变化时，值对象就不在是原来的值对象了。实体在代码形态上，通常采用充血模型实现，与实体相关的所有业务逻辑都在该实体类中实现，实体自身保障事务的原子性，当需要多个实体共同实现某个逻辑时，则会在领域服务层实现跨多个实体的组合封装。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;聚合根也是实体，它用来管理聚合内所有的实体以及值对象，一个聚合内包含了聚合根、实体、值对象、领域服务等，它们按照聚合的业务规则完成聚合内的领域逻辑，一个聚合只有一个聚合根，聚合和聚合之间只能通过聚合根的唯一ID标识进行引用，对于跨聚合的服务调用，往往是在应用服务层组合编排他们之间的调用关系，实现相互之间的协同。在DDD中强调一次事务最多只修改一个聚合的数据，因此在聚合内部会采用数据强一致性，而聚合之间则采用数据最终一致性的方式实现数据一致性。&lt;/p&gt;&lt;p&gt;&lt;span&gt;通常在大部分领域模型中，有70%的聚合通常只有一个实体，即聚合根，该实体内部没有包含其他实体，只包含一些值对象；&lt;/span&gt;&lt;span&gt;另外30%的聚合中，基本上也只包含两到三个实体。&lt;/span&gt;&lt;span&gt;这意味着大部分的聚合都只是一个实体，该实体同时也是聚合根。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;领域事件是领域模型中非常重要的一环，一个个领域事件可以串联起完整的业务逻辑闭环，领域事件采用事件驱动的方式用来解耦服务与服务之间的依赖关系，在服务内部可以采用事件总线的方式实现逻辑的串联，服务与服务之间可以采用消息队列的方式实现服务间的解耦和数据最终一致性。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;工厂和仓储&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;此工厂非彼设计模式的工厂，领域驱动的工厂强调封装了所有创建对象的复杂操作过程。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;仓储就是持久化机制，比如：Dao层、Cache层等。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;在整个发票总库DDD的实践过程中，我们也是按照战略设计和战术设计两个阶段进行：业务抽象的过程就是业务领域建模的过程，对应DDD的战略设计，而系统架构设计并落地的过程则对应DDD的战术设计。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;244&quot; data-backw=&quot;448&quot; data-croporisrc=&quot;https://mmbiz.qlogo.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pLJvVZxaulib9Y96zoWYRia2RQR0kToPZm8a1Bjed9mFJUKB2PR6NQnUQ/0?wx_fmt=png&quot; data-cropx1=&quot;119.5847750865052&quot; data-cropx2=&quot;1113.9100346020762&quot; data-cropy1=&quot;121.79930795847751&quot; data-cropy2=&quot;662.1453287197232&quot; data-ratio=&quot;0.5442655935613682&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pECsrgiaK86HIUmibaqdtEEqV5zPtVyicODzKCxe5QYCN3uRrKNC76DN7Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;994&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在战略设计阶段&lt;/strong&gt;&lt;span&gt;，因为发票总库的业务逻辑相对比较清晰，也有助于我们采用事件风暴的方式，快速的找出了聚合、领域对象、领域类型，为构建领域模型奠定了基础，以下是我们梳理完成的发票域的对象清单：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;325&quot; data-backw=&quot;413&quot; data-croporisrc=&quot;https://mmbiz.qlogo.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5puUAQRgw9EZ2zRJoBs8K4aLqxHgXOyaM293GDaybZtJuPQw0cYM0rmQ/0?wx_fmt=png&quot; data-cropx1=&quot;179.3771626297578&quot; data-cropx2=&quot;1093.9792387543253&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;719.7231833910035&quot; data-ratio=&quot;0.787746170678337&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pxicLKpxia1BLUVLXJjBEr1HxMth0iaicbjvm2G96eYulxggLPsg2xpNvzg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;914&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;从上图中可以看到，整个发票域我们只定义了发票这个聚合，同时它也是聚合根，在发票领域服务内部，还包含了排重规则&lt;/span&gt;&lt;span&gt;、发票日志两个实体以及发票审核状态、业务类型、入库类型、应用唯一标识四个值对象，结合发票入库这个领域事件以及发票总库的业务逻辑&lt;/span&gt;&lt;span&gt;，在发票领域服务内对外提供了发票入库、发票排重等方法，对于数据存储，则需要统一抽象了仓储接口（不受限于具体的数据存储介质）。&lt;/span&gt;&lt;span&gt;    &lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt; &lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;在战术设计阶段&lt;/strong&gt;，我们完成了分层设计、规范设计等事项，在分层设计中我们遵循了DDD的四层分层模型，如下所示：&lt;/span&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;354&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.6123188405797102&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pZySrK0GW3XvhiasaWTZEibT8v9iaAyt5fGCibnaJVeuRQDDkq7WRA6jOUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用户接口层主要实现后端服务与前端调用入口的接口数据适配和转换；&lt;/span&gt;&lt;span&gt;应用层主要用来协调领域层多个聚合之间的服务组合和编排，以及负责领域事件的订阅和发布等职责；&lt;/span&gt;&lt;span&gt;领域层是领域模型的核心，实现领域模型的核心业务逻辑，其内部逻辑相对稳定，不会受外界变化（如：&lt;/span&gt;&lt;span&gt;底层数据存储介质的变换、通讯方式的变化等）的影响，基础层主要用来提供通用的基础工具类或基础服务。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;结合上述四层模型，在实际代码工程的构建过程中，我们也是参考了COLA架构进行构建，具体如下：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;466&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.8051063829787234&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pH1afb38dibTC6yR7oLdydkNzDvVMC1n85Iya0KujHlGP62debLx7TFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1175&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;作为服务的启动模块（SpringBootApplication）以及用户接口层（Controller），定义了所有对外发布的接口以及DTO到领域层实体类的转换。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;应用层模块，提供了发票领域服务的方法定义及实现，同时作为事件总线（EventBus）的接收层，提供了多个发票领域事件的Handler处理器，对领域事件做下一步处理，比如：发送MQ消息等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;领域层模块，定义了发票领域的实体、值对象、发票事件、发票领域服务以及仓储层接口，同时包含了创建领域实体的工厂类。在发票领域服务类中，通过发票实体以及发票规则实体的协作，完成发票入库逻辑，当发票入库事件发生后，会通过事件总线（EventBus）将该领域事件发布出去。而在发票的聚合根实体内，则通过事务控制了发票实体以及发票日志实体的数据存储。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;基础设施层模块，提供了发票仓储接口、发票日志仓储接口的实现类以及从领域实体类到仓储层PO对象类的转换，同时提供了事件总线（EventBus）以及加解密等通用工具类。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;客户端模块，提供了API定义以及DTO、VO等对象的定义。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;为了防止开发过程中的随心所欲，开发规范的制定就显得特别重要，但也是最容易被无视的点，其结果就是架构的一致性被严重破坏，代码的可维护性将急剧下降，架构将形同虚设。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;首先是分包设计，在领域驱动设计里，推荐使用了基于业务的分包，即通过软件所实现的业务功能进行模块化划分，而不是从技术的角度划分(比如首先划分出service和infrastruture等包)&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;123&quot; data-backw=&quot;523&quot; data-croporisrc=&quot;https://mmbiz.qlogo.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5po9gGicZtbte4H30ic1ciaSIUUEicQev6xs2ox36Hw1qLUbnkdO4icoGy2zw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;1158.2006920415226&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;272.38754325259515&quot; data-ratio=&quot;0.23575129533678757&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pGozicLmKWfFUStYL7nSfeUJOFA9g7nYEuqKZdnbtGk8FnvnCtOYPGmA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1158&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;类名应该是自明的，也就是看到类名就知道里面是干了什么事，这也就反向要求我们的类也必须是单一职责的。为了避免团队人员对dto、bo、vo、po等这些pojo对象的二义性理解，我们制定了类名规范。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;666&quot; data-backw=&quot;578&quot; data-ratio=&quot;1.1518858307849134&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5peF5JqQsD6wUhCJUvzLYlO6mj8dpc3KCAPAKZkVibZLW8YVDgypgehfw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;981&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;方法名约定&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;308&quot; data-backw=&quot;450&quot; data-croporisrc=&quot;https://mmbiz.qlogo.cn/mmbiz_png/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pptOFrR7JHOjF2vs19WvxVZ8beKsmkFd8upVYCsoNbKWqNBR73WnpDQ/0?wx_fmt=png&quot; data-cropx1=&quot;98.08650519031141&quot; data-cropx2=&quot;766.8581314878893&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;457.7370242214533&quot; data-ratio=&quot;0.6856287425149701&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5pKtEh82rzwcKwHkp8mYTl2ZAmP1FL4zkBYDMzWhiaelq8VvxbutCduBA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;668&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;span/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;经过上述过程，我们按照DDD的方式完成了购车发票总库的迁移工作，目前系统已经顺利上线，因为大家都是第一次做DDD的落地实践，在整个过程中会发现从理论到实践难免会出现一些理解的偏差和分歧，比如：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;解：创建聚合根通过Factory完成；业务逻辑优先在聚合根边界内完成；聚合根中不合适放置的业务逻辑才考虑放到DomainService中。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;解：DDD的概念提出之初，事务的层次体现在跨领域交付的应用服务层。在微服务的流行的现阶段，各领域之间通过接口交互，在实施过程中，我们要灵活根据业务逻辑和当前的架构来把事务粒度做到最小。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;解：读操作便可以根据自身所需独立设计数据结构，而不用受写模型数据结构的牵制。因此，领域驱动设计通常适用于增删改的业务操作，但不适用于分析统计。在一个系统中，增删改的业务可以采用领域驱动的设计，但在非增删改的分析汇总场景中，则不必采用领域驱动的设计，直接 SQL 查询就好了，也就不必再遵循聚合的约束了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;解：领域实体里的方法映射的是现实世界的动作行为，改变的是状态、时间，而领域服务是实体的能力，领域服务在设计上应该是无状态的，它存在的意义就是协调领域对象共完成某个操作。业务逻辑应该放在领域实体里还是领域服务里应该根据这个原则来评估。&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在实践过程中出现这些问题其实都很正常，每一个问题都不一定会有标准答案，比如上述没有提到的问题：一个领域对象究竟是定义为实体还是值对象等，即便是上述我们有答案的问题也仅仅是我们的一些见解，仁者见仁，智者见智，面对这些问题，我们一方面需要摆脱掉之前的思维定式，再次深入的理解理论定义，一方面也需要借鉴参考一些成熟代码案例，找到适合我们自身项目的结论。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;按照DDD的模式进行项目的落地实施，相比与传统开发模式也会有一些质的提升，主要表现在以下几方面：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DDD首先强调的是确定领域边界，领域边界确定后所有设计均会在同一个领域范围内进行开展，清晰的领域边界对应了实际系统落地过程中合理的系统边界，这对于后续系统内高内聚功能的延展起到了积极的帮助作用。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;在传统的开发模式中，不同环节的人员经常会出现对于同一对象的叫法不一致、定义不一致的现象，这在一定程度上也造成了大家对于系统理解的偏差，而在DDD模式中，提出了通用语言的概念，参与其中的所有成员在系统生命周期内均采用简单清晰明了的通用语言进行交流，通用语言成为团队内外部系统交流的统一语言，既减少了信息的失真, 又确保了大家在同一领域知识体系内交流的便利性、理解的一致性。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;传统开发模式采用贫血模型，将属性与业务逻辑彻底分离，通过get / set方法改变对象属性，对象属性可以被随意的修改，在面对简单的业务场景，贫血模型还可以应对，但是在面对复杂的业务场景时，传统开发模式的贫血模型维护成本高的弊端就会被暴露。与贫血模型相对应的，DDD的开发模式强调采用充血模型，将属性和行为聚合到一个实体内，业务逻辑更加便于维护和管理，也更加符合面向对象的编程逻辑，这也是基于充血模型的 DDD 开发模式越来越被人提倡的原因。&lt;/span&gt;&lt;span/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;DDD所涵盖的范围其实是比较广泛的，在具体的实践过程中是需要结合实际的项目场景和团队成员的实际情况等多方面因素，不断的摸索，经过长时间的学习和积累，逐步的在团队内部建立DDD的方法论以及对应的开发模式、技术架构。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;以上就是我们的DDD实践总结，欢迎对此感兴趣的同学与我们交流，共同进步。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-backh=&quot;434&quot; data-backw=&quot;578&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/UPBmkBoiaVwvXPMZLLx7toI4AvneeOn5p79Z4yMmTV5IAIZXtCXgYGTPpkezuAXrsV1sewb8MT7KicSrYUMK47MQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;span/&gt;&lt;span&gt;王松，2015年加入汽车之家，现担任商业平台中心电商中台技术负责人；&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;方利，2016年加入汽车之家，现担任商业平台中心电商中台系统架构师，负责订单交易相关系统的架构及研发工作；&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;发票总库DDD实践小组其他成员：张凡、张林林，均担任商业平台中心电商中台软件开发工程师，拥有多年的电商交易系统研发经验。&lt;/span&gt;&lt;span/&gt;&lt;/section&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>e4658ceeaedd5d851bb4d8faa6c22b69</guid>
<title>重磅！Apache Kafka 3.0 发布</title>
<link>https://toutiao.io/k/oemgb5c</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.5623608017817372&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/3EcuPIickyyFByicvuGSqZtnCnDa51bSkzuJricNsWHzvK5l4rhtiaLkERp892iaBbjfQlsl6YxiaVwzpBRpHZ0ic6N3Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1796&quot;/&gt;&lt;/p&gt;&lt;p&gt;我很高兴代表 Apache Kafka® 社区宣布 Apache Kafka 3.0 的发布。Apache Kafka 3.0 是一个涉及多方面的大版本。Apache Kafka 3.0 引入了各种新功能、突破性的 API 更改以及对 KRaft 的改进——Apache Kafka 的内置共识机制将取代 Apache ZooKeeper™。&lt;/p&gt;&lt;p&gt;虽然 KRaft 尚未被推荐用于生产（已知差距列表），但我们对 KRaft 元数据和 API 进行了许多改进。Exactly-once 和分区重新分配支持值得强调。我们鼓励您查看 KRaft 的新功能并在开发环境中试用它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 Apache Kafka 3.0 开始，生产者默认启用最强的交付保证 ( &lt;code&gt;acks=all&lt;/code&gt;, &lt;code&gt;enable.idempotence=true&lt;/code&gt;)。这意味着用户现在默认获得排序和持久性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，不要错过 Kafka Connect 任务重启增强、KStreams 基于时间戳同步的改进以及 MirrorMaker2 更灵活的配置选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;常规变化&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-750（第一部分）：弃用 Kafka 中对 Java 8 的支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 3.0 中，Apache Kafka 项目的所有组件都已弃用对 Java 8 的支持。这将使用户有时间在下一个主要版本 (4.0) 之前进行调整，届时 Java 8 支持将被取消。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-751（第一部分）：弃用 Kafka 中对 Scala 2.12 的支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对 Scala 2.12 的支持在 Apache Kafka 3.0 中也已弃用。与 Java 8 一样，我们给用户时间来适应，因为计划在下一个主要版本 (4.0) 中删除对 Scala 2.12 的支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;Kafka 代理、生产者、消费者和管理客户端&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-630：Kafka Raft 快照&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在 3.0 中引入的一个主要功能是 KRaft 控制器和 KRaft 代理能够为名为 &lt;span&gt;__cluster_metadata&lt;/span&gt; 的元数据主题分区生成、复制和加载快照。Kafka 集群使用此主题来存储和复制有关集群的元数据信息，如代理配置、主题分区分配、领导等。随着此状态的增长，Kafka Raft Snapshot 提供了一种有效的方式来存储、加载和复制此信息.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-746：修改 KRaft 元数据记录&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自第一版 Kafka Raft 控制器以来的经验和持续开发表明，需要修改一些元数据记录类型，当 Kafka 被配置为在没有 ZooKeeper (ZK) 的情况下运行时使用这些记录类型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-730：KRaft 模式下的生产者 ID 生成&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 3.0 和KIP-730 中，Kafka 控制器现在完全接管了生成 Kafka 生产者 ID 的责任。控制器在 ZK 和 KRaft 模式下都这样做。这让我们更接近桥接版本，这将允许用户从使用 ZK 的 Kafka 部署过渡到使用 KRaft 的新部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-679：Producer 将默认启用最强的交付保证&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 3.0 开始，Kafka 生产者默认开启幂等性和所有副本的交付确认。这使得默认情况下记录交付保证更强。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-735：增加默认消费者会话超时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Kafka Consumer 的配置属性的默认值&lt;code&gt;session.timeout.ms&lt;/code&gt;从 10 秒增加到 45 秒。这将允许消费者在默认情况下更好地适应暂时的网络故障，并在消费者似乎只是暂时离开组时避免连续重新平衡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-709：扩展 OffsetFetch 请求以接受多个组 ID&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;请求 Kafka 消费者组的当前偏移量已经有一段时间了。但是获取多个消费者组的偏移量需要对每个组进行单独的请求。在 3.0 和KIP-709 中，fetch 和 AdminClient API 被扩展为支持在单个请求/响应中同时读取多个消费者组的偏移量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-699：更新 FindCoordinator 以一次解析多个 Coordinator&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持可以以有效方式同时应用于多个消费者组的操作在很大程度上取决于客户端有效发现这些组的协调者的能力。这通过KIP-699成为可能，它增加了对通过一个请求发现多个组的协调器的支持。Kafka 客户端已更新为在与支持此请求的新 Kafka 代理交谈时使用此优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-724：删除对消息格式 v0 和 v1 的支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;自 2017 年 6 月随Kafka 0.11.0推出四年以来，消息格式 v2 一直是默认消息格式。因此，在桥下流过足够多的水（或溪流）后，3.0 的主要版本为我们提供了弃用旧消息格式（即 v0 和 v1）的好机会。这些格式今天很少使用。在 3.0 中，如果用户将代理配置为使用消息格式 v0 或 v1，他们将收到警告。此选项将在 Kafka 4.0 中删除（有关详细信息和弃用 v0 和 v1 消息格式的影响，请参阅KIP-724）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-707： &lt;code&gt;KafkaFuture 的&lt;/code&gt;未来&lt;/h3&gt;&lt;p&gt;当&lt;code&gt;KafkaFuture&lt;/code&gt;引入该类型以促进 Kafka AdminClient 的实现时，Java 8 之前的版本仍在广泛使用，并且 Kafka 正式支持 Java 7。快进几年后，现在 Kafka 运行在支持&lt;code&gt;CompletionStage&lt;/code&gt;和&lt;code&gt;CompletableFuture&lt;/code&gt;类类型的Java 版本上。使用KIP-707，&lt;code&gt;KafkaFuture&lt;/code&gt;添加了一种返回&lt;code&gt;CompletionStage&lt;/code&gt;对象的方法，并以&lt;code&gt;KafkaFuture&lt;/code&gt;向后兼容的方式增强了可用性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-466：添加对 List&amp;lt;T&amp;gt; 序列化和反序列化的支持&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KIP-466为泛型列表的序列化和反序列化添加了新的类和方法——这一特性对 Kafka 客户端和 Kafka Streams 都非常有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-734：改进 AdminClient.listOffsets 以返回时间戳和具有最大时间戳的记录的偏移量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;用户列出 Kafka 主题/分区偏移量的功能已得到扩展。使用KIP-734，用户现在可以要求 AdminClient 返回主题/分区中具有最高时间戳的记录的偏移量和时间戳。（这是不是与什么的AdminClient收益已经为最新的偏移，这是下一个记录的偏移，在主题/分区写入混淆。）这个扩展现有ListOffsets API允许用户探测&lt;em&gt;生动活泼&lt;/em&gt;的通过询问哪个是最近写入的记录的偏移量以及它的时间戳是什么来分区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;kafka Connect&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-745：连接 API 以重新启动连接器和任务&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Kafka Connect 中，连接器在运行时表示为一组&lt;code&gt;Connector&lt;/code&gt;类实例和一个或多个&lt;code&gt;Task&lt;/code&gt;类实例，并且通过 Connect REST API 可用的连接器上的大多数操作都可以应用于整个组。从一开始，一个值得注意的例外&lt;code&gt;restart&lt;/code&gt;是&lt;code&gt;Connector&lt;/code&gt;和&lt;code&gt;Task&lt;/code&gt;实例的端点。要重新启动整个连接器，用户必须单独调用以重新启动连接器实例和任务实例。在 3.0 中，KIP-745使用户能够通过一次调用重新启动所有或仅失败的连接器&lt;code&gt;Connector&lt;/code&gt;和&lt;code&gt;Task&lt;/code&gt;实例。此功能是附加功能，&lt;code&gt;restart&lt;/code&gt;REST API的先前行为保持不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-738：删除 Connect 的内部转换器属性&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在之前的主版本 ( Apache Kafka 2.0 ) 中弃用它们之后，&lt;code&gt;internal.key.converter&lt;/code&gt;并&lt;code&gt;internal.value.converter&lt;/code&gt;在 Connect 工作器的配置中作为配置属性和前缀被删除。展望未来，内部 Connect 主题将专门使用&lt;code&gt;JsonConverter&lt;/code&gt;来存储没有嵌入模式的记录。任何使用不同转换器的现有 Connect 集群都必须将其内部主题移植到新格式（有关升级路径的详细信息，请参阅KIP-738）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-722：默认启用连接器客户端覆盖&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从Apache Kafka 2.3.0 开始，可以配置连接器工作器以允许连接器配置覆盖连接器使用的 Kafka 客户端属性。这是一个广泛使用的功能，现在有机会发布一个主要版本，默认启用覆盖连接器客户端属性的功能（默认&lt;code&gt;connector.client.config.override.policy&lt;/code&gt;设置为&lt;code&gt;All&lt;/code&gt;）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-721：在连接&lt;code&gt;Log4j&lt;/code&gt;配置中启用连接器日志上下文&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一个在 2.3.0 中引入但到目前为止尚未默认启用的功能是连接器日志上下文。这在 3.0 中发生了变化，连接器上下文默认添加&lt;code&gt;log4j&lt;/code&gt;到 Connect 工作器的日志模式中。从以前的版本升级到 3.0 将&lt;code&gt;log4j&lt;/code&gt;通过在适当的情况下添加连接器上下文来更改导出的日志行的格式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;Kafka Streams&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-695：进一步改进 Kafka Streams 时间戳同步&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KIP-695增强了 Streams 任务如何选择获取记录的语义，并扩展了配置属性的含义和可用值&lt;code&gt;max.task.idle.ms&lt;/code&gt;。此更改需要 Kafka 消费者 API 中的一种新方法，&lt;code&gt;currentLag&lt;/code&gt;如果本地已知且无需联系 Kafka Broker，则能够返回特定分区的消费者滞后。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-715：在流中公开提交的偏移量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;3.0开始，三个新的方法添加到&lt;code&gt;TaskMetadata&lt;/code&gt;接口：&lt;code&gt;committedOffsets&lt;/code&gt;，&lt;code&gt;endOffsets&lt;/code&gt;，和&lt;code&gt;timeCurrentIdlingStarted&lt;/code&gt;。这些方法可以允许 Streams 应用程序跟踪其任务的进度和运行状况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-740：清理公共 API &lt;code&gt;TaskId&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KIP-740代表了&lt;code&gt;TaskId&lt;/code&gt;该类的重大革新。有几种方法和所有内部字段已被弃用，新的&lt;code&gt;subtopology()&lt;/code&gt;和&lt;code&gt;partition()&lt;/code&gt;干将替换旧&lt;code&gt;topicGroupId&lt;/code&gt;和&lt;code&gt;partition&lt;/code&gt;字段（参见KIP-744的相关变化和修正KIP-740）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-744：迁移&lt;code&gt;TaskMetadata&lt;/code&gt;，并&lt;code&gt;ThreadMetadata&lt;/code&gt;与内部实现的接口&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;KIP-744将KIP-740提出的更改更进一步，并将实现与许多类的公共 API 分开。为了实现这一点，引入了新的接口&lt;code&gt;TaskMetadata&lt;/code&gt;、&lt;code&gt;ThreadMetadata&lt;/code&gt;和&lt;code&gt;StreamsMetadata&lt;/code&gt;，而弃用了具有相同名称的现有类。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-666：添加&lt;code&gt;Instant&lt;/code&gt;基于方法到&lt;code&gt;ReadOnlySessionStore&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;交互式查询 API 扩展了&lt;code&gt;ReadOnlySessionStore&lt;/code&gt;和&lt;code&gt;SessionStore&lt;/code&gt;接口中的一组新方法，这些方法接受&lt;code&gt;Instant&lt;/code&gt;数据类型的参数。此更改将影响需要实现新方法的任何自定义只读交互式查询会话存储实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-622：添加&lt;code&gt;currentSystemTimeMs&lt;/code&gt;和&lt;code&gt;currentStreamTimeMs&lt;/code&gt;到&lt;code&gt;ProcessorContext&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该&lt;code&gt;ProcessorContext&lt;/code&gt;增加在3.0两个新的方法，&lt;code&gt;currentSystemTimeMs&lt;/code&gt;和&lt;code&gt;currentStreamTimeMs&lt;/code&gt;。新方法使用户能够分别查询缓存的系统时间和流时间，并且可以在生产和测试代码中以统一的方式使用它们。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-743：删除&lt;code&gt;0.10.0-2.4&lt;/code&gt;Streams 内置指标版本配置的配置值&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;3.0 中取消了对 Streams 中内置指标的旧指标结构的支持。KIP-743正在&lt;code&gt;0.10.0-2.4&lt;/code&gt;从配置属性中删除该值&lt;code&gt;built.in.metrics.version&lt;/code&gt;。这&lt;code&gt;latest&lt;/code&gt;是目前此属性的唯一有效值（自 2.5 以来一直是默认值）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-741：将默认 SerDe 更改为 null&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;删除了默认 SerDe 属性的先前默认值。流过去默认为&lt;code&gt;ByteArraySerde&lt;/code&gt;. 用3.0开始，没有缺省，和用户需要任一组其的SerDes根据需要在API中或通过设置默认&lt;code&gt;DEFAULT_KEY_SERDE_CLASS_CONFIG&lt;/code&gt;和&lt;code&gt;DEFAULT_VALUE_SERDE_CLASS_CONFIG&lt;/code&gt;在它们的流配置。先前的默认值几乎总是不适用于实际应用程序，并且造成的混乱多于方便。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-733：更改 Kafka Streams 默认复制因子配置&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了主要版本的机会，Streams 配置属性的默认值&lt;code&gt;replication.factor&lt;/code&gt;会从 1 更改为 -1。这将允许新的 Streams 应用程序使用在 Kafka 代理中定义的默认复制因子，因此在它们转移到生产时不需要设置此配置值。请注意，新的默认值需要 Kafka Brokers 2.5 或更高版本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-732：弃用 eos-alpha 并用 eos-v2 替换 eos-beta&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 3.0 中不推荐使用的另一个 Streams 配置值是&lt;code&gt;exactly_once&lt;/code&gt;作为属性的值&lt;code&gt;processing.guarantee&lt;/code&gt;。该值&lt;code&gt;exactly_once&lt;/code&gt;对应于 Exactly Once Semantics (EOS) 的原始实现，可用于连接到 Kafka 集群版本 0.11.0 或更高版本的任何 Streams 应用程序。此EOS的第一实现已经通过流第二实施EOS的，这是由值表示取代&lt;code&gt;exactly_once_beta&lt;/code&gt;在&lt;code&gt;processing.guarantee&lt;/code&gt;性质。展望未来，该名称&lt;code&gt;exactly_once_beta&lt;/code&gt;也已弃用并替换为新名称&lt;code&gt;exactly_once_v2&lt;/code&gt;。在下一个主要版本 (4.0) 中，&lt;code&gt;exactly_once&lt;/code&gt;和&lt;code&gt;exactly_once_beta&lt;/code&gt;都将被删除，&lt;code&gt;exactly_once_v2&lt;/code&gt;作为 EOS 交付保证的唯一选项。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-725：优化 WindowedSerializer 和 WindowedDeserializer 的配置&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;配置属性&lt;code&gt;default.windowed.key.serde.inner&lt;/code&gt;和&lt;code&gt;default.windowed.value.serde.inner&lt;/code&gt;已弃用，取而代之的是&lt;code&gt;windowed.inner.class.serde&lt;/code&gt;供消费者客户端使用的单个新属性。建议 Kafka Streams 用户通过将其传递到 SerDe 构造函数来配置他们的窗口化 SerDe，然后在拓扑中使用它的任何地方提供 SerDe。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-633：弃用 Streams 中宽限期的 24 小时默认值&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Kafka Streams 中，允许窗口操作根据称为宽限期的配置属性处理窗口外的记录。以前，这个配置是可选的，很容易错过，导致默认为24小时。这是&lt;code&gt;Suppression&lt;/code&gt;运营商用户经常感到困惑的原因，因为它会缓冲记录直到宽限期结束，因此会增加 24 小时的延迟。在 3.0 中，&lt;code&gt;Windows&lt;/code&gt;类通过工厂方法得到增强，这些工厂方法要求它们使用自定义宽限期或根本没有宽限期来构造。已弃用默认宽限期为 24 小时的旧工厂方法，以及与&lt;code&gt;grace()&lt;/code&gt;已设置此配置的新工厂方法不兼容的相应API。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-623：&lt;code&gt;internal-topics&lt;/code&gt;为流应用程序重置工具添加“ ”选项&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过&lt;code&gt;kafka-streams-application-reset&lt;/code&gt;添加新的命令行参数，应用程序重置工具的 Streams 使用变得更加灵活：&lt;code&gt;--internal-topics&lt;/code&gt;. 新参数接受逗号分隔的主题名称列表，这些名称对应于可以使用此应用程序工具安排删除的内部主题。将此新参数与现有参数相结合，&lt;code&gt;--dry-run&lt;/code&gt;允许用户在实际执行删除操作之前确认将删除哪些主题并在必要时指定它们的子集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;&lt;span&gt;MirrorMaker&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-720：弃用 MirrorMaker v1&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 3.0 中，不推荐使用 MirrorMaker 的第一个版本。展望未来，新功能的开发和重大改进将集中在 MirrorMaker 2 (MM2) 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;KIP-716：允许使用 MirrorMaker2 配置偏移同步主题的位置&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 3.0 中，用户现在可以配置 MirrorMaker2 创建和存储用于转换消费者组偏移量的内部主题的位置。这将允许 MirrorMaker2 的用户将源 Kafka 集群维护为严格只读的集群，并使用不同的 Kafka 集群来存储偏移记录（即目标 Kafka 集群，甚至是源和目标集群之外的第三个集群）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Apache Kafka 3.0 是 Apache Kafka 项目向前迈出的重要一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.52&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/3EcuPIickyyEwbzUrMVnXJaXicEHYVRUz1Xar9X3Tic8ZUNJ3IfuVNP8wUpaKiafwYOY6lfXjlYbcJUJklKI7psbsg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>a96089b6ee9a4dc42ab63c68dde452a5</guid>
<title>WebRTC Mesh 架构</title>
<link>https://toutiao.io/k/xxezcd8</link>
<content:encoded>&lt;div&gt;&lt;div&gt;
&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>fc00bfa41535118684a591ca14f7e381</guid>
<title>浅谈数据流水线</title>
<link>https://toutiao.io/k/ieztx9k</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;entry-content&quot; itemprop=&quot;articleBody&quot;&gt;&amp;#13;
&lt;p&gt;当下我们听过很多热门的技术名词，例如：机器学习模型、推荐系统、高管驾驶舱、BI等等，在这些技术背后一个关键的角色就是：数据。这些数据通常不是单一的，原始的数据，而是需要从多个数据源获取，并经过复杂的提取、清洗、处理、加工等过程才能最终提供真正的价值。&lt;/p&gt;
&lt;p&gt;我们常说“数据是未来的石油”，其实也就是在说，数据并不是“开采”出来就可以直接提供价值的，而是要经过若干流程的“加工”和“提纯”才可以产生价值。而对于数据的加工和处理流程，我们通常将其称为&lt;strong&gt;数据流水线&lt;/strong&gt;，也就是 Data Pipeline。&lt;/p&gt;
&lt;h3&gt;什么是数据流水线&lt;/h3&gt;
&lt;p&gt;那么什么是数据流水线呢？数据流水线从广义来讲，会包含源数据的接入、数据的处理、数据交付、数据管理、数据治理以及数据的访问模式等，同时也包含狭义上的数据流水线，也即对数据做相应的ETL处理，本文我们将重点关注于狭义的数据流水线。&lt;/p&gt;
&lt;p&gt;它通常指从若干数据源中迁移数据，将迁移的数据进行复杂的数据处理之后，并加载到目标数据系统中的一系列流程，且数据的价值正是在每一步的流转中逐步产生的。数据流水线通常也是实现机器学习模型、数据分析、业务报表等技术的基础。&lt;/p&gt;
&lt;p&gt;数据流水线的复杂性取决于数据源的数据结构、数据质量以及我们要实现的业务需求。&lt;/p&gt;
&lt;p&gt;一条最简单的数据流水线可以只包含从一个数据源（例如：网络日志文件）复制数据，经过基础的数据清洗（例如：去除空值、无效值），再加载到目标数据仓库（例如：Hive）中。但通常情况下，一条数据流水线会包含多个步骤，包括多数据源的数据萃取、多步骤的数据处理、多环节的数据验证、有时还可能包含机器学习模型的训练和运行等任务。且这些不同的任务通常来自于不同的系统，并可以用不同的技术或语言来实现。&lt;/p&gt;
&lt;div id=&quot;attachment_14783&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;a href=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/1-data-pipeline-etl.png&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-14783&quot; src=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/1-data-pipeline-etl-768x273.png&quot; alt=&quot;&quot; class=&quot;size-medium_large wp-image-14783&quot; srcset=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/1-data-pipeline-etl-768x273.png 768w, https://insights.thoughtworks.cn/wp-content/uploads/2021/09/1-data-pipeline-etl-300x107.png 300w, https://insights.thoughtworks.cn/wp-content/uploads/2021/09/1-data-pipeline-etl.png 822w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/a&gt;&lt;p id=&quot;caption-attachment-14783&quot; class=&quot;wp-caption-text&quot;&gt;一条简单的数据流水线&lt;/p&gt;&lt;/div&gt;
&lt;h3&gt;为什么要构建数据流水线&lt;/h3&gt;
&lt;p&gt;我们都知道，冰山露出在水面之外的部分只占冰山整体的一小部分，而只有这些露出水面的部分才可以被人看到，但冰山的大部分都是隐藏在海水之下的。对于数据类相关的项目亦是如此，高管可能只关注放在他办公室的管理驾驶舱或仪表盘，业务部门可能只关注在某个推荐模型所带来产品点击率是否得到提升，产品部门可能主要关注在某个分货优化模型是否带来了产品分货效率的提升等等。但这些人们所能看到的也只是冰山的一角罢了。&lt;/p&gt;
&lt;p&gt;要生成每一个报表，每一个模型，都依赖于一个复杂的机制，而这些隐藏在水面之下的部分通常很难被人所理解。因为要产生这些价值，都需要不断对数据进行加工和处理，不仅仅包含从数据源中提取数据并加载进入目标的数据系统，更包含过程中对于数据的清洗、数据的结构化和规范化、指标逻辑的处理与计算、数据隐私与安全性的处理、分析型数据立方体的预计算等任务。而对数据进行加工处理的这些过程，通过不断的抽象，就可以将冗杂的代码，甚至是有一些重复的代码，通过一种更高效的模式表达，也就是数据流水线。&lt;/p&gt;
&lt;p&gt;数据流水线就是为了能够高效组织并运行这些不同阶段的任务。因此，在上面提到的例子中，在每一个报表或模型生成的背后，都至少有一条数据流水线在后台运行着，以支撑报表或模型最终为用户产生价值。通过数据流水线的方式处理数据，就可以更好的追踪数据的血缘关系，也通过开发一些通用的中间层数据，从而减少重复的开发工作。同时，数据流水线也是一种将复杂的问题逐步分解的过程，在每一条数据流水线中只处理单一的事情，最终以将复杂的问题简单化，也更有利于维护数据的准确性。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/2-data-pipeline-etl.png&quot;&gt;&lt;img src=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/2-data-pipeline-etl.png&quot; alt=&quot;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;如何构建数据流水线&lt;/h3&gt;
&lt;p&gt;现在我们知道在一条数据流水线中既包括了从数据源中提取数据、加载处理好的数据到目标数据源中，还包括对数据的清洗、处理、加工、建模、分析等等一系列复杂的任务。可以说，每构建一条数据流水线对于数据工程师都是一次新的挑战，因为我们总会遇到不同的数据源，不同的基础架构，也为了不同的数据分析目标和模型而构建。&lt;/p&gt;
&lt;p&gt;但值得庆幸的是，有一些比较通用的模式已经比较成熟了，我们可以直接按照模式中对数据流水线的抽象来构建不同的数据流水线，其中最经典的模式莫过于 ETL （Extract、Transform、Load）了，从狭义上来讲，很多时候我们就可以直接把 ETL 看做是数据流水线，而不是数据流水线所遵循的模式了。&lt;/p&gt;
&lt;p&gt;ETL 作为最广为人知的数据流水线模式，主要是指数据从提取，经过转换，再最终加载到目标数据源的过程，以提高数据的管理效率，可以帮助数据开发人员更快速的迭代以不断满足业务发展的数据需求。下面我们就对其稍微详细的介绍一下。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/3-data-pipeline-etl.png&quot;&gt;&lt;img src=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/3-data-pipeline-etl.png&quot; alt=&quot;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Extract（提取）&lt;/h4&gt;
&lt;p&gt;“提取”步骤通常是数据流水线的起点，主要是指从各种数据源中收集数据，为数据的转换和加载做准备。&lt;br/&gt;
由于数据源中的数据还未经过任何处理和加工，通常其数据质量都无法保障，且现在大多数的组织中，为了支持各自业务的发展，通常组织内部都拥有多个数据源，且数据源的类型也不仅仅是数据库类型，通常还会包含文件、API或事件消息类型。因此，当我们决定要接入一个或多个数据源时，建议能够从以下几个方面加以考虑。&lt;/p&gt;
&lt;h4&gt;数据源的所有权&lt;/h4&gt;
&lt;p&gt;为了构建一个数据产品，数据团队通常需要从多个源系统中提取数据，这些系统可能由不同的团队和组织管理，甚至还需要接入一些第三方平台，以获取外部数据提供输入。例如：一个电商平台，它的订单和商品数据可能存在关系型数据库 Postgres 中，但它们可能还需要同时接入第三方网站分析平台来追踪其网站上用户对网站的使用情况，这种情况下就至少需要接入两个不同的数据源以了解客户行为。&lt;/p&gt;
&lt;p&gt;了解源系统的所有权是很重要的。对于第三方系统来说，它们可能会对开放出来的数据有所限制，并不是所有的数据信息都是开放可供访问的。对于第三方系统数据的访问方式来说，当前大多数服务提供商都会提供 Rest API 供客户访问，但很少有服务提供商会直接提供数据库供客户连接。在这种情况下，我们就要在数据流水线中提供能够接入 Rest API 类型数据的基础设施，以支持此类数据的获取。&lt;/p&gt;
&lt;p&gt;同样，对于大多数内部的系统也是如此。通常情况下，组织内部大多数业务系统的数据库在设计之初很少考虑到之后被数据团队大规模提取的场景，比如：数据库中未设置“最新更新时间”类似字段，导致数据库难以做到增量更新等。同样还有数据库访问方式的问题，能否直接连接到它们的数据库读取数据还是要再同步数据库到备库中才能供数据团队提取数据，这些问题都是需要和对方系统的团队成员确认清楚的，有时还会需要对方系统做出一些更改，幸运的话，他们可能会很愿意合作，但通常情况下合作都会遇到一些阻碍。&lt;/p&gt;
&lt;h4&gt;数据接入形式和数据结构&lt;/h4&gt;
&lt;p&gt;无论是内部数据源或是外部第三方数据源，当我们在接入一个新的数据源时，都要先搞明白对方的数据接入形式以及对应数据的数据结构。常见的数据接入形式如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;业务系统背后连接的数据库，例如：MySQL、Postgres、MongoDB 等&lt;/li&gt;
&lt;li&gt;Rest API&lt;/li&gt;
&lt;li&gt;消息流，例如：Apache Kafka&lt;/li&gt;
&lt;li&gt;文件类型，例如：CSV 文件、Excel 文件、网络访问日志等，这些文件通常会存储在网络存储服务（例如：FTP）或云存储服务（例如：Amazon S3）中。&lt;/li&gt;
&lt;li&gt;数据仓库或数据湖，例如：Apache Hive&lt;/li&gt;
&lt;li&gt;HDFS 或 HBase数据库&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了要理清楚如何能够接入到数据源中，还要明白数据源的数据结构是什么样的。常见的数据结构类型如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结构化数据

&lt;/li&gt;
&lt;li&gt;半结构化数据
&lt;ul&gt;
&lt;li&gt;Rest API 返回的 JSON数据&lt;/li&gt;
&lt;li&gt;MySQL 或 Postgres 等数据库中存储的 JSON 类型的数据&lt;/li&gt;
&lt;li&gt;JSON 类型的文件&lt;/li&gt;
&lt;li&gt;XML文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;非结构化数据

&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每种不同结构的数据都有其各自的优势及挑战。对于结构化数据来说，由于它结构良好，非常易于提取和处理，但同时这种结构一般是为了业务系统的实现而结构化设计的，在提取到数据之后，通常还需要一些工作来对数据进行清理，并重新建模以适应数据项目中之后的数据分析需求。&lt;/p&gt;
&lt;p&gt;半结构化数据（例如：JSON）当前已经得到较多的使用了，但是和结构化数据不同的是，我们不能保证每个数据集中的每个数据都拥有相同的数据结构。因此，这就为在数据流水线中如何保证数据不会丢失，以及数据的完整性提出了要求。&lt;/p&gt;
&lt;p&gt;而非结构化数据其实在组织中也是非常常见的，许多企业在进行数字化转型的过程中有大量的历史数据文件保留了下来，而为了应对后续数据分析的需求，这些文件的数据输入又是十分重要的，在这种情况下，能够高效的将文件这种非结构化数据接入到数据项目中就非常重要了。例如：产品销量的预测需要大量的历史数据作为支撑，NLP 模型需要依赖于大量的文本数据来训练等。因此，当我们识别出来在一个数据项目中需要对接文件类型的数据时，我们就要考虑如何能够高效的、增量的接入文件数据，并对文件数据进行解析和处理。&lt;/p&gt;
&lt;h4&gt;数据量&lt;/h4&gt;
&lt;p&gt;在接入数据源时，还要考虑到数据源的数据量的大小，是 TB 级别的数据，还是 PB 级别的数据，以及数据源的更新频率是什么样的。首先，数据源中数据量的大小并不绝对意味着数据中所蕴含价值的高低，因此对于大数据集或小数据集，我们在接入数据时都应该同等对待。很多情况下我们可能需要同时接入大数据集与小数据集，然后将它们 join 起来再一同进入数据处理阶段，这时还需要谨慎地对数据进行建模，以避免大小量级的数据源之间的互相影响，因此需要针对不同量级的数据源设计不同的技术与接入方法。&lt;/p&gt;
&lt;p&gt;对于大数据集，我们通常会在首次基于 Apache Sqoop 进行全量数据提取，之后通过 SparkSQL 按照数据的最近更新时间进行增量的更新；而对于小数据集的提取可以灵活一些，由于其数据量不大，进行数据提取时所消耗的资源不多，这样我们既可以通过全量+增量的方式进行数据提取，也可以每次都进行全量的数据提取。&lt;/p&gt;
&lt;h4&gt;数据质量&lt;/h4&gt;
&lt;p&gt;数据源不仅数据结构与接入方式多种多样，就连数据源的质量也都千差万别。在接入数据源时，要能够有效的对数据源的质量进行识别，了解数据源在设计时的一些限制或者缺陷，并尝试在数据流水线的某个环节对其进行修复。常见的脏数据通常具备以下特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;重复记录&lt;/li&gt;
&lt;li&gt;让人模棱两可的记录&lt;/li&gt;
&lt;li&gt;被孤立的记录，即某个记录的外键值引用了不存在的主键&lt;/li&gt;
&lt;li&gt;记录不完整或丢失字段&lt;/li&gt;
&lt;li&gt;记录编码错误&lt;/li&gt;
&lt;li&gt;记录之间的格式不一致，例如：日期有些存储为 2020-12-06，有些记录却存储为 2020.12.06&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了上述这些特征，脏数据通常还可能以其他各种形式存在。当前应该还没有任何银弹能够保证数据源的数据是绝对干净、完整和一致的，因此在数据流水线中对数据进行处理时只能假设我们将会遇到最脏的数据，在数据流水线的不同阶段对数据源进行不断的清洗和验证，以不断得到最干净整洁的数据。&lt;/p&gt;
&lt;h4&gt;数据源的带宽和延迟问题&lt;/h4&gt;
&lt;p&gt;了解了数据源的数据结构和数据质量等问题之后，还有一点最好也要加以考虑，就是数据源的带宽和延迟问题。由于我们通常接入的是大规模的数据，在这种情况下，当我们接入的是源系统的数据库，那么可能会有延迟的问题，一旦我们大规模的读取数据，是否会对源系统造成影响，是否需要同步数据源到备库之后再供我们来提取数据呢。当接入的是 Rest API 或消息类型的数据时，那么数据的推送速率是多少呢。&lt;/p&gt;
&lt;p&gt;总之，当我们对数据源了解的越多，就越能够帮助我们设计出高效的数据处理流水线。&lt;/p&gt;
&lt;h4&gt;Transform（转换）&lt;/h4&gt;
&lt;p&gt;一般情况下我们会将数据仓库分为 ODS（Operation Data Store，又称操作数据层）和 DW（Data Warehouse，又称数据仓库层）两层。当我们从数据源中提取到数据之后，我们会先将数据存储在 ODS 层对数据进行清洗，将脏数据或不完整的数据进行处理或者过滤，之后再从 ODS 层向 DW 层进行转换，这时我们通常会按照不同的业务域对不同的数据进行主题域的划分，再对不同的主题域进行数据建模，之后再根据不同的业务指标或规则对数据进行计算、分析和聚合。&lt;/p&gt;
&lt;p&gt;数据清洗主要是希望解决数据源的数据质量问题，如之前我们提到的，数据源中可能会存在重复、模棱两可、被孤立的数据记录等问题存在，我们希望在数据清洗过程中将数据质量进行提高。但是，数据清洗工作不是一蹴而就的，而是一个反复不断的过程，只能不断的发现问题，再解决问题。由于我们不知道源数据可能会出现什么形式的质量问题，并且也不可能阻止暂时有质量问题的数据进入 ODS 层，但我们一定知道哪些数据是有用的，因此我们要注意在进行数据清洗的过程中一定不要将有用的数据过滤掉，且对每一个数据清洗规则或过滤规则进行验证和确认。&lt;/p&gt;
&lt;p&gt;数据转换过程主要包含数据的统一、数据粒度的确认以及指标和规则的计算工作。在数据转换过程中，需要对不同系统提取来的数据进行整合，将不同数据源中的相同类型的数据进行统一，例如：在 A 系统中的一个客户编码是 AA001，但同一个客户在 B 系统中的客户编码却是BB001，这样在转换过程中就需要和客户确认，明确其规则，尽可能对其统一从而转换为一个编码。此外，在业务系统中一般都存储的是非常明细的数据，但数据仓库中的数据是为了分析的，不需要特别明细的数据，这时在转换过程中，可以将业务系统中的数据按照数据仓库的粒度进行聚合。最后，就是在对数据进行分析的过程中，我们通常会按照设计好的指标体系或不同的业务规则，在数据转换的过程中对各个指标和规则进行计算，并将计算好的数据存储起来，这样就可以在后续的数据分析过程中使用。&lt;/p&gt;
&lt;h4&gt;Load（加载）&lt;/h4&gt;
&lt;p&gt;了解了数据的提取和转换，数据加载就比较容易理解了。当数据从数据源中提取出来，经过 ODS 层和 DW 层的数据清洗和转换，再将计算好的数据持久化存储到目标数据源中的过程就是数据加载。通常在实际的工作中，数据加载还需要结合所采用的数据库系统（Oracle、MySQL、Impala 等）来确定最佳的数据加载方案，从而最大化的节约 CPU、硬盘 IO 以及网络资源。&lt;/p&gt;
&lt;h3&gt;如何管理数据流水线&lt;/h3&gt;
&lt;p&gt;随着组织内数据流水线的数量以及复杂性的增加，对数据流水线中的任务的管理也变得越来越复杂，这时就需要引入任务调度平台了，通过任务调度平台对数据流水线中的多个任务进行管理和调度。假如我们有一条数据流水线，其中包含了一个用 Python 编写的数据提取的任务，以及用 Spark SQL编写的数据转换任务等，这些任务必须在全天按照特定的时间和顺序来执行。虽然在 Linux 系统中，也可以通过传统的 cron job 启动定时任务达到类似的效果，但随着任务数量的增多，以及任务之间依赖的复杂度提升，要想管理这些任务及其之间的依赖关系就是一个非常大的挑战了，但好在当下已经有很多用来进行任务的调度和编排的工具可供选择了，例如：Apache Airflow、Apache Oozie、Azkaban。&lt;/p&gt;
&lt;p&gt;数据流水线中的任务的执行通常都是有方向的，也就是说它们通常以一个或多个任务开始，并以另一个或几个任务作为结束，且后面的任务在其所依赖的任务未完成之前是不会运行的，这样就可以保证任务从开始到结束是有一个明确的有方向的执行路径的。此外，数据流水线中的任务也是非循环的，也就是说一个任务只能依赖于其前面任务的执行，无法再指向先前已经完成的任务，即无法循环执行一个已经完成的任务，否则的话，数据流水线就可能会无穷尽的运行下去了。&lt;/p&gt;
&lt;p&gt;考虑到数据流水线中“有方向”和“非循环”这两个约束，基本所有的任务调度工具都会将其工作流编排为有向无环图（DAG）的形式。如下图所示是一条简单的数据流水线。在该示例中，必须先完成任务 A，才能启动任务 B 和任务 C，且直到任务 B 和任务 C 全部完成之后，任务 D 才可以开始运行，当任务 D 完成之后，这条数据流水线才算完成。&lt;/p&gt;
&lt;div id=&quot;attachment_14786&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;a href=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/4-data-pipeline-etl.png&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-14786&quot; src=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/4-data-pipeline-etl-768x269.png&quot; alt=&quot;&quot; class=&quot;size-medium_large wp-image-14786&quot; srcset=&quot;https://insights.thoughtworks.cn/wp-content/uploads/2021/09/4-data-pipeline-etl-768x269.png 768w, https://insights.thoughtworks.cn/wp-content/uploads/2021/09/4-data-pipeline-etl-300x105.png 300w, https://insights.thoughtworks.cn/wp-content/uploads/2021/09/4-data-pipeline-etl.png 822w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/a&gt;&lt;p id=&quot;caption-attachment-14786&quot; class=&quot;wp-caption-text&quot;&gt;采用 DAG 形式表示的 ETL 任务&lt;/p&gt;&lt;/div&gt;
&lt;p&gt;此外，虽然我们用 DAG 来表示一组任务的执行方式，但这只是一种表示方式，并不是定义任务逻辑的实际位置，在任务调度平台中可以定义各种类型的任务。还以上图为例，其中 query_and_extract 任务可能是一个 python 脚本用来从数据源中提取数据，clean_and_transform_A 任务可能是一段 SQL 脚本用来对数据进行清洗和转换，而load_data_to_dw 可能仅仅是一段 shell 脚本来将处理过的数据加载到数据仓库中。任务编排工具只是负责任务执行顺序的编排和运行，这些任务仍然会以不同的形式在各自不同的节点上运行。&lt;/p&gt;
&lt;h3&gt;总结&lt;/h3&gt;
&lt;p&gt;本文对数据流水线及其主要的构建模式 ETL 进行了简要的介绍。源数据会在数据流水线的各个阶段中移动，从数据的采集，到数据的清洗和转换，再到数据的加载，数据的价值也在一步步的转移过程中逐步体现出来。&lt;/p&gt;
&lt;p&gt;一条数据流水线可能会帮助构建供财务使用的 OLAP 多维分析数据集，也可能会帮助数据科学家来处理数据并训练算法模型等等。总之，数据流水线就像一条数据供应链一般，它可以帮助处理、优化和丰富数据，供各种业务和应用程序使用，从而不断的为企业和组织带来价值。&lt;/p&gt;
&amp;#13;
&lt;/div&gt;&amp;#13;
&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>