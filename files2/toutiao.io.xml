<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
            <title>开发者头条</title>
            <link>http://toutiao.io/</link>
            <description></description>
<item>
<guid>1b27505659a7175f349f6aebb08695fa</guid>
<title>从 Spark 做批处理到 Flink 做流批一体</title>
<link>https://toutiao.io/k/tryss1z</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;p id=&quot;js_tags&quot; class=&quot;article-tag__list single-tag__wrp js_single&quot; data-len=&quot;1&quot; role=&quot;link&quot; aria-labelledby=&quot;js_article-tag-card__left&quot; aria-describedby=&quot;js_article-tag-card__right&quot;&gt;
                                            
                                                                                    &lt;span aria-hidden=&quot;true&quot; id=&quot;js_article-tag-card__left&quot; class=&quot;article-tag-card__left&quot;&gt;
                                    &lt;span class=&quot;article-tag-card__title&quot;&gt;收录于话题&lt;/span&gt;
                                    &lt;span class=&quot;article-tag__item-wrp no-active js_tag&quot; data-url=&quot;https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzU3Mzg4OTMyNQ==&amp;amp;action=getalbum&amp;amp;album_id=1929702550740484100#wechat_redirect&quot; data-tag_id=&quot;&quot; data-album_id=&quot;1929702550740484100&quot; data-tag_source=&quot;4&quot;&gt;
                                        &lt;span class=&quot;article-tag__item&quot;&gt;#行业案例&lt;/span&gt;
                                    &lt;/span&gt;
                                &lt;/span&gt;
                                &lt;span aria-hidden=&quot;true&quot; id=&quot;js_article-tag-card__right&quot; class=&quot;article-tag-card__right&quot;&gt;52个&lt;span class=&quot;weui-hidden_abs&quot;&gt;内容&lt;/span&gt;&lt;/span&gt;
                                                                                        &lt;/p&gt;

                
                                
                
                

                
                                                                

                
                                


                
                
                
                
                                                
                                                                
                                
                                
                
                &lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;section&gt;&lt;span&gt;&lt;span&gt;▼ 关注「&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Flink 中文社区&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;」，获取更多技术干货 &lt;span&gt;▼&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section class=&quot;mp_profile_iframe_wrp&quot;&gt;&lt;mpprofile class=&quot;js_uneditable custom_select_card mp_profile_iframe&quot; data-pluginname=&quot;mpprofile&quot; data-id=&quot;MzU3Mzg4OTMyNQ==&quot; data-headimg=&quot;http://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu6FJHxaI14AsXuzeg4SybT0hiaCSohrIY75oiaOMzhQU7RouiafjNa76k2CtD6xxB2JqnawqFqV3zg3A/0?wx_fmt=png&quot; data-nickname=&quot;Flink 中文社区&quot; data-alias=&quot;&quot; data-signature=&quot;Apache Flink 官微，Flink PMC 维护&quot; data-from=&quot;0&quot;/&gt;&lt;/section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section donone=&quot;shifuMouseDownCard(&#x27;shifu_c_008&#x27;)&quot; label=&quot;Copyright Reserved by PLAYHUDONG.&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section donone=&quot;shifuMouseDownCard(&#x27;shifu_c_008&#x27;)&quot; label=&quot;Copyright Reserved by PLAYHUDONG.&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section donone=&quot;shifuMouseDownCard(&#x27;shifu_c_008&#x27;)&quot; label=&quot;Copyright Reserved by PLAYHUDONG.&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;摘要：&lt;/strong&gt;&lt;/span&gt;本⽂由社区志愿者苗文婷整理，内容来源⾃ LinkedIn 大数据高级开发工程师张晨娅在 Flink Forward Asia 2020 分享的《从 Spark 做批处理到 Flink 做流批一体》，主要内容为：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ol class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;为什么要做流批一体？&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;当前行业已有的解决方案和现状，优势和劣势&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;探索生产实践场景的经验&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;Shuflle Service 在 Spark 和 Flink 上的对比，以及 Flink 社区后面可以考虑做的工作&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;总结&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ol&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Tips：&lt;/strong&gt;FFA 2021 重磅开启，点击&lt;/span&gt;&lt;span&gt;&lt;strong&gt;「阅读原&lt;/strong&gt;&lt;strong&gt;文」&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;即可报名～&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;img data-ratio=&quot;1&quot; data-type=&quot;png&quot; data-w=&quot;20&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu78FqIxdIQicVe5cg78bpax1XDKxMS06V8h6bib5fhicN8n5zK7Z4oDWWgzgbAeCibuKRnD5eibTcg73mg/640?wx_fmt=png&quot;/&gt; GitHub 地址 &lt;img data-ratio=&quot;1&quot; data-type=&quot;png&quot; data-w=&quot;20&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu78FqIxdIQicVe5cg78bpax1XDKxMS06V8h6bib5fhicN8n5zK7Z4oDWWgzgbAeCibuKRnD5eibTcg73mg/640?wx_fmt=png&quot;/&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;欢迎大家给 Flink 点赞送 star~&lt;/span&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-galleryid=&quot;&quot; data-ratio=&quot;1.024390243902439&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu58p6JubKoFyrKVibtOyk1CTpJialGPpBBg6uRknWESa1xwDsR8yeKiah9z0lnproCED8dp6l4bmfgQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;492&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;一、为什么要做流批一体&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p cid=&quot;n17&quot; mdtype=&quot;paragraph&quot;&gt;&lt;/p&gt;&lt;p cid=&quot;n17&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvR500asINpicaZXVQWQqwkS0seuG0KcPkxLsfw9wxJdlZhdewtibU08KQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;做流批一体到底有哪些益处，尤其是在 BI/AI/ETL 的场景下。整体来看，如果能帮助用户做到流批一体，会有以上 4 个比较明显的益处：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;可以避免代码重复，复用代码核心处理逻辑&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;代码逻辑能完全一致是最好的，但这会有一定的难度。&lt;span&gt;但整体来讲，现在的商业逻辑越来越长，越来越复杂，要求也很多，如果我们使用不同的框架，不同的引擎，用户每次都要重新写一遍逻辑，压力很大并且难以维护。所以整体来讲，尽量避免代码重复，帮助用户复用代码逻辑，就显得尤为重要。&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;流批一体有两个方向&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这两个方向要考虑的问题很不一样，目前 Flink 做 Streaming、Spark 做 Batch 等等一些框架在批处理或流处理上都比较成熟，都已经产生了很多的单方面用户。&lt;span&gt;当我们想帮助用户移到另外一个方向上时，比如一些商业需求，通常会分成两类，是先从流处理开始到批处理，还是从批处理开始到流处理。之后介绍的两个生产实践场景案例，正好对应这两个方向。&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;减少维护工作量&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;避免维护多套系统，系统之间的差异可能非常大，框架和引擎都不一样，会带来比较多的问题。&lt;span&gt;如果公司内部有多条 pipeline ，一个实时一个离线，会造成数据不一致性，因此会在数据验证、数据准确性查询、数据存储等方面做很多工作，尽量去维护数据的一致性。&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;学习更多&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;框架和引擎很多，商业逻辑既要跑实时，也要跑离线，所以，支持用户时需要学习很多东西。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;二、当前行业现状&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p cid=&quot;n17&quot; mdtype=&quot;paragraph&quot;&gt;&lt;/p&gt;&lt;p cid=&quot;n42&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvgHibquWicWEqKvrqzC7wCRPjgzr3uLssks17l6DlcaCu28OPUsHhb8oQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Flink 和 Spark 都是同时支持流处理和批处理的引擎。我们一致认为 Flink 的流处理做的比较好，那么它的批处理能做到多好？同时，Spark 的批处理做的比较好，那么它的流处理能不能足够帮助用户解决现有的需求？&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;现在有各种各样的引擎框架，能不能在它们之上有一个统一的框架，类似于联邦处理或者是一些简单的 physical API，比如 Beam API 或者是自定义接口。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Beam 方面需要考虑的问题，是它在批处理和流处理上的优化能做到多好？Beam 目前还是偏物理执行，之后的计划是我们需要考究的。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;LinkedIn，包括其他公司，会考虑做一些自定义接口的解决方案，考虑有一个共通的 SQL 层，通用的 SQL 或 API 层，底下跑不同的框架引擎。这里需要考虑的问题是，像 Spark 、Flink 都是比较成熟的框架了，已经拥有大量的用户群体。当我们提出一个新的 API ，一个新的解决方案，用户的接受度如何? 在公司内部应该如何维护一套新的解决方案？&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;三、生产案例场景&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p cid=&quot;n17&quot; mdtype=&quot;paragraph&quot;&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p cid=&quot;n17&quot; mdtype=&quot;paragraph&quot;&gt;&lt;span&gt;后面内容主要聚焦在 Flink 做 batch 的效果，Flink 和 Spark 的简单对比，以及 LinkedIn 内部的一些解决方案。分享两个生产上的实例场景，一个是在机器学习特征工程生成时如何做流批一体，另一个是复杂的 ETL 数据流中如何做流批一体。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h2 cid=&quot;n51&quot; mdtype=&quot;heading&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3.1 案例 A - 机器学习特征工程&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p cid=&quot;n52&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvxicicoJIj0zibGnT30NsUPG2jGq0tA1HGtAKICPLnF4XRC3EDBXt8nU2w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;第一类方向，流处理 -&amp;gt; 批处理，归类为流批一体。&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;案例 A 的主体逻辑是在机器学习中做特征生成时，如何从流处理到批处理的流批一体。核心的业务逻辑就是特征转换，转化的过程和逻辑比较复杂，用它做一些标准化。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;比如在 LinkedIn 的页面上输入的一些会员信息背景等，需要将这些信息提取出来标准化掉，才能进行一些推荐，帮你找一些工作等等。当会员的身份信息有更新时，会有过滤、预处理的逻辑、包括读取 Kafka 的过程，做特征转换的过程中，可能会有一些小表查询。这个逻辑是非常直接的，没有复杂的 join 操作及其他的数据处理过程。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n56&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvd2vH2rENaw6eBTicW4YPPkzgJcJthDqOF87edz5fbKmhOZzI2r78txg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;以前它的 pipeline 是实时的，需要定期从离线 pipeline 中读取补充信息来更新流。这种 backfill 对实时集群的压力是很大的，在 backfill 时，需要等待 backfill 工作起来，需要监控工作流不让实时集群宕掉。所以，用户提出能不能做离线的 backfill，不想通过实时流处理做 backfill。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;当前我们的用户是使用 Beam on Samza 做流处理，他们非常熟悉 Beam API 和 Spark Dataset API，也会用 Dataset API 去做除了 backfill 之外的一些其他业务处理。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;需要特别强调的是， Dataset API 很多都是直接对 Object 操作，对 type 安全性要求很高，如果建议这些用户直接改成 SQL 或者 DataFrame 等 workflow 是不切实际的，因为他们已有的业务逻辑都是对 Object 进行直接操作和转化等。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n60&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvf0iclysXk2ZS7puelqu4oMJuzGVasbgMHdRlL83xKGYibAz8dnOSJYRw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;在这个案例下，我们能提供给用户一些方案选择，Imperative API 。看下业界提供的方案：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n70&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvQpyvspwiarPvfp0CnbzibpXZqz62EdqrfEEIIoXV6CHdnSGPO22Y9LDw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;从用户的反馈上来说，Flink 的 DataStream (DataSet) API 和 Spark 的 Dataset API 在用户 interface 上是非常接近的。作为 Infra 工程师来说，想要帮用户解决问题，对 API 的熟悉程度就比较重要了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;但是 Beam 和 Flink 、Spark 的 API 是非常不一样的，它是 Google 的一条生态系统，我们之前也帮助用户解决了一些问题，他们的 workflow 是在 Beam on Samza 上，他们用 p collections 或者 p transformation 写了一些业务逻辑，output、input 方法的 signature 都很不一样，我们开发了一些轻量级 converter 帮助用户复用已有的业务逻辑，能够更好的用在重新写的 Flink 或 Spark 作业里。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;从 DAG 上来看，案例 A 是一个非常简单的业务流程，就是简单直接的对 Object 进行转换。Flink 和 Spark 在这个案例下，性能表现上是非常接近的。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n77&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvrUBWItmKI4Et279Vlk66lcTCNthAx7ujXy6ewdcibT3ewQZdR5ByvSA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;通常，我们会用 Flink Dashboard UI 看一些异常、业务流程等，相比 Spark 来说是一个比较明显的优势。Spark 去查询 Driver log，查询异常是比较麻烦的。但是 Flink 依旧有几个需要提升的地方：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;History Server - 支持更丰富的 Metrics 等&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n83&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJv8ZbdmNNMT3Y9p2sf9QjWs9PvPt0Glv0AACzMStXO0aibQne8HsjIK2w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Spark History Server UI 呈现的 metrics 比较丰富的，对用户做性能分析的帮助是比较大的。Flink 做批处理的地方是否也能让 Spark 用户能看到同等的 metrics 信息量，来降低用户的开发难度，提高用户的开发效率。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;p cid=&quot;n89&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvcGM1uaLVuZhTTjhzpJnDAqkuw3tqyF02XUj42Vw2SX6ILmcGKrSkIA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;分享一个 LinkedIn 从两三年前就在做的事情。LinkedIn 每天有 200000 的作业跑在集群上，需要更好的工具支持批处理用户运维自己的作业，我们提供了 Dr. Elephant 和 GridBench 来帮助用户调试和运维自己的作业。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Dr. Elephant 已开源，能帮助用户更好的调试作业，发现问题并提供建议。另外，从测试集群到生产集群之前，会根据 Dr. Elephant 生成的报告里评估结果的分数来决定是否允许投产。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;GridBench 主要是做一些数据统计分析，包括 CPU 的方法热点分析等，帮助用户优化提升自己的作业。GridBench 后续也有计划开源，可以支持各种引擎框架，包括可以把 Flink 加进来，Flink job 可以用 GridBench 更好的做评估。GridBench Talk: Project Optimum: Spark Performance at LinkedIn Scale [7]。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;用户不仅可以看到 GridBench 生成的报告，Dr. Elephant 生成的报告，也可以通过命令行看到 job 的一些最基本信息，应用 CPU 时间、资源消耗等，还可以对不同 Spark job 和 Flink job 之间进行对比分析。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;以上就是 Flink 批处理需要提升的两块地方。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;h2 cid=&quot;n101&quot; mdtype=&quot;heading&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3.2 案例 B - 复杂的 ETL 数据流&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p cid=&quot;n102&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJv6IPLWnsnRUwOQLPNe89TMzh2pOpxfHC6gkicc7KVDosXbmQeQGDsNxQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;第二类方向，批处理 -&amp;gt; 流处理，归类为流批一体。&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;ETL 数据流的核心逻辑相对复杂一些，比如包括 session window 聚合窗口，每个小时计算一次页面的用户浏览量，分不同的作业，中间共享 metadata table 中的 page key，第一个作业处理 00 时间点，第二个作业处理 01 时间点，做一些 sessionize 的操作，最后输出结果，分 open session、close session ，以此来做增量处理每个小时的数据。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n107&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJv5nAU6NTXTvfNQWLKekaHFicvHpgkJS8CJ4qZoTQrMWuhdFoiaticibIeUg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;这个 workflow 原先是通过 Spark SQL 做的离线增量处理，是纯离线的增量处理。当用户想把作业移到线上做一些实时处理，需要重新搭建一个比如 Beam On Samza 的实时的 workflow，在搭建过程中我们和用户有非常紧密的联系和沟通，用户是遇到非常多的问题的，包括整个开发逻辑的复用，确保两条业务逻辑产生相同的结果，以及数据最终存储的地方等等，花了很长时间迁移，最终效果是不太好的。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;另外，用户的作业逻辑里同时用 Hive 和 Spark 写了非常多很大很复杂的 UDF ，这块迁移也是非常大的工作量。用户对 Spark SQL 和 Spark DataFrame API 是比较熟悉的。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;上图中的黑色实线是实时处理的过程，灰色箭头主要是批处理的过程，相当于是一个 Lambda 的结构。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n112&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvXF6TAkRZOEwbctsxUGIDIl3ibd7mnTRZLEUcnXRz1tTrMlzxShDAFvQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;针对案例 B，作业中包括很多 join 和 session window，他们之前也是用 Spark SQL 开发作业的。很明显我们要从 Declartive API 入手，当前提供了 3 种方案：&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;第一个选择是 Flink Table API/SQL ，流处理批处理都可以做，同样的SQL，功能支持很全面，流处理和批处理也都有优化。可以看下文章 Alibaba Cloud Blog: What&#x27;s All Involved with Blink Merging with Apache Flink? [8] 和 FLINK-11439 INSERT INTO flink_sql SELECT * FROM blink_sql [9]。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;第二个选择是  Spark DataFrame API/SQL ，也是可以用相同的 interface 做批处理和流处理，但是 Spark 的流处理支持力度还是不够的。可以看下文章 Databricks Blog: Deep Dive into Spark SQL’s Catalyst Optimizer [10] 和 Databricks Blog: Project Tungsten: Bringing Apache Spark Closer to Bare Metal [11]。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;第三个选择是 Beam Schema Aware API/SQL ，Beam 更多的是物理的 API ，在 Schema Aware API/SQL 上目前都在开展比较早期的工作，暂不考虑。所以，之后的主要分析结果和经验都是从 Flink Table API/SQL 和 Spark DataFrame API/SQL 的之间的对比得出来的。可以看下文章 Beam Design Document - Schema-Aware PCollections [12] 和 Beam User Guide - Beam SQL overview [13]。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n122&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvlWlcaicb7jOsuNYHQMoTPJRfC2Shj5Ub286425K651GOl3upu9FQX5Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;从用户的角度来说，Flink Table API/SQL 和 Spark DataFrame API/SQL 是非常接近的，有一些比较小的差别，比如 keywords、rules、 join 具体怎么写等等，也会给用户带来一定的困扰，会怀疑自己是不是用错了。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Flink 和 Spark 都很好的集成了 Hive ，比如 HIve UDF 复用等，对案例B中的 UDF 迁移，减轻了一半的迁移压力。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Flink 在 pipeline 模式下的性能是明显优于 Spark 的，可想而知，要不要落盘对性能影响肯定是比较大的，如果需要大量落盘，每个 stage 都要把数据落到磁盘上，再重新读出来，肯定是要比不落盘的 pipeline 模式的处理性能要差的。pipeline 比较适合短小的处理，在 20 分钟 40 分钟还是有比较大的优势的，如果再长的 pipeline 的容错性肯定不能和 batch 模式相比。Spark 的 batch 性能还是要比 Flink 好一些的。这一块需要根据自己公司内部的案例进行评估。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n128&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvfseKEG6efTCuX52ibWAJcUx248o8alED0VHmv1bV0GoM4HaSkWXOibcQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Flink 对 window 的支持明显比其他引擎要丰富的多，比如 session window，用户用起来非常方便。我们用户为了实现 session window ，特意写了非常多的 UDF ，包括做增量处理，把 session 全部 build 起来，把 record 拿出来做处理等等。现在直接用 session window operator ，省了大量的开发消耗。同时 group 聚合等 window 操作也都是流批同时支持的。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;Session Window：&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;pre spellcheck=&quot;false&quot; cid=&quot;n132&quot; mdtype=&quot;fences&quot;&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;cs&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;// Session Event-time Window&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;.window(Session withGap 10.minutes on $&quot;rowtime&quot; as $&quot;w&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;// Session Processing-time Window (assuming a processing-time attribute &quot;proctime&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;.window(Session withGap 10.minutes on $&quot;proctime&quot; as $&quot;w&quot;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;Slide Window：&lt;/span&gt;&lt;/strong&gt;&lt;/section&gt;&lt;pre spellcheck=&quot;false&quot; cid=&quot;n134&quot; mdtype=&quot;fences&quot;&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;cs&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;// Sliding Event-time Window&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;.window(Slide over 10.minutes every 5.minutes on $&quot;rowtime&quot; as $&quot;w&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;// Sliding Processing-time Window (assuming a processing-time attribute &quot;proctime&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;.window(Slide over 10.minutes every 5.minutes on $&quot;proctime&quot; as $&quot;w&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;// Sliding Row-count Window (assuming a processing-time attribute &quot;proctime&quot;)&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;.window(Slide over 10.rows every 5.rows on $&quot;proctime&quot; as $&quot;w&quot;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/pre&gt;&lt;p cid=&quot;n137&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvqegicsFpNwRCdFcHuSicLoI9G4OEFiclGSRS2fTqBfPicp07xjuTahMy0w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;UDF 是在引擎框架之间迁移时最大的障碍。如果 UDF 是用 Hive 写的，那是方便迁移的，因为不管是 Flink 还是 Spark 对 Hive UDF 的支持都是很好的，但如果 UDF 是用 Flink 或者 Spark 写的，迁移到任何一个引擎框架，都会遇到非常大的问题，比如迁移到 Presto 做 OLAP 近实时查询。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;为了实现 UDF 的复用，我们 LinkedIn 在内部开发了一个 transport 项目，已经开源至 github [14] 上, 可以看下 LinkedIn 发表的博客：Transport: Towards Logical Independence Using Translatable Portable UDFs [15]。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;transport 给所有引擎框架提供一个面向用户的 User API ，提供通用的函数开发接口，底下自动生成基于不同引擎框架的 UDF ，比如 Presto、Hive、Spark、Flink 等。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;用一个共通的 UDF API 打通所有的引擎框架，能让用户复用自己的业务逻辑。用户可以很容易的上手使用，比如如下用户开发一个 MapFromTwoArraysFunction:&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section class=&quot;code-snippet__fix code-snippet__js&quot;&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;java&quot;&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;public class MapFromTwoArraysFunction extends StdUDF2&amp;lt;StdArray,StdArray,StdMap&amp;gt;{&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    private StdType _mapType;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    @Override&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    public List&amp;lt;String&amp;gt; getInputParameterSignatures(){&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        return ImmutableList.of(&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &quot;array[K]&quot;,&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;            &quot;array[V]&quot;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        );&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    }&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    @Override&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    public String getOutputParameterSignature(){&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        return &quot;map(K,V)&quot;;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    }&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;@Override&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;public void init(StdFactory stdFactory){&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    super.init(stdFactory);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;@Override&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;public StdMap eval(StdArray a1, StdArray a2){&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    if(a1.size() != a2.size()) {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        return null;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    }&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    StdMap map = getStdFactory().createMap(_mapType);&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    for(int i = 0; i &amp;lt; a1.size; i++) {&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;        map.put(a1.get(i), a2.get(i));&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    }&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    return map;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/section&gt;&lt;p cid=&quot;n147&quot; mdtype=&quot;paragraph&quot;&gt;&lt;/p&gt;&lt;p cid=&quot;n147&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJviaicsKfr43UaRicIib4Bw9WBjG5biaib6UgZGxhEAaHNdSARXnsf335Rdplw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;处理用户的 SQL 迁移问题 ，用户之前是用 Spark SQL 开发的作业，之后想使用流批一体，改成 Flink SQL 。目前的引擎框架还是比较多的，LinkedIn 开发出一个 coral 的解决方案，已在 github [16] 上开源，在 facebook 上也做了一些 talk ，包括和 transport UDF 一起给用户提供一个隔离层使用户可以更好的做到跨引擎的迁移，复用自己的业务逻辑。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;看下 coral 的执行流程，首先作业脚本中定义 熟悉的 ASCII SQL 和 table 的属性等，之后会生成一个 Coral IR 树状结构，最后翻译成各个引擎的 physical plan。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n152&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJv8ydJsvo1oaDFl1rnbzoviavku17MiaeomwCRibQyXzUsfJjCJHK7Kwgvg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;在案例 B 分析中，流批统一，在集群业务量特别大的情况下，用户对批处理的性能、稳定性、成功率等是非常重视的。其中 Shuffle Service ，对批处理性能影响比较大。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;四、Shuffle Service &lt;/strong&gt;&lt;/p&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;在 Spark 和 Flink 上的对比&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p cid=&quot;n157&quot; mdtype=&quot;paragraph&quot;&gt;&lt;/p&gt;&lt;p cid=&quot;n157&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJv3X0fILvqZFkbGd0yplmC72WZdn7xYFlwZqVBsUdOWWKkaPGZ4nPQIw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;In-memory Shuffle，Spark 和 Flink 都支持，比较快，但不支持可扩展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Hash-based Shuffle ，Spark 和 Flink 都支持 ， 相比 In-memory Shuffle ，容错性支持的更好一些，但同样不支持可扩展。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Sort-based Shuffle，对大的 Shuffle 支持可扩展，从磁盘读上来一点一点 Sort match 好再读回去，在 FLIP-148: Introduce Sort-Based Blocking Shuffle to Flink  [17] 中也已经支持。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;External Shuffle Service， 在集群非常繁忙，比如在做动态资源调度时，外挂服务就会非常重要，对 Shuffle 的性能和资源依赖有更好的隔离，隔离之后就可以更好的去调度资源。FLINK-11805 A Common External Shuffle Service Framework [18] 目前处于 reopen 状态。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Disaggregate Shuffle，大数据领域都倡导 Cloud Native 云原生，计算存储分离在 Shuffle Service 的设计上也是要考虑的。FLINK-10653 Introduce Pluggable Shuffle Service Architecture [19] 引入了可插拔的 Shuffle Service 架构。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n165&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvAcsbmPxpmOp5aYT1SkkVVhCRODMQbk15YB5FTlGQQkd6yfdLIwUPlw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;Spark 对 Shuffle Service 做了一个比较大的提升，这个工作也是由 LinkedIn 主导的 magnet 项目，形成了一篇名称为 introducing-magnet 的论文 (Magnet: A scalable and performant shuffle architecture for Apache Spark) [20]，收录到了 LinkedIn blog 2020 里。magnet 很明显的提升了磁盘读写的效率，从比较小的 random range ，到比较大的顺序读，也会做一些 merging ，而不是随意的随机读取 shuffle data ，避免 random IO 的一些问题。&lt;/span&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p cid=&quot;n169&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvQ5QUS1icJC10EtiaGOj10kEugqCNBtutCGvC4fZLIr9vntpT3iaYic9PXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;通过 Magent Shuffle Service 缓解了 Shuffle 稳定性和可扩展性方面的问题。在此之前，我们发现了很多 Shuffle 方面的问题，比如 Job failure 等等非常高。如果想用 Flink 做批处理，帮助到以前用 Spark 做批处理的用户，在 Shuffle 上确实要花更大功夫。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;在 Shuffle 可用性上，会采用 best-effort 方式去推 shuffle blocks，忽略一些大的 block ，保证最终的一致性和准确性；&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;为 shuffle 临时数据生成一个副本，确保准确性。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;如果 push 过程特别慢，会有提前终止技术。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;Magent Shuffle 相比 Vanilla Shuffle ，读取 Shuffle data 的等待时间缩较少了几乎 100%，task 执行时间缩短了几乎 50%，端到端的任务时长也缩短了几乎 30%。&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;p cid=&quot;n182&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.19018867924528302&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvEicsAjo64iaoFs8gyic7DJLTXC3xiaTsEGKibObEibmE8wolEZnibXsgAicRNQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2650&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section powered-by=&quot;xiumi.us&quot;&gt;&lt;section&gt;&lt;p cid=&quot;n68&quot; mdtype=&quot;heading&quot;&gt;&lt;strong&gt;五、总结&lt;/strong&gt;&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p cid=&quot;n185&quot; mdtype=&quot;paragraph&quot;&gt;&lt;img data-ratio=&quot;0.5625&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6HhibXew3LttuGqCuZicWtJvjeA4x2D07KdlTeiaJtjdwWpyibwdiaHFj2yp83Rxhk6wUTISicTBia6GzicA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1920&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;LinkedIn 非常认可和开心看到 Flink 在流处理和批处理上的明显优势，做的更加统一，也在持续优化中。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;Flink 批处理能力有待提升，如 history server，metrics，调试。用户在开发的时候，需要从用户社区看一些解决方案，整个生态要搭建起来，用户才能方便的用起来。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;Flink 需要对 shuffle service 和大集群离线工作流投入更多的精力，确保 workflow 的成功率，如果规模大起来之后，如何提供更好的用户支持和对集群进行健康监控等。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;ul class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;&lt;span&gt;随着越来越多的框架引擎出现，最好能给到用户一个更加统一的 interface，这一块的挑战是比较大的，包括开发和运维方面，根据 LinkedIn 的经验，还是看到了很多问题的，并不是通过一个单一的解决方案，就能囊括所有的用户使用场景，哪怕是一些 function 或者 expression，也很难完全覆盖到。像 coral、transport UDF。&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;blockquote class=&quot;js_blockquote_wrap&quot; data-type=&quot;2&quot; data-url=&quot;&quot; data-author-name=&quot;&quot; data-content-utf8-length=&quot;52&quot; data-source-title=&quot;&quot;&gt;&lt;section class=&quot;js_blockquote_digest&quot;&gt;&lt;section&gt;&lt;span&gt;原视频：&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;https://www.bilibili.com/video/BV13a4y1H7XY?p=12&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;section&gt;&lt;strong&gt;&lt;span&gt;参考链接&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[1] &lt;span&gt;https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158866741&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[2] &lt;span&gt;https://cwiki.apache.org/confluence/display/FLINK/FLIP-134%3A+Batch+execution+for+the+DataStream+API&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[3] &lt;span&gt;https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[4] &lt;span&gt;https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[5] &lt;span&gt;https://beam.apache.org/documentation/runners/spark/&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[6] &lt;span&gt;https://issues.apache.org/jira/browse/BEAM-8470&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[7] &lt;span&gt;https://www.youtube.com/watch?v=D47CSeGpBd0&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[8] &lt;span&gt;https://www.alibabacloud.com/blog/whats-all-involved-with-blink-merging-with-apache-flink_595401&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[9] &lt;span&gt;https://issues.apache.org/jira/browse/FLINK-11439&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[10] &lt;span&gt;https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[11] &lt;span&gt;https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[12] &lt;span&gt;https://docs.google.com/document/d/1tnG2DPHZYbsomvihIpXruUmQ12pHGK0QIvXS1FOTgRc/edit#heading=h.puuotbien1gf&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[13] &lt;span&gt;https://beam.apache.org/documentation/dsls/sql/overview/&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[14] &lt;span&gt;https://github.com/linkedin/transport&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[15] &lt;span&gt;https://engineering.linkedin.com/blog/2018/11/using-translatable-portable-UDFs&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[16] &lt;span&gt;https://github.com/linkedin/coral&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[17] &lt;span&gt;https://cwiki.apache.org/confluence/display/FLINK/FLIP-148%3A+Introduce+Sort-Based+Blocking+Shuffle+to+Flink&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[18] &lt;span&gt;https://issues.apache.org/jira/browse/FLINK-11805&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[19] &lt;span&gt;https://issues.apache.org/jira/browse/FLINK-10653&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;[20] &lt;span&gt;https://engineering.linkedin.com/blog/2020/introducing-magnet&lt;/span&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/section&gt;&lt;hr/&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;&lt;img data-ratio=&quot;1&quot; data-type=&quot;png&quot; data-w=&quot;20&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu7FIXNc036LLUkKfmG7dYG4GXTiaRe5yCHLwJoVfZgIHG9mZgoBY0jnJWLIDRLeiafE0fnMYFOX9x3g/640?wx_fmt=png&quot;/&gt;  Flink Forward Asia 2021&lt;strong&gt;  &lt;img data-ratio=&quot;1&quot; data-type=&quot;png&quot; data-w=&quot;20&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu7FIXNc036LLUkKfmG7dYG4GXTiaRe5yCHLwJoVfZgIHG9mZgoBY0jnJWLIDRLeiafE0fnMYFOX9x3g/640?wx_fmt=png&quot;/&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;strong&gt;报名现已开放&lt;/strong&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;&lt;span&gt;Flink Forward Asia 2021 重磅启动！FFA 2021 将于 12 月 4-5 日在北京·国家会议中心举办，预计将有 3000+ 开发者参与，探讨交流 Flink 最新动态。报名通道已开启，扫描下图二维码，或&lt;/span&gt;&lt;span&gt;点击文末「&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;阅读原文&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;」即可报名 FFA 2021～&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/8AsYBicEePu6ufbb8ec23OX3vzE6FpSYl22MoichPgpOcpgBNNb2ibLtr0mAYnCAaibx2CiclDADB4kIGQicCn1ibyjgA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;hr/&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;section&gt;&lt;span&gt;更多 Flink 相关技术问题，可扫码加入社区钉钉交流群～&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.2078189300411524&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/8AsYBicEePu6PUTQaA1BP3Fb8uViccQpspmTibIYEfM7Wv6VACia9CDQfcN8huMVCafZ5s36wThUmbYRTOzMu4hd8A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;972&quot;/&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt; &lt;img class=&quot;__bg_gif&quot; data-ratio=&quot;1&quot; data-type=&quot;gif&quot; data-w=&quot;400&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_gif/Z6bicxIx5naLWBBEcl44aIic1Mthe1nZiaramW5s4e8WwyCYYbTzu6uPBpgI6sxNXNymEnOYKpJpcrItUia7lS64mA/640?wx_fmt=gif&quot;/&gt;  &lt;/span&gt;&lt;span&gt;戳我，报名 FFA 2021 大会！&lt;/span&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>77e9b00581c50ac3d68debd181ac65d6</guid>
<title>Java GC 入门</title>
<link>https://toutiao.io/k/oyw72wo</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;container post-content&quot;&gt;&lt;p&gt;最近学了一些 Java GC 的知识，按自己的理解整理了一些 GC 算法遇到的问题和解决的思路。&lt;/p&gt;
&lt;p&gt;免责声明：以下所有内容都是个人理解，可能与事实不符。&lt;/p&gt;
&lt;h2 id=&quot;标记：三色算法&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#标记：三色算法&quot;/&gt;标记：三色算法&lt;/h2&gt;
&lt;p&gt;垃圾回收，需要先找出什么是垃圾，之后才能谈回收问题，一些方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有垃圾：认为所有对象都是存活的&lt;/li&gt;
&lt;li&gt;Reference Counting（引用计数）：每个对象/资源设置一个引用计数，有新的引用则计数加一，引用释放后计数减一，所有引用都释放后则认为该对象是垃圾&lt;/li&gt;
&lt;li&gt;Tracing（跟踪）：遍历对象的引用关系，过程中不可达的对象即为垃圾&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;引用计数在 C++ 和 Rust 之类的语言中比较常用，Java 中用的是 Tracing 的方式，遍历对象间的引用。那么从哪开始遍历呢？这些遍历的起始点称为 GC Root，在 Java 中有&lt;a href=&quot;https://help.eclipse.org/latest/index.jsp?topic=%2Forg.eclipse.mat.ui.help%2Fconcepts%2Fgcroots.html&amp;amp;cp=37_2_3&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这么一些&lt;/a&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程运行栈上的所有引用，例如方法的参数，创建的临时变量等等&lt;/li&gt;
&lt;li&gt;系统加载的一些的类，比如说 &lt;code&gt;java.util.*&lt;/code&gt; 里的类&lt;/li&gt;
&lt;li&gt;JNI handles&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的 GC Roots 没列全，非专业做 GC 的话其实也没必要掌握。关键需要了解 GC Root 代表的就是我们“确定”还在用的引用，比如方法里创建了一个 &lt;code&gt;HashMap&lt;/code&gt;，方法还返回前都“确定”还会用到，就认为是 Root（这里说得不准确，可能 new 出的对象就没人用，但从算法角度还是认为它是 Root）。&lt;/p&gt;
&lt;p&gt;有了 GC Root，要如何扫描呢？Java 里用的是三色算法。三色算法是一个“逻辑算法”，本质上就是是树/森林的遍历，但为了方便描述和讨论，把遍历过程中的节点细化成三个状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Black: 对象可达，且对象的所有引用都已经扫描了（“扫描”在可以理解成遍历过了或加入了待遍历的队列）&lt;/li&gt;
&lt;li&gt;Gray: 对象可达，但对象的引用还没有扫描过（因此 Gray 对象可理解成在搜索队列里的元素）&lt;/li&gt;
&lt;li&gt;White: 不可达对象或还没有扫描过的对象&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每次迭代都会将 Grey 引用的 White 对象标成 Grey，并将 Grey 对象标记成 Black，直到没有 Grey 对象为止。标记之后一个对象最终只会是 Black 或者 White，其中所有可达的对象最终都会是 Black，如&lt;a href=&quot;https://en.wikipedia.org/wiki/Tracing_garbage_collection#Tri-color_marking&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下例&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;Tri-color-example-20211006095339-gfbvko5.svg&quot; alt=&quot;Tri-color-example.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这里并没的说明 Grey 对象的遍历顺序，所以实际上实现成宽搜或深搜都是可以的。&lt;/p&gt;
&lt;h2 id=&quot;回收：sweep-vs-compact-vs-copy&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#回收：sweep-vs-compact-vs-copy&quot;/&gt;回收：Sweep vs Compact vs Copy&lt;/h2&gt;
&lt;p&gt;上节说考虑的是“什么是垃圾”的问题，标识出了垃圾对象，下一步是如何“回收”。通常有 Sweep/Compact/Copy 三种处理方式，直观上理解是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;sweep-compact-copy-20211006101722-qjw5r3s.svg&quot; alt=&quot;sweep-compact-copy.svg&quot;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sweep 指的是把垃圾清除了，但它不会移动活动对象，不过久了以后内存容易碎片化&lt;/li&gt;
&lt;li&gt;Compact 除了丢弃垃圾对象外，还会移动活动对象，紧凑地放到一个新的地方，能解决碎片化问题，但可能需要先计算目标地址，修正指针再移动对象，速度较慢。&lt;/li&gt;
&lt;li&gt;Copy 本质上和 Compact 是一样的，不过它的一些计算最会更少。但通常需要保留了一半的内存，移动时直接移动到另一半，空间开销会更大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;三种方法有各自的优势，需要使用方自己做权衡。这里引用 &lt;a href=&quot;https://hllvm-group.iteye.com/group/topic/38223#post-248757&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R 大的帖子&lt;/a&gt; 总结如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;br/&gt;&lt;/th&gt;
&lt;th&gt;Mark-Sweep&lt;/th&gt;
&lt;th&gt;Mark-Compact&lt;/th&gt;
&lt;th&gt;Mark-Copy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;速度&lt;/td&gt;
&lt;td&gt;中等&lt;/td&gt;
&lt;td&gt;最慢&lt;/td&gt;
&lt;td&gt;最快&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;空间开销&lt;/td&gt;
&lt;td&gt;少（但有碎片）&lt;/td&gt;
&lt;td&gt;少（无碎片）&lt;/td&gt;
&lt;td&gt;通常需要活动对象的 2 倍（无碎片）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;移动对象？&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这几种方法都有使用。如 CMS 最后的 S 代表的就是 Sweep；传统的 Serial GC 和 Parallel GC，包括新的 G1、Shenandoah、ZGC 都可以理解成是 Compact；而 Serial, Parallel, CMS 的 Young GC 都用的是 Copy。&lt;/p&gt;
&lt;h2 id=&quot;分代假设&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#分代假设&quot;/&gt;分代假设&lt;/h2&gt;
&lt;p&gt;如果接触过 GC，会知道 GC 最让人头疼的是 Stop-the-World 停顿，GC 算法的一些阶段会把用户线程的执行完全暂定，造成不可预期的停顿。我们希望这个时间尽可能短甚至完全去除。GC 的“效率”跟多方面因素有关，比如活动对象（active object）越多，Marking 需要遍历的节点越多，越耗时；比如内存越大，Sweep 清理垃圾时需要遍历的区域越大，耗时越长；等等。于是人们在想怎么“偷懒”来提升效率。&lt;/p&gt;
&lt;p&gt;分代假设就是这样一个&lt;a href=&quot;https://plumbr.io/handbook/garbage-collection-in-java#generational-hypothesis&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;发现/假设&lt;/a&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多数对象一般创建不久后就被废弃了/死了&lt;/li&gt;
&lt;li&gt;一段时间后还在使用/活着的对象，通常还会继续存在/活（非常）长的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从对象存活时间和对象数量的视角来看，分代假设就是这样的（&lt;a href=&quot;https://plumbr.io/app/uploads/2015/05/object-age-based-on-GC-generation-generational-hypothesis.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原图&lt;/a&gt;）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;object-age-based-on-GC-generation-generational-hypothesis-20211006105013-yrmxkz4.png&quot; alt=&quot;object-age-based-on-GC-generation-generational-hypothesis.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;当然这个假设不一定符合实际，比如 LRU 缓存，越老的对象越可能被淘汰。不过多数应用还是符合这个假设的。于是如果将对象按时间分成年轻代和老年代，我们就可以偷懒了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;年轻代的对象死得快，因此通常回收年轻代收益更高，于是可以更频繁回收年轻代，少最回收老年代&lt;/li&gt;
&lt;li&gt;回收年轻代时的标记阶段可以简化。例如存在 Old -&amp;gt; Young 的引用，正确的做法是用三色算法判断 Old 里的对象是死是活，再来判断 Young 对象的死活，但在分代假设下，可以偷懒地认为 Old 的对象就是活的，这样可以减少 Mark 的时间且不太影响回收的效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是在分代假设下，传统的 GC 流程变成了这样（&lt;a href=&quot;https://plumbr.io/app/uploads/2015/05/how-java-garbage-collection-works.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原图&lt;/a&gt;）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;how-java-garbage-collection-works-20211006105843-u5qw3te.png&quot; alt=&quot;how-java-garbage-collection-works.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;新对象从 Eden 区分配，Young GC 时存活的进 Survivor 区，Survivor 区有两个，相互做 Copy 操作。在 Survivor 区存活了 15 次 GC 的，就移动到 Old/Tenured 区。Young GC 时会忽略 Tenured 区。&lt;/p&gt;
&lt;h2 id=&quot;并发提速&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#并发提速&quot;/&gt;并发提速&lt;/h2&gt;
&lt;p&gt;前面提到了 GC 最让人头疼的是 STW 停顿，分代策略让我们频繁做 Young GC，少量做 Full GC，但真的做 Full GC 时停顿时间还是非常大，于是人们想到了并发。CMS 中的 CM 指的是 Concurrent Mark 即是“并发标记”。而 Shenandoah GC 和 ZGC 又实现了“回收”的并发。&lt;/p&gt;
&lt;p&gt;开始前要注意的是“并发”和“并行”在 GC 里的概念是不一样的，可以这么去区分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并行：起多个线程一起处理，但对应用线程依旧是 STW 的&lt;/li&gt;
&lt;li&gt;并发：GC 线程处理 GC 任务的同时，应用线程依旧可以运行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如早期的 Parallel GC 本质上就是“并行”而不是“并发”，GC 过程还是 STW 的。虽然仅一字之差，“并发”会带来非常多的问题，新的 GC 算法也用了许多解决方案，但这些方案都是有代价的。&lt;/p&gt;
&lt;h3 id=&quot;并发标记&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#并发标记&quot;/&gt;并发标记&lt;/h3&gt;
&lt;p&gt;前面提到 Java 里会用三色算法来遍历堆中的引用关系，算法假设引用关系在遍历期间不变，如果变化了会怎么样呢？主要有两个场景：新增对象和引用修改。&lt;/p&gt;
&lt;p&gt;第一个问题是新增对象：在标记期间新增的对象通过旧的 GC Roots 可能不可达，标记结束后可能还是 White，会被认为是垃圾而被错误释放。&lt;/p&gt;
&lt;p&gt;第二个问题是：标记期间应用线程修改引用会影响正确性。&lt;/p&gt;
&lt;p&gt;其中一些修改不会造成错误，只是会影响回收效率。如断开 Black1 -&amp;gt; Black2 引用，
Black2 最终应该被释放，但不释放 Black2 不会造成程序错误。但如果修改同时满足下面两个条件则会影响正确性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;应用线程增加 Black -&amp;gt; White 的引用，这意味着这个 White 对象标记结束后是被引用的，预期是 Black&lt;/li&gt;
&lt;li&gt;应用线程断开了 Grey -&amp;gt; White 的（直接或间接）引用，这意味着原本 White 对象能通过该 Grey 对象被遍历到，但现在却遍历不到了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;条件一和条件二的共同结果是，标记过程会遗漏这个 White 对象，因为通过 Grey 对象不可达，且 Black 对象不会被二次扫描。于是 GC 结束后它会被释放，但它同时还被 Black 对象引用着，程序会出错。&lt;/p&gt;
&lt;p&gt;并发标记算法如何解决这两个问题？&lt;/p&gt;
&lt;h4 id=&quot;incremental-update&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#incremental-update&quot;/&gt;Incremental Update&lt;/h4&gt;
&lt;p&gt;Incremental update 的想法是破坏条件一。标记期间记录增加的每个 Black -&amp;gt; White 引用中的 White 对象，把它标记为 Grey。对于标记期间新增的对象，则需要在标记结束前重新扫描一次 GC Roots 做 Marking。&lt;/p&gt;
&lt;p&gt;在实现上，就需要去“监听” Black -&amp;gt; White 引用的创建。以 CMS 为例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CMS 会在程序的引用赋值语句（如 &lt;code&gt;obj.foo = bar&lt;/code&gt;）后，插入一段代码（称为 barrier，因为是在赋值结束后的 barrier，所以称为 post write barrier），这段代码会记录 foo -&amp;gt; bar 的引用。&lt;/li&gt;
&lt;li&gt;CMS 会在内存中开辟一块区域，称为 Card Table，用来记录 foo -&amp;gt; bar 的引用&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在标记过程中新增的 Black -&amp;gt; White 的引用，都可以在 Card Table 中找到。于是要保证标记的正确性，只需要在标记结束前从 Card Table 中找到 foo -&amp;gt; bar 的引用，再用三色算法遍历一下 bar 及其引用即可。当然还需要再重新扫描 GC Roots 处理新增的对象。&lt;/p&gt;
&lt;p&gt;实现细节上，Card Table 里并不会像 HashMap 一样记录一个 A -&amp;gt; B 的映射，这样存储访问的效率都很低。Card Table 是一个 bitmap，先将内存按 512B 分成一个个区域，称为 Card，每个 Card 对应 bitmap 里的一位。bitmap 置 1 代表对应 Card 中包含需要重新扫描的对象。在标记结束前找到为 dirty 的 Card，重新扫描其中的（所有）对象及其引用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;card-table-20211006153844-b500f13.svg&quot; alt=&quot;card-table.svg&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;snapshot-at-the-beginning&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#snapshot-at-the-beginning&quot;/&gt;Snapshot At The Beginning&lt;/h4&gt;
&lt;p&gt;Snapshot At The Beginning(SATB) 的想法则是破坏条件二，在标记开始之前做快照，快照之后新增的对象都不处理，认为是 Black；当要删除旧的引用（换句话说，在新的赋值 &lt;code&gt;obj.foo = bar&lt;/code&gt; 生效之前），记录旧的引用，这样在标记结束前再扫描这些旧的引用即可，这样原先的 Grey -&amp;gt; White 的引用虽然断开了，但 White 对象依旧可以扫到。以 G1 为例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在每个 Region 会有 TAMS(top at marking start) 指针，标记开始时设置为 Top 的值，区域内新增对象后 Top 指针增长，可以认为 [TAMS, Top] 之间的对象都是新对象，都置为 Black 即可&lt;/li&gt;
&lt;li&gt;在赋值语句之前加入 barrier，例如 &lt;code&gt;obj.foo = bar&lt;/code&gt; 可以拆成 &lt;code&gt;barrier(obj.foo); obj.foo = bar&lt;/code&gt;，barrier 会对赋值前的指针做记录。因为是在写指针之前做的操作，因此也叫 pre write barrier&lt;/li&gt;
&lt;li&gt;G1 使用了和 Card Table 类似的结构叫 Remember Set(RSet)，用来记录 pre write barrier 传递的指针。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最终的操作与 Incremental Update 类似，在标记结束前，重新扫描 RSet 里记录的指针，也会有额外的操作把 [TAMS, Top] 之间的对象标记成 Black。&lt;/p&gt;
&lt;p&gt;实现细节上，G1 将内存分成了多个 Region。每个 Region 有自己的一个 RSet，这点与 Card Table 不同，它是全局的。RSet 的结构如下：
&lt;img src=&quot;RSet-20211006163543-vi8geu1.svg&quot; alt=&quot;RSet.svg&quot;/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个 Region 有自己的 RSet&lt;/li&gt;
&lt;li&gt;RSet 里记录的的：指向当前 Region 的有 Region xx Card yy, …&lt;/li&gt;
&lt;li&gt;如果要回收 Region3，只需要扫描 Region3 对应的 Reset 里的指针（即 R1C4 和 R2C2）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，RSet 的&lt;a href=&quot;http://09itblog.site/?p=1093&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;具体实现&lt;/a&gt;和上图不太一样，如一般用 HashMap 来存储；但如果 region 里的 card 数过多就会退化成 bitmap；引用的 region 过多，则 region 也会用 bitmap 来存储。细节上也有很多优化，比如 barrier 的更新是先记录到一个 Thread Local 的队列上，异步更新到 RSet 中的。&lt;/p&gt;
&lt;h3 id=&quot;concurrent-copy-compact&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#concurrent-copy-compact&quot;/&gt;Concurrent Copy/Compact&lt;/h3&gt;
&lt;p&gt;不管是在 CMS 和 G1 里，并发的内容主要还是以 Marking 为主，Copy/Compact 还是 STW 的。如 CMS 的 Young GC Copy，G1 的 Evacuation Compact，都是 STW 的。为了追求接近硬实时的效果，Shenandoah GC 和 ZGC 都尝试将“回收”阶段并发化，减少 Copy/Compact 的 STW 停顿时间。而正如并发标记里会需要处理新对象和并发修改的问题，并发 Copy/Compact 也会遇到不少问题。&lt;/p&gt;
&lt;h3 id=&quot;并发修改问题&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#并发修改问题&quot;/&gt;并发修改问题&lt;/h3&gt;
&lt;p&gt;Copy/Compact 的过程，需要先将对象复制到新的位置，再修改所有该对象的引用，指向新的地址。在 STW 的方案下，过程如下（摘自 &lt;a href=&quot;https://shipilev.net/talks/javazone-Sep2018-shenandoah.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://shipilev.net/talks/javazone-Sep2018-shenandoah.pdf&lt;/a&gt; ）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;Copy-STW-20211006170030-k2d6hvm.svg&quot; alt=&quot;Copy-STW.svg&quot;/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先复制对象&lt;/li&gt;
&lt;li&gt;将复制后对象的指针存放在原对象的 Header 中（复用空间）&lt;/li&gt;
&lt;li&gt;遍历堆上的指针，将指针的值置为对象的 Header 中存储的指针（&lt;code&gt;*ref = **ref&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;所有指针更新完毕，释放旧的对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;但允许并发时，会出现不同线程对不同副本做读写的问题，此时应该保留哪个副本？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;Copy-Concurrent-Problem-20211006171017-agr14hx.svg&quot; alt=&quot;Copy-Concurrent-Problem.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;并发回收算法的核心也就在于怎么解决 Copy 期间多线程对两个副本的同步。下面会介绍 Shenadoah GC 和 ZGC 的做法，它们都会用到 load barrier 来修正并发情况下应用线程的读操作。&lt;/p&gt;
&lt;h3 id=&quot;brooks-pointer&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#brooks-pointer&quot;/&gt;Brooks Pointer&lt;/h3&gt;
&lt;p&gt;Shenandoah GC 对这个问题的解法是：为每个对象都增加一个 Forwarding 指针，在 Copy/Compact 过程中，通过 CAS 来更新这个指针指向新的副本，期间指向该对象的指针的读写，都要经过 Forwarding 找到正确的对象，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;Copy-Brooks-Pointer-20211006210633-1tpjsh4.svg&quot; alt=&quot;Copy-Brooks-Pointer.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这个方案的有效性本身并不难理解。技术上，这个方案需要拦截所有的对读写操作，让它通过 &lt;code&gt;FwdPtr&lt;/code&gt; 完成。Shenandoah GC 通过 Write Barrier + Load Barrier 来完成。&lt;/p&gt;
&lt;p&gt;一个小细节：在执行 Write 操作时，Write Barrier 如果发现当前处于并发 Copy 阶段，但对象还没有被 Copy，则 Write Barrier 会执行 Copy 操作，否则写到旧的副本里也没有意义。但读操作时并不会主动做 Copy 的动作。&lt;/p&gt;
&lt;p&gt;这个算法的难点在于实现和优化。Shenandoah 中做了许多额外的处理：例如在更多地方增加 barrier，比如 &lt;code&gt;==&lt;/code&gt; 、&lt;code&gt;compareAndSwap&lt;/code&gt;等操作；例如去除对 NULL 检查的 Barrier，把 barrier 放在循环外来提高性能。&lt;/p&gt;
&lt;p&gt;另外 Brooks Pointer 中的 Brooks 是人名，Rodney A. Brooks 在 1984 年为 Lisp 发明的。&lt;/p&gt;
&lt;h3 id=&quot;zgc-relocation&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#zgc-relocation&quot;/&gt;ZGC Relocation&lt;/h3&gt;
&lt;p&gt;一个 A-&amp;gt;B 的引用有两个参与者，引用方 A 和被引用方 B。Shenandoah 是在被引用方 B 中增加 Forwarding Pointer 来屏蔽底层的 Copy 的动作。而 ZGC 则是在引用方 A 处动手，具体有这么几个机制：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在指针中挑几个 bit 来做标记，其中 &lt;code&gt;remapped&lt;/code&gt; 位代表的当前指针是否指向 Copy 后的地址&lt;/li&gt;
&lt;li&gt;Copy/Compact 的过程中，ZGC 会为 Region 创建 forwarding table，用于保存新旧对象地址的映射&lt;/li&gt;
&lt;li&gt;ZGC （只）会用 Read Barrier，在访问指针时，如果当前指针的 &lt;code&gt;remapped&lt;/code&gt; 位为 &lt;code&gt;0&lt;/code&gt;，代表指针未更新，会查找 forwarding table 的值来更新当前指针，之后再进行访问&lt;/li&gt;
&lt;li&gt;如果在 Copy/Compact 的过程中指针并没有被访问，则在下次 marking 时会由 GC Thread 来更新指针。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果画成图，大概是这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;ZGC-Copy-20211006214331-yi4a2ex.svg&quot; alt=&quot;ZGC-Copy.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;相比于 Brooks Pointer，这个算法会更受限，比如无法支持 32 位的机器，不能开启指针压缩等等。&lt;/p&gt;
&lt;h2 id=&quot;那么代价呢？&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#那么代价呢？&quot;/&gt;那么代价呢？&lt;/h2&gt;
&lt;p&gt;先假设这样一个情形，如果我们看 GC 的日志，记录 GC 开始结束，（虚构）画出下面这张图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;Fake-Throughput-Diagram-20211006215644-jkdhlyg.svg&quot; alt=&quot;Fake-Throughput-Diagram.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;图中 2 的位置，我们发现应用程序的 TPS 和响应时间都变差了，但看了下 GC 的日志发现每次 GC 的停顿时间都很短，可能会觉得 GC 没有问题。但如果仔细观察，会发现 GC 变得频繁了，而 GC 是消耗 CPU 时间的，更频繁的 GC 意味着应用线程能用的时间也更少了，因此会造成 TPS 和响应时间变差的情况。&lt;/p&gt;
&lt;p&gt;除了 GC 带来的停顿之外，要意识到 GC 是有代价的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GC 的一些内部结构需要占用额外的内存，如 Card Table, RSet, Forwarding Pointer, Forwarding Table, etc.&lt;/li&gt;
&lt;li&gt;Shenadoah, ZGC 这种重度 barrier 使用者，不发生 GC 时也会有额外的 CPU 占用（比如 Shenandoah 大概 20%，ZGC 大概 15%，视具体程序有变化），这也是低延时 GC 的额外代价&lt;/li&gt;
&lt;li&gt;另外在真正执行 GC 时，GC 线程也会占用 CPU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般 GC 算法保证的停顿的时间越短，则消耗的 CPU 越大，换言之吞吐越小。没有通用的最优的 GC 算法，根据应用程序的不同和愿意付出的代价来选择 GC 算法吧。&lt;/p&gt;
&lt;h2 id=&quot;小结&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#小结&quot;/&gt;小结&lt;/h2&gt;
&lt;p&gt;文章中粗浅地讨论了 Java GC 算法中的几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标记用的三色算法，它是树遍历的一个抽象描述，有助于理解和讨论&lt;/li&gt;
&lt;li&gt;回收用的 Sweep, Compact, Copy 三种策略和各自的优缺点&lt;/li&gt;
&lt;li&gt;分代假设：越年轻的对象越可能死亡，越老的对象越可能活得久。GC 算法可以通过分代来提高性能&lt;/li&gt;
&lt;li&gt;为了减少停顿时间，GC 算法引入了并发标记和并发回收，而它们本身又引入了新的问题
&lt;ul&gt;
&lt;li&gt;并发标记的问题介绍了 Incremental Update 和 Snapshot at The Beginning，分别打破引发问题的两个必要条件的一个&lt;/li&gt;
&lt;li&gt;并发回收问题介绍了 Shenandoah GC 使用的 Brooks Pointer 和 ZGC 使用的策略。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;最后简单讨论了 GC 算法对应用程序本身的影响。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;参考&quot;&gt;&lt;a class=&quot;header-anchor&quot; href=&quot;#参考&quot;/&gt;参考&lt;/h2&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>7c2633d12ad8359cc52691b1b484bb62</guid>
<title>实践篇：Redis 客户端缓存在 Spring Boot 应用的探究</title>
<link>https://toutiao.io/k/0qkonm0</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section&gt;&lt;p&gt;本文探究Redis最新特性--客户端缓存在SpringBoot上的应用实战。&lt;/p&gt;&lt;h2&gt;&lt;span&gt;Redis Tracking&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;Redis客户端缓存机制基于Redis Tracking机制实现的。我们先了解一下Redis Tracking机制。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;为什么需要Redis Tracking&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;Redis由于速度快、性能高，常常作为MySQL等传统数据库的缓存数据库。但由于Redis是远程服务，查询Redis需要通过网络请求，在高并发查询情景中难免造成性能损耗。所以，高并发应用通常引入本地缓存，在查询Redis前先检查本地缓存是否存在数据。&lt;br/&gt;假如使用MySQL存储数据，那么数据查询流程下图所示。&lt;/p&gt;&lt;figure&gt;&lt;img data-ratio=&quot;1.8895705521472392&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/Of81vjDNtAy64LpuFbywiaWMWU8oLbic70lI9weNeA2PSuToG5DG8MXUic2KWXtblhZcaRiaqchgyXhEDHzib9HjIFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;163&quot; title=&quot;&quot;/&gt;&lt;/figure&gt;&lt;p&gt;引入多端缓存后，修改数据时，各数据缓存端如何保证数据一致是一个难题。通常的做法是修改MySQL数据，并删除Redis缓存、本地缓存。当用户发现缓存不存在时，会重新查询MySQL数据，并设置Redis缓存、本地缓存。&lt;br/&gt;在分布式系统中，某个节点修改数据后不仅要删除当前节点的本地缓存，还需要发送请求给集群中的其他节点，要求它们删除该数据的本地缓存，如下图所示。如果分布式系统中节点很多，那么该操作会造成不少性能损耗。&lt;/p&gt;&lt;figure&gt;&lt;img data-ratio=&quot;1.044776119402985&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/Of81vjDNtAy64LpuFbywiaWMWU8oLbic70XYgZwaUQAa1RRibh4yrEL43eWGvLzibXTbX1FSdFR2xq6ERcuBurCicBA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;268&quot; title=&quot;&quot;/&gt;&lt;/figure&gt;&lt;p&gt;为此，Redis 6提供了Redis Tracking机制，对该缓存方案进行了优化。开启Redis Tracking后，Redis服务器会记录客户端查询的所有键，并在这些键发生变更后，发送失效消息通知客户端这些键已变更，这时客户端需要将这些键的本地缓存删除。基于Redis Tracking机制，某个节点修改数据后，不需要再在集群广播“删除本地缓存”的请求，从而降低了系统复杂度，并提高了性能。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;Redis Tracking的应用&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;下表展示了Redis Tracking的基本使用&lt;/p&gt;&lt;figure&gt;&lt;img data-ratio=&quot;0.5523138832997988&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/Of81vjDNtAy64LpuFbywiaWMWU8oLbic70l2SABhbfreHLgHHFFwcRasEHiaRTybC3fngHsxibCyvNwZGJJqIgLkvw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;994&quot; title=&quot;picture 1&quot;/&gt;&lt;span/&gt;&lt;/figure&gt;&lt;br/&gt;（1）为了支持Redis服务器推送消息，Redis在RESP2协议上进行了扩展，实现了RESP3协议。HELLO 3命令表示客户端与Redis服务器之间使用RESP3协议通信。&lt;br/&gt;注意：Redis 6.0提供了Redis Tracking机制，但该版本的redis-cli并不支持RESP3协议，所以这里需要使用Redis 6.2版本的redis-cli进行演示。&lt;br/&gt;（2）CLIENT TRACKING on命令的作用是开启Redis Tracking机制，此后Redis服务器会记录客户端查询的键，并在这些键变更后推送失效消息通知客户端。失效消息以invalidate开头，后面是失效键数组。&lt;br/&gt;上表中的客户端 client1 查询了键 score 后，客户端 client2 修改了该键，这时 Redis 服务器会马上推送失效消息给客户端 client1，但 redis-cli 不会直接展示它收到的推送消息，而是在下一个请求返回后再展示该消息，所以 client1 重新发送了一个 PING请求。&lt;p&gt;&lt;/p&gt;&lt;p&gt;上面使用的非广播模式，另外，Redis Tracking还支持广播模式。在广播模式下，当变更的键以客户端关注的前缀开头时，Redis服务器会给所有关注了该前缀的客户端发送失效消息，不管客户端之前是否查询过这些键。&lt;br/&gt;下表展示了如何使用Redis Tracking的广播模式。&lt;/p&gt;&lt;figure&gt;&lt;img data-ratio=&quot;0.2777212614445575&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_png/Of81vjDNtAy64LpuFbywiaWMWU8oLbic70Rjcc9hRmlfo85kDGzYCXfD9RTY2LjJDf9HNFFPZem9icqEvJPnRicaqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;983&quot; title=&quot;picture 2&quot;/&gt;&lt;/figure&gt;&lt;br/&gt;说明一下CLIENT TRACKING命令中的两个参数：&lt;br/&gt;BCAST参数：启用广播模式。&lt;br/&gt;PREFIX参数：声明客户端关注的前缀，即客户端只关注cache开头的键。&lt;/section&gt;&lt;section&gt;&lt;span&gt;可以看到，使用广播模式后，即使client1没有查询键“cached:1”，该键变更时client1仍然会收到失效消息。&lt;/span&gt;&lt;p&gt;强调一下非广播模式与广播模式的区别：&lt;br/&gt;非广播模式：Redis服务器记录客户查询过的键，当这些键发生变化时，Redis发送失效消息给客户端。&lt;br/&gt;广播模式：Redis服务器不记录客户查询过的键，当变更的键以客户端关注的前缀开头时，Redis就会发送失效消息给客户端。&lt;/p&gt;&lt;p&gt;关于Redis Tracking的更多内容，我已经在新书《Redis核心原理与实践》中详细分析，这里不再赘述。&lt;/p&gt;&lt;h2&gt;&lt;span&gt;Redis客户端缓存&lt;/span&gt;&lt;/h2&gt;&lt;p&gt;既然Redis提供了Tracking机制，那么客户端就可以基于该机制实现客户端缓存了。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;Lettuce实现&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;Lettuce（6.1.5版本）已经支持Redis客户端缓存（单机模式下），使用CacheFrontend类可以实现客户端缓存。&lt;/p&gt;&lt;pre&gt;&lt;code&gt;public &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; main(&lt;span&gt;String&lt;/span&gt;[] args) throws InterruptedException {&lt;br/&gt;    &lt;span&gt;// [1]&lt;/span&gt;&lt;br/&gt;    RedisURI redisUri = RedisURI.builder()&lt;br/&gt;            .withHost(&lt;span&gt;&quot;127.0.0.1&quot;&lt;/span&gt;)&lt;br/&gt;            .withPort(&lt;span&gt;6379&lt;/span&gt;)&lt;br/&gt;            .build();&lt;br/&gt;    RedisClient redisClient = RedisClient.create(redisUri);&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;// [2]&lt;/span&gt;&lt;br/&gt;    StatefulRedisConnection&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt; connect = redisClient.connect();&lt;br/&gt;    &lt;span&gt;Map&lt;/span&gt;&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt; clientCache = &lt;span&gt;new&lt;/span&gt; ConcurrentHashMap&amp;lt;&amp;gt;();&lt;br/&gt;    CacheFrontend&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt; frontend = ClientSideCaching.enable(CacheAccessor.forMap(clientCache), connect,&lt;br/&gt;            TrackingArgs.Builder.enabled());&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;// [3]&lt;/span&gt;&lt;br/&gt;    &lt;span&gt;while&lt;/span&gt; (&lt;span&gt;true&lt;/span&gt;) {&lt;br/&gt;        &lt;span&gt;String&lt;/span&gt; cachedValue = frontend.&lt;span&gt;get&lt;/span&gt;(&lt;span&gt;&quot;k1&quot;&lt;/span&gt;);&lt;br/&gt;        System.out.println(&lt;span&gt;&quot;k1 ---&amp;gt; &quot;&lt;/span&gt; + cachedValue);&lt;br/&gt;        Thread.sleep(&lt;span&gt;3000&lt;/span&gt;);&lt;br/&gt;    }&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;ol class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;构建RedisClient。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;构建CacheFrontend。&lt;br/&gt;ClientSideCaching.enable开启客户端缓存，即发送“CLIENT TRACKING”命令给Redis服务器，要求Redis开启Tracking机制。&lt;br/&gt;最后一个参数指定了Redis Tracking的模式，这里用的是最简单的非广播模式。&lt;br/&gt;这里可以看到，通过Map保存客户端缓存的内容。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;重复查询同一个值，查看缓存是否生效。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们可以通过Redis的Monitor命令监控Redis服务收到的命令，使用该命令就可以看到，开启客户端缓存后，Lettuce不会重复查询同一个键。&lt;br/&gt;而且我们修改这个键后，Lettuce会重新查询这个键的最新值。&lt;/p&gt;&lt;p&gt;通过Redis的Client List命令可以查看连接的信息&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&amp;gt; CLIENT LIST&lt;br/&gt;id=4 addr=192.168.56.1:50402 fd=7 name= age=23 idle=22 flags=t ...&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;flags=t&lt;/code&gt;代表这个连接启动了Tracking机制。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;SpringBoot应用&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;那么如何在SpringBoot上使用呢？请看下面的例子&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;span&gt;@Bean&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;public&lt;/span&gt; CacheFrontend&amp;lt;String, String&amp;gt; &lt;span&gt;redisCacheFrontend&lt;/span&gt;&lt;span&gt;(RedisConnectionFactory redisConnectionFactory)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    StatefulRedisConnection connect = getRedisConnect(redisConnectionFactory);&lt;br/&gt;    &lt;span&gt;if&lt;/span&gt; (connect == &lt;span&gt;null&lt;/span&gt;) {&lt;br/&gt;        &lt;span&gt;return&lt;/span&gt; &lt;span&gt;null&lt;/span&gt;;&lt;br/&gt;    }&lt;br/&gt;&lt;br/&gt;    CacheFrontend&amp;lt;String, String&amp;gt; frontend = ClientSideCaching.enable(&lt;br/&gt;            CacheAccessor.forMap(&lt;span&gt;new&lt;/span&gt; ConcurrentHashMap&amp;lt;&amp;gt;()),&lt;br/&gt;            connect,&lt;br/&gt;            TrackingArgs.Builder.enabled());&lt;br/&gt;&lt;br/&gt;    &lt;span&gt;return&lt;/span&gt; frontend;&lt;br/&gt;}&lt;br/&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;private&lt;/span&gt; StatefulRedisConnection &lt;span&gt;getRedisConnect&lt;/span&gt;&lt;span&gt;(RedisConnectionFactory redisConnectionFactory)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;    &lt;span&gt;if&lt;/span&gt;(redisConnectionFactory &lt;span&gt;instanceof&lt;/span&gt; LettuceConnectionFactory) {&lt;br/&gt;        AbstractRedisClient absClient = ((LettuceConnectionFactory) redisConnectionFactory).getNativeClient();&lt;br/&gt;        &lt;span&gt;if&lt;/span&gt; (absClient &lt;span&gt;instanceof&lt;/span&gt; RedisClient) {&lt;br/&gt;            &lt;span&gt;return&lt;/span&gt; ((RedisClient) absClient).connect();&lt;br/&gt;        }&lt;br/&gt;    }&lt;br/&gt;    &lt;span&gt;return&lt;/span&gt; &lt;span&gt;null&lt;/span&gt;;&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其实也简单，通过RedisConnectionFactory获取一个StatefulRedisConnection连接，就可以创建CacheFrontend了。&lt;br/&gt;这里RedisClient#connect方法会创建一个新的连接，这样可以将使用客户端缓存、不使用客户端缓存的连接区分。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;结合Guava缓存&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;Lettuce的StatefulRedisConnection类还提供了addListener方法，可以设置回调方法处理Redis推送的消息。&lt;br/&gt;利用该方法，我们可以将Guava的缓存与Redis客户端缓存结合&lt;/p&gt;&lt;pre&gt;&lt;code&gt;&lt;span&gt;@Bean&lt;/span&gt;&lt;br/&gt;public LoadingCache&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt; redisGuavaCache(RedisConnectionFactory redisConnectionFactory) {&lt;br/&gt;    &lt;span&gt;// [1]&lt;/span&gt;&lt;br/&gt;    StatefulRedisConnection connect = getRedisConnect(redisConnectionFactory);&lt;br/&gt;    &lt;span&gt;if&lt;/span&gt; (connect != &lt;span&gt;null&lt;/span&gt;) {&lt;br/&gt;        &lt;span&gt;// [2]&lt;/span&gt;&lt;br/&gt;        LoadingCache&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt; redisCache = CacheBuilder.newBuilder()&lt;br/&gt;                .initialCapacity(&lt;span&gt;5&lt;/span&gt;)&lt;br/&gt;                .maximumSize(&lt;span&gt;100&lt;/span&gt;)&lt;br/&gt;                .expireAfterWrite(&lt;span&gt;5&lt;/span&gt;, TimeUnit.MINUTES)&lt;br/&gt;                .build(&lt;span&gt;new&lt;/span&gt; CacheLoader&amp;lt;&lt;span&gt;String&lt;/span&gt;, &lt;span&gt;String&lt;/span&gt;&amp;gt;() {&lt;br/&gt;                    public &lt;span&gt;String&lt;/span&gt; load(&lt;span&gt;String&lt;/span&gt; key) { &lt;br/&gt;                        &lt;span&gt;String&lt;/span&gt; val = (&lt;span&gt;String&lt;/span&gt;)connect.&lt;span&gt;sync&lt;/span&gt;().&lt;span&gt;get&lt;/span&gt;(key);&lt;br/&gt;                        &lt;span&gt;return&lt;/span&gt; val == &lt;span&gt;null&lt;/span&gt; ? &lt;span&gt;&quot;&quot;&lt;/span&gt; : val;&lt;br/&gt;                    }&lt;br/&gt;                });&lt;br/&gt;        &lt;span&gt;// [3]&lt;/span&gt;&lt;br/&gt;        connect.&lt;span&gt;sync&lt;/span&gt;().clientTracking(TrackingArgs.Builder.enabled());&lt;br/&gt;        &lt;span&gt;// [4]&lt;/span&gt;&lt;br/&gt;        connect.addListener(message -&amp;gt; {&lt;br/&gt;            &lt;span&gt;if&lt;/span&gt; (message.getType().equals(&lt;span&gt;&quot;invalidate&quot;&lt;/span&gt;)) {&lt;br/&gt;                &lt;span&gt;List&lt;/span&gt;&amp;lt;&lt;span&gt;Object&lt;/span&gt;&amp;gt; content = message.getContent(StringCodec.UTF8::decodeKey);&lt;br/&gt;                &lt;span&gt;List&lt;/span&gt;&amp;lt;&lt;span&gt;String&lt;/span&gt;&amp;gt; keys = (&lt;span&gt;List&lt;/span&gt;&amp;lt;&lt;span&gt;String&lt;/span&gt;&amp;gt;) content.&lt;span&gt;get&lt;/span&gt;(&lt;span&gt;1&lt;/span&gt;);&lt;br/&gt;                keys.forEach(key -&amp;gt; {&lt;br/&gt;                    redisCache.invalidate(key);&lt;br/&gt;                });&lt;br/&gt;            }&lt;br/&gt;        });&lt;br/&gt;        &lt;span&gt;return&lt;/span&gt; redisCache;&lt;br/&gt;    }&lt;br/&gt;    &lt;span&gt;return&lt;/span&gt; &lt;span&gt;null&lt;/span&gt;;&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;&lt;/pre&gt;&lt;ol class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;获取Redis连接。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;创建Guava缓存类LoadingCache，该缓存类如果发现数据不存在，则查询Redis。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;开启Redis客户端缓存。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;添加回调函数，如果收到Redis发送的失效消息，则清除Guava缓存。&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;&lt;span&gt;Redis Cluster模式&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;上面说的应用必须在Redis单机模式下（或者主从、Sentinel模式），遗憾的是，&lt;br/&gt;目前发现Lettuce（6.1.5版本）还没有支持Redis Cluster下的客户端缓存。&lt;br/&gt;简单看了一下源码，目前发现如下原因：&lt;br/&gt;Cluster模式下，Redis命令需要根据命令的键，重定向到键的存储节点执行。&lt;br/&gt;而对于“CLIENT TRACKING”这个没有键的命令，Lettuce并没有将它发送给Cluster中所有的节点，而是将它发送给一个固定的默认的节点（可查看ClusterDistributionChannelWriter类），所以通过StatefulRedisClusterConnection调用RedisAdvancedClusterCommands.clientTracking方法并没有开启Redis服务的Tracking机制。&lt;br/&gt;这个其实也可以修改，有时间再研究一下。&lt;/p&gt;&lt;h3&gt;&lt;span&gt;需要注意的问题&lt;/span&gt;&lt;/h3&gt;&lt;p&gt;那么单机模式下，Lettuce的客户端缓存就真的没有问题了吗？&lt;/p&gt;&lt;p&gt;仔细思考一下Redis Tracking的设计，发现使用Redis客户端缓存有两个点需要关注：&lt;/p&gt;&lt;p&gt;1.  开启客户端缓存后，Redis连接不能断开。&lt;br/&gt;如果Redis连接断了，并且客户端自动重连，那么新的连接是没有开启Tracking机制的，该连接查询的键不会受到失效消息，后果很严重。&lt;br/&gt;同样，开启Tracking的连接和查询缓存键的连接必须是同一个，不能使用A连接开启Tracking机制，使用B连接去查询缓存键（所以客户端不能使用连接池）。&lt;/p&gt;&lt;p&gt;Redis服务器可以设置timeout配置，自动超过该配置没有发送请求的连接。&lt;br/&gt;而Lettuce有自动重连机制，重连后的连接将收不到失效消息。&lt;br/&gt;有两个解决思路：&lt;br/&gt;（1）实现Lettuce心跳机制，定时发送PING命令以维持连接。&lt;br/&gt;（2）即使使用心跳机制，Redis连接依然可能断开（网络跳动等原因），可以修改自动重连机制（Lettuce的ReconnectionHandler类），增加如下逻辑：如果连接原来开启了Tracking机制，则重连后需要自动开启Tracking机制。&lt;br/&gt;需要注意，如果使用的是非广播模式，需要清空旧连接缓存的数据，因为连接已经变更，Redis服务器不会将旧连接的失效消息发送给新连接。&lt;/p&gt;&lt;p&gt;2. 启用缓存的连接与未启动缓存的连接应该区分。&lt;br/&gt;这点比较简单，上例例子中都使用RedisClient#connect方法创建一个新的连接，专用于客户端缓存。&lt;/p&gt;&lt;p&gt;客户端缓存是一个强大的功能，需要我们去用好它。可惜当前暂时还没有完善的Java客户端支持，本书分享了我的一些方案与思路，欢迎探讨。我后续会关注继续Lettuce的更新，如果Lettuce提供了完善的Redis客户端缓存支持，再更新本文。&lt;/p&gt;&lt;p&gt;关于Redis Tracking的详细使用与实现原理，我在新书《Redis核心原理与实践》做了详尽分析，文章最后，介绍一下这本书：&lt;br/&gt;本书通过深入分析Redis 6.0源码，总结了Redis核心功能的设计与实现。通过阅读本书，读者可以深入理解Redis内部机制及最新特性，并学习到Redis相关的数据结构与算法、Unix编程、存储系统设计，分布式系统架构等一系列知识。&lt;br/&gt;&lt;/p&gt;&lt;p&gt;书籍详情：&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzI2MDQzMTU2MA==&amp;amp;mid=2247484112&amp;amp;idx=1&amp;amp;sn=c7d2d153a65a72fb3cd074803c7c2fbc&amp;amp;chksm=ea688977dd1f0061831cae4f5361182603b2e5778cae8aa0fc8f7e92ab75bc08c81afd717e6c&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;新书介绍 -- 《Redis核心原理与实践》&lt;/a&gt;&lt;/p&gt;&lt;/section&gt;
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>dcf57fb20f130f5bf351f116ec8bbe9c</guid>
<title>卷！Java 学这么多才能找到第一份工作</title>
<link>https://toutiao.io/k/gbgyn1h</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;section data-tool=&quot;mdnice编辑器&quot; data-website=&quot;https://www.mdnice.com&quot; data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我的读者里有很多 Java 新人，新人是指正在学 Java 的、以及工作时间不长的年轻人，他们经常问我一个问题：&lt;/p&gt;&lt;blockquote data-tool=&quot;mdnice编辑器&quot;&gt;&lt;p&gt;Java 学到什么程度才能找到一份还不错的工作？&lt;/p&gt;&lt;/blockquote&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;今天我就从我自己面试新人的角度来回答一下，我会把面试的知识点进行拆解，希望大家看完文章之后，能从中找到学习 Java 的重点。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;学习是为了找到工作、拿到 offer，这就有一个公式：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;strong&gt;offer = 工作需要的技术栈 + 计算机基础知识 + 项目经验 + 加分项&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;分别看看公式右边的这些方面具体是什么。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;1. 工作需要的技术栈&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;工作需要的技术栈往往和特定语言以及配套的周边工具相关。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对 Java 技术栈来说，又可以分为以下几个方面：&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.1 基本语法&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;基本语法在面试里不是说会用 for 循环、if else 语句之类去写代码就行，而是利用基本语法去编写更合理、更规范、更可读的代码。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;比如，是不是能让变量处于很合适的作用域？String、StringBuffer、StringBuilder 适合的应用场景是不是非常熟悉了？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果对于 Java 的基础还很薄弱，推荐去看《Effective Java》，以便真的能够写出来好代码。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;另外，为了能更加规范的写出高质量工程代码，推荐研究下阿里的《Java开发手册》。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.2 常用数据结构&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果要想通过面试找到工作，Java 中一些常用数据结构一定要好好掌握。最常用的就是以下几种：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;LinkedList&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;ArrayList&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Stack&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;ArrayBlockingQueue&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;LinkedBlockingQueue&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;HashMap&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;LinkedHashMap&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;TreeMap&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;ConcurrentHashMap&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;HashSet&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;TreeSet&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上列举出来的这些数据结构，不仅要熟练使用它们，更需要理解它们的原理。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;知道了原理，才能证明你能灵活且正确的使用这些数据结构。所以，面试中经常会问这些数据结构的原理。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果对这些结构的原理还不熟悉，这里推荐一本很老但是依然很有用的书《Java Generics and Collections》，它详细介绍了 Java 中各个集合的实现。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;但是这本书没有中文版，如果英文书读的很痛苦，大家可以去网上搜索一些好的文章，也能达到相同的效果。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.3 异步与多线程&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;现在只要编写高性能的代码，就离不开异步；只要在多核 CPU 下工作，就离不开多线程去平行执行任务。所以，异步和多线程是一位后端工程师必须掌握的核心技能。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而要达到找工作的水平，我认为至少满足如下几个条件：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;能准确的理解进程和线程的概念&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;至少弄懂什么叫 race condition 和死锁之类的概念&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;需要理解 JVM 的内存模型&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;了解常见的多线程编程模式&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于异步和多线程，我也不用多说了，肯定是首先推荐去读懂《Java并发编程实战》，然后还有一本《图解Java多线程设计模式》。这两本书读透了，面试异步和多线程，基本你也没什么问题了。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.4 IO 操作&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里说的 IO 操作，主要是利用 Java 去读写文件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Java 的 IO都在 java.io 包中，大概有 40 个流类。其实不用每一个都掌握，最基本的要求就是：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;理解 IO 流的概念&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;IO 里哪些是字节流哪些是字符流&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;IO 里哪些是节点流，哪些是处理流&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;哪些 IO 流带缓冲可以提高性能的&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;了解了这些东西，才能在实际开发中做到有的放矢，从而把 IO 用对用好。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;推荐看下 《Java IO》和《Java NIO》这两本书。其中《Java NIO》有中文版，《Java IO》没有，如果英文不好的同学，可以看个千锋教育的视频：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Java入门基础教程视频—（IO框架），地址如下：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;https://www.bilibili.com/video/BV1Tz4y1X7H7&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.5 常用框架&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除了 Java 语言本身的语法和库啊什么的，要找到工作还请务必熟悉至少一套开发框架。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Java 最常用的框架就是：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;Spring&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Spring MVC&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Spring Boot&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Mybatis&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;Netty&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;其实，Spring、Spring MVC、Spring Boot 都是 Spring 体系的。所以，要找后端工作，Spring 是必须要掌握的。有了 Spring 的基础，别的 Spring 体系可以即插即用的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;SSM 中的的 M——Mybatis 也最好掌握了，有数据库基础的话，Mybatis 学起来也不算难。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;Netty 是一套网络框架，我估计初期用到的机会不多，但奈何有些面试官会问、会考，所以，若有闲暇功夫，推荐对它看个大概。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;相关推荐的书有《Spring 5 开发大全》、《MyBatis技术内幕》、《Netty进阶之路：跟着案例学Netty》。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.6 常用数据库&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;一般来说，找工作，熟悉一个数据库就好。推荐 MySQL，这是最常见的数据库（其次是 Oracle）。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;要熟悉到什么程度呢？我认为最低水平要知道怎么用 MySQL 执行 CRUD 操作。但是这还不够保险，因为很多面试官为了防止新人删库跑路，会对面试者要求更高。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;像在 CRUD 里，R 代表查询，是后端工程师最常打交道的操作。&lt;strong&gt;而查询用的又对又好是对工程师使用 MySQL 水平的重要标准&lt;/strong&gt;。所以，面试官就希望面试者能对 &lt;strong&gt;MySQL 的索引原理&lt;/strong&gt;有一定的了解，面试的时候也会经常问这类问题。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;同样的道理，除了查询以外，CUD 即增、改、删是非常危险的，既可能影响性能，还可能出现各种误操作。所以，为了保证把人招进来，不会对数据库乱操作，面试的时候大概率会考察程序员对 &lt;strong&gt;MySQL 中的锁和 MVCC&lt;/strong&gt; 的理解情况，明白怎么用才安全可靠。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果对这些都不太熟悉，推荐看下《MySQL是怎样运行的》，对 MySQL 讲的非常透彻。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;1.7 Linux 基本操作&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;现在只要想做后端工程师，就肯定避不开和 Linux 系统打交道。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;基本的命令操作是肯定要熟悉的，比如：目录切换、设置环境变量、文件的增删查改等等。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除了这些基本命令，还要理解 Linux 中一切皆文件的概念。并且对于一些重要概念，比如：管道、重定向、标准输入输出、标准错误输出等，也要明白他们是什么东西。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;有时候，还需要开发人员直接在线上机器去编辑文件，所以还要会用 Vi/Vim 去在线编辑一些文件。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;另外，如果服务器有了问题，起码基本的查询性能的命令，也要会用。比如用 netstat、vmstat 等命令去查看下机器状态。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这些知识可以看下《鸟哥的Linux私房菜 基础学习篇 第四版》。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;2. 计算机基础知识&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;除了技术栈以外，还需要面试者有扎实的计算机基础。这个计算机基础是考察面试者在学校的学习状况的，如果你计算机基础不好，那大概率说明你本身学习态度、学习能力至少有一项不太合格。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对 Java 后端工程师，咱们挑两个很重要的基础知识说说：&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;2.1 计算机网络&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;后端大部分的时候就是各种接口和协议，数据传来传去，总是需要关注到网络问题的。如果一个后端工程师，对 TCP 不熟悉，对 Http 一问三不知，那么谁能相信你是一个合格的后端工程师了呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;对于计算机网络，我推荐好好看看《计算机网络（原书第7版）》这本书，在自己的头脑中，构建出一套完整的网络体系来。&lt;/p&gt;&lt;h2 data-tool=&quot;mdnice编辑器&quot;&gt;2.2 操作系统&lt;/h2&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;学完了计算机网络，接下来就是去好好学习下操作系统。学习操作系统就是去学习如何在复杂的情况下，去协调分配各种像 CPU、内存、IO 等这些竞争性使用的计算机资源。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;学懂了操作系统，你心里就有了一套在复杂情况下，去管理协调资源的成熟套路和方法论。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而这些成熟的套路和方法论，最终都是你后面能成长为优秀的工程师，以及能应对各种复杂业务需求的核心竞争力。所以，操作系统学懂是很重要的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果对操作系统不熟悉，我推荐看下《操作系统导论》这本书，人民邮电出版社出版，王海鹏翻译的。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;3. 做过的项目&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;面试官是如何在面试中考察你是否真的做过项目呢？&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;其实挺简单的，&lt;strong&gt;只需要问一下你做的项目中核心模块的业务流程，然后挑出这个业务流程中的一些技术难点或者技术特点，重点问你是怎么实现的&lt;/strong&gt;，就能大概判断出你是否真的做过项目了。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;至于项目，主流的项目有这么几类：&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在面试前，最好去专门熟悉下不同类项目的核心流程都有哪些。比如，电商的核心流程就有购物流程，社交的核心流程就有私聊、群发消息，出行平台则是人们叫车的这个过程。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;而在这些核心业务流程里，一定有技术难点需要克服，比如：&lt;/p&gt;&lt;ul data-tool=&quot;mdnice编辑器&quot; class=&quot;list-paddingleft-2&quot;&gt;&lt;li&gt;&lt;section&gt;购物车到底放在客户端还是服务器端？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;放客户端，排序规则，缓存的更新策略你是怎么解决的？&lt;/section&gt;&lt;/li&gt;&lt;li&gt;&lt;section&gt;而放在服务器端呢，你和客户端之间通信频率啊，客户商品的实时更新啊，又是怎么考量的？&lt;/section&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这些没有经过实战，很难给面试官讲清楚。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，在应聘工作前，最好就是自己能去参与个项目，多学多问，多考虑为什么项目中需要这样做的道理。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果真的参与不了，我推荐去 github 下载个开源的项目，然后弄清楚架构和业务流程，对于一些不懂得，可以问下开源的作者，或者去问下已经工作的前辈，又或者去专业的论坛问。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;github 的开源项目可以参考我之前写的这篇：&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzU3MTg3NDYwNg==&amp;amp;mid=2247485134&amp;amp;idx=1&amp;amp;sn=f8c35fd83d5290e8aabf66f219ab518f&amp;amp;chksm=fcd8ca8fcbaf439945fb0dad200f948763329c8817b60af2fce333ae5e15bbd6381d265e6987&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;你们要的学Java的练手项目，来了&lt;/a&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;总之，一定要从我给的分类项目中找一个项目，去深入了解业务流程和对应的技术实现，以及实现背后的动机，只有这样，面试官才会真的认为你有了实战经验。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;4. 加分项&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;说完了项目经验，我再说说加分项（技术博客、开源项目、竞赛奖项等），加分项在去大厂应聘的时候尤其有用。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;无论你是有开源项目还是有博客文章，都说明了一件事——你是一个爱学习、乐于分享的人。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;大厂里，很重视的一件事就是分享。分享经验、分享技术、分享心得，这些都是大厂非常提倡的事情，如果在一群候选人里，你有加分项，别人没有，面试官会选谁不言而喻了吧。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;所以，一定要经常性的对公众输出一些东西，文章啊、开源项目啊、竞赛分享啊，能输出什么输出什么。这样才能&lt;strong&gt;让面试官看到你有实力的证据以及你 open 的态度&lt;/strong&gt;，从而在面试中占有更多的优势。&lt;/p&gt;&lt;h1 data-tool=&quot;mdnice编辑器&quot;&gt;结尾&lt;/h1&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;好了，说了这么多，其实一言以蔽之，就是知识储备一定要足。要说达到什么水平就一定能找到工作，这完全是根据你应聘的公司，应聘的岗位以及你的竞争对手的优秀程度去动态变化的。&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;以上知识点，可能有人会感觉多，也可能有人会觉得不够，无论怎样，希望这篇文章能帮新人弄清楚学习的重点，希望大家能坚持学起来。&lt;strong&gt;你只有拼命努力，才能看起来毫不费力。&lt;/strong&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;看完有收获，可以分享给别人，或者随手点个&lt;/span&gt;&lt;span&gt;&lt;strong&gt;在看&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，让更多的人看到&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;你好，我是四猿外。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;一家上市公司的技术总监，管理的技术团队一百余人。想了解我如何管理团队——&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzU3MTg3NDYwNg==&amp;amp;mid=2247485282&amp;amp;idx=1&amp;amp;sn=f368ffae1845809ccf06859f988a88a8&amp;amp;chksm=fcd8cb23cbaf4235db644759c3d8099045d10fc952b950d429e4a5e07ed9a806fddf85c451d3&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot; wah-hotarea=&quot;click&quot; hasload=&quot;1&quot;&gt;我，管理100多人团队的二三事&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;我从一名非计算机专业的毕业生，转行到程序员，一路打拼，一路成长。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;我会通过公众号，&lt;br/&gt;把自己的成长故事写成文章，&lt;br/&gt;把枯燥的技术文章写成故事。&lt;/span&gt;&lt;/p&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span&gt;我建了一个读者交流群，里面大部分是程序员，一起聊技术、工作、八卦。欢迎加我微信，拉你入群。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img&quot; data-galleryid=&quot;&quot; data-ratio=&quot;0.9852216748768473&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/6nbNnibOq5KQibCDibpTo0kqofPehQvDDibibcb3bQUELdY3Knsl4r0RcgsV9l4icr3icmZQfaBXtSFNTxmdQlAZT1OQg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;609&quot;/&gt;&lt;/p&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
<item>
<guid>aa5dad20936b0db1b3766045d0fd0d56</guid>
<title>Hum to Search 背后的机器学习</title>
<link>https://toutiao.io/k/38xpihl</link>
<content:encoded>&lt;div&gt;&lt;div&gt;&lt;div class=&quot;rich_media_content &quot; id=&quot;js_content&quot;&gt;
                    

                    
                    
                    
                    &lt;p&gt;萦绕 在您脑海中的旋律，通常被称为“耳虫”，是一种众所周知且有时令人恼火的现象——一旦耳虫出现，就很难摆脱它。研究发现，与原始歌曲互动，无论是听还是唱，都会将耳虫赶走。但是如果你不太记得这首歌的名字，只能哼着旋律怎么办？&lt;/p&gt;&lt;p&gt;将哼唱的旋律与其原始的复调录音室录音相匹配的现有方法面临着几个挑战。通过歌词、背景人声和乐器，音乐或录音室录音的音频可能与哼唱的曲调截然不同。由于错误或设计，当有人哼唱他们对歌曲的诠释时，通常音高、调、节奏或节奏可能会略有不同甚至显着不同。这就是为什么这么多现有的通过哼唱进行查询的方法将哼唱的曲调与歌曲的预先存在的仅旋律或哼唱版本的数据库相匹配，而不是直接识别歌曲。但是，这种方法通常依赖于需要手动更新的有限数据库。&lt;/p&gt;&lt;p&gt;Hum to Search 于 10 月推出，是 Google 搜索中一个全新的完全机器学习的系统，它允许人们仅使用哼唱的歌曲来查找歌曲。与现有方法相比，这种方法从歌曲的频谱图生成旋律的嵌入，而无需生成中间表示。这使模型能够将哼唱的旋律直接与原始（和弦）录音相匹配，而无需每个音轨的哼唱或 MIDI 版本或其他复杂的手工设计逻辑来提取旋律。这种方法极大地简化了 Hum to Search 的数据库，使其能够通过嵌入来自世界各地的原始录音（甚至是最新版本）不断刷新。&lt;/p&gt;&lt;h2&gt;&lt;em&gt;背景&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;许多现有的音乐识别系统在处理音频样本之前将其转换为频谱图，以便找到良好的匹配。然而，在识别哼唱旋律的一个挑战是，哼唱曲调通常包含信息相对较少，通过如示出的这个示例哼唱的啊朋友再见。哼唱版本与相应录音室录音中的相同片段之间的差异可以使用频谱图来可视化，如下所示：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;img data-ratio=&quot;0.7073643410852714&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/eG1jA7faiceFhJfMKsXicUhvHHtnNSwWsImibWyRiaR0FHdHnT8JcXciawh7tI0PeibHwVzY8atcYbOuhgRTqgvwyvCQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;516&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;给定左侧的图像，模型需要从超过 5000 万张外观相似的图像（对应于其他歌曲的录音室录音片段）的集合中定位与右侧图像对应的音频。为了实现这一点，模型必须学会专注于主导旋律，而忽略背景人声、乐器和声音音色，以及来自背景噪音或房间混响的差异。为了通过肉眼找到可能用于匹配这两个频谱图的主导旋律，一个人可能会在上图底部附近的线条中寻找相似之处。&lt;/p&gt;&lt;p&gt;之前为发现音乐所做的努力，特别是在识别在咖啡馆或俱乐部等环境中播放的录制音乐的背景下，展示了如何将机器学习应用于这个问题。Now Playing于 2017 年发布到 Pixel 手机，它使用设备上的深度神经网络来识别歌曲，而无需连接服务器，而Sound Search进一步开发了这项技术，以提供基于服务器的识别服务，以实现更快、更准确的搜索超过 1 亿首歌曲。下一个挑战是利用从这些版本中学到的知识来识别来自类似大型歌曲库的哼唱或演唱的音乐。&lt;/p&gt;&lt;h2&gt;&lt;em&gt;机器学习设置&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;开发 Hum to Search 的第一步是修改“正在播放”和“声音搜索”中使用的音乐识别模型，以处理哼唱的录音。原则上，许多这样的检索系统（例如，图像识别）以类似的方式工作。神经网络使用成对的输入（这里是成对的带有录制音频的哼唱或演唱音频）进行训练，为每个输入生成嵌入，稍后将用于匹配哼唱的旋律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-ratio=&quot;0.7632575757575758&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/eG1jA7faiceFhJfMKsXicUhvHHtnNSwWsI7Ymiceab8cLnibLH0wmiah7A8kgfyTicpqoia6CHdWLpmPnYFbA3U4MSRIg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;528&quot;/&gt;&lt;/p&gt;&lt;p&gt;为了实现哼唱识别，网络应该生成嵌入，使包含相同旋律的音频对彼此接近，即使它们具有不同的乐器伴奏和歌声。包含不同旋律的成对音频应该相距很远。在训练中，网络会提供这样的音频对，直到它学会使用此属性生成嵌入。&lt;/p&gt;&lt;p&gt;然后，经过训练的模型可以生成类似于歌曲参考录音的嵌入的曲调嵌入。找到正确的歌曲只需从流行音乐的音频计算出的参考录音数据库中搜索类似的嵌入即可。&lt;/p&gt;&lt;h2&gt;&lt;em&gt;训练数据&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;因为模型的训练需要歌曲对（录制和演唱），所以第一个挑战是获得足够的训练数据。我们的初始数据集主要由唱歌的音乐片段组成（其中很少包含哼唱）。为了使模型更加健壮，我们在训练期间增强了音频，例如通过随机改变唱歌输入的音高或节奏。由此产生的模型对于唱歌的人来说效果很好，但对于哼唱或吹口哨的人来说却不是。&lt;/p&gt;&lt;p&gt;为了提高模型在哼唱旋律上的表现，我们使用SPICE从现有音频数据集中生成了模拟“哼唱”旋律的额外训练数据，SPICE是我们更广泛的团队开发的音高提取模型，作为FreddieMeter项目的一部分。SPICE 从给定的音频中提取音高值，然后我们使用它来生成由离散音频音调组成的旋律。这个系统的第一个版本将这个原始剪辑转换成这些音调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-ratio=&quot;1.5096153846153846&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/eG1jA7faiceFhJfMKsXicUhvHHtnNSwWsIrcIxYE59dVZcicqmk36lfxBLFOTCzJxdfcl5fjRQXNqsDd6KIykuQcQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;208&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们后来通过用神经网络替换简单的音调发生器来改进这种方法，该神经网络生成类似于实际哼唱或吹口哨曲调的音频。例如，网络从上述唱歌的片段中生成了这个哼唱示例或吹口哨示例。&lt;/p&gt;&lt;p&gt;作为最后一步，我们通过混合和匹配音频样本来比较训练数据。例如，如果我们有来自两个不同歌手的相似剪辑，我们会将这两个剪辑与我们的初步模型对齐，因此能够向模型显示代表相同旋律的附加音频剪辑对。&lt;/p&gt;&lt;h2&gt;&lt;em&gt;机器学习改进&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;在训练 Hum to Search 模型时，我们从三元组损失函数开始。这种损失已被证明在各种分类任务（如图像和录制的音乐）中表现良好. 给定一对对应于相同旋律的音频（嵌入空间中的点 R 和 P，如下所示），triplet loss 将忽略源自不同旋律的训练数据的某些部分。这有助于机器改进学习行为，无论是当它发现一个太“容易”的不同旋律时，因为它已经远离 R 和 P（见点 E），或者因为它太难了，考虑到模型的当前在学习状态下，音频最终与 R 太接近了——尽管根据我们的数据，它代表了不同的旋律（见 H 点）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-ratio=&quot;0.2063253012048193&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/eG1jA7faiceFhJfMKsXicUhvHHtnNSwWsIY4oWr3TRicUp15JmMKHjiaAKEK7dHl8C3S2zib2KtynN9Vj7nX49dibSoQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;664&quot;/&gt;&lt;/p&gt;&lt;p&gt;我们发现，我们可以通过采取这些额外的训练数据（点H和E）考虑提高模型的精确度，即通过在一个批次的例子制订的模式的信心一般概念：如何确保是模型，所有它看到的数据可以正确分类，还是看到不符合其当前理解的示例？基于这种置信度概念，我们添加了一个损失，使嵌入空间所有区域的模型置信度达到 100%，从而提高了我们模型的精度和召回率。&lt;/p&gt;&lt;p&gt;上述变化，尤其是我们对训练数据的变化、增强和叠加，使部署在 Google 搜索中的神经网络模型能够识别唱歌或哼唱的旋律。当前系统在包含超过 50 万首歌曲的歌曲数据库上达到了很高的准确性，我们正在不断更新这些歌曲。这首歌的语料库仍有成长的空间，以包含更多世界上的许多旋律。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img class=&quot;rich_pages wxw-img js_insertlocalimg&quot; data-ratio=&quot;2&quot; data-s=&quot;300,640&quot; data-src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/eG1jA7faiceFhJfMKsXicUhvHHtnNSwWsIlcIEle86SrAaCTDNwElHalW4wUHHDPVZAQ2tGeq8QEk4KBg2TPYTwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;200&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&lt;em&gt;在 Google 应用中哼哼搜索&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;要试用该功能，您可以打开最新版本的 Google 应用，点按麦克风图标并说“这首歌是什么？” 或单击“搜索歌曲”按钮，之后您可以哼唱、唱歌或吹口哨！我们希望 Hum to Search 可以帮助您解决这个问题，或者可能只是帮助您查找和播放歌曲而无需输入歌曲名称。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
                &lt;/div&gt;

                

                



                
                &lt;/div&gt;&lt;/div&gt;</content:encoded>
</item>
</channel></rss>